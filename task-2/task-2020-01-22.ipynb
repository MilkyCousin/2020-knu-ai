{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pi\n",
    "nltk==3.4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from this link:\n",
    "    \n",
    "    https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\r\\n\\r\\nCongratulations from me as well, use ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\r\\n\\r\\nCongratulations from me as well, use ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous day we worked with already preprocessed data for us.  \n",
    "This day try to make this preprocessing by ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text lowercasing: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a column 'comment_text_lower' in a dataframe, and make all of the text from the column 'comment_text' copied to the 'comment_text_lower' column, but lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df['comment_text_lower'] = df['comment_text'].apply(str.lower) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\r\\nwhy the edits made under my use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                  comment_text_lower  \n",
       "0  explanation\\r\\nwhy the edits made under my use...  \n",
       "1  d'aww! he matches this background colour i'm s...  \n",
       "2  hey man, i'm really not trying to edit war. it...  \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions...  \n",
       "4  you, sir, are my hero. any chance you remember...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a column 'comment_text_tokenized_space' in a dataframe, and make all of the text from the column 'comment_text' copied to the 'comment_text_tokenized_space' column, but lowercased tokenized by space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df['comment_text_tokenized_space'] = df['comment_text_lower'].apply(str.split) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_lower</th>\n",
       "      <th>comment_text_tokenized_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\r\\nwhy the edits made under my use...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>[d'aww!, he, matches, this, background, colour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>[hey, man,, i'm, really, not, trying, to, edit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions...</td>\n",
       "      <td>[\", more, i, can't, make, any, real, suggestio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you,, sir,, are, my, hero., any, chance, you,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                  comment_text_lower  \\\n",
       "0  explanation\\r\\nwhy the edits made under my use...   \n",
       "1  d'aww! he matches this background colour i'm s...   \n",
       "2  hey man, i'm really not trying to edit war. it...   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                        comment_text_tokenized_space  \n",
       "0  [explanation, why, the, edits, made, under, my...  \n",
       "1  [d'aww!, he, matches, this, background, colour...  \n",
       "2  [hey, man,, i'm, really, not, trying, to, edit...  \n",
       "3  [\", more, i, can't, make, any, real, suggestio...  \n",
       "4  [you,, sir,, are, my, hero., any, chance, you,...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There a lot of words tokenized by space, but they contains additional punctuation characters. Let's try to delete them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Load punctuation \n",
    "from string import punctuation\n",
    "\n",
    "punctuation+='“”'\n",
    "\n",
    "def clean_token(token): \n",
    "    '''\n",
    "    Args: token: str \n",
    "    Returns: token: str \n",
    "    \n",
    "    This function deletes all of the punctuation characters \n",
    "    in the token and returns the cleaned one \n",
    "    '''\n",
    "    chars_cleaned = token.strip(punctuation) \n",
    "    return \"\".join(chars_cleaned)\n",
    "\n",
    "# Use method apply - read about it more if needed (pandas, df.apply, lambda functions, list of comprehension)\n",
    "df['comment_text_tokenized_space_cleaned'] = df['comment_text_tokenized_space'].apply(\n",
    "    lambda x: [clean_token(w) for w in x]  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_lower</th>\n",
       "      <th>comment_text_tokenized_space</th>\n",
       "      <th>comment_text_tokenized_space_cleaned</th>\n",
       "      <th>nltk_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\r\\nwhy the edits made under my use...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[Explanation, Why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>[d'aww!, he, matches, this, background, colour...</td>\n",
       "      <td>[d'aww, he, matches, this, background, colour,...</td>\n",
       "      <td>[D'aww, !, He, matches, this, background, colo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>[hey, man,, i'm, really, not, trying, to, edit...</td>\n",
       "      <td>[hey, man, i'm, really, not, trying, to, edit,...</td>\n",
       "      <td>[Hey, man, ,, I, 'm, really, not, trying, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions...</td>\n",
       "      <td>[\", more, i, can't, make, any, real, suggestio...</td>\n",
       "      <td>[, more, i, can't, make, any, real, suggestion...</td>\n",
       "      <td>[``, More, I, ca, n't, make, any, real, sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you,, sir,, are, my, hero., any, chance, you,...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>[You, ,, sir, ,, are, my, hero, ., Any, chance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                  comment_text_lower  \\\n",
       "0  explanation\\r\\nwhy the edits made under my use...   \n",
       "1  d'aww! he matches this background colour i'm s...   \n",
       "2  hey man, i'm really not trying to edit war. it...   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                        comment_text_tokenized_space  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d'aww!, he, matches, this, background, colour...   \n",
       "2  [hey, man,, i'm, really, not, trying, to, edit...   \n",
       "3  [\", more, i, can't, make, any, real, suggestio...   \n",
       "4  [you,, sir,, are, my, hero., any, chance, you,...   \n",
       "\n",
       "                comment_text_tokenized_space_cleaned  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d'aww, he, matches, this, background, colour,...   \n",
       "2  [hey, man, i'm, really, not, trying, to, edit,...   \n",
       "3  [, more, i, can't, make, any, real, suggestion...   \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...   \n",
       "\n",
       "                                      nltk_tokenized  \n",
       "0  [Explanation, Why, the, edits, made, under, my...  \n",
       "1  [D'aww, !, He, matches, this, background, colo...  \n",
       "2  [Hey, man, ,, I, 'm, really, not, trying, to, ...  \n",
       "3  [``, More, I, ca, n't, make, any, real, sugges...  \n",
       "4  [You, ,, sir, ,, are, my, hero, ., Any, chance...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a lot of work, yes?   \n",
    "Let's try to use already implemented methods for performing a tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hedviga258\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  sent_tokenize - if you need to separate sentences one from another \n",
    "#  word_tokenize - if you need to tokenize a sentence\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt') # if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df['nltk_tokenized'] = df.comment_text.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_lower</th>\n",
       "      <th>comment_text_tokenized_space</th>\n",
       "      <th>comment_text_tokenized_space_cleaned</th>\n",
       "      <th>nltk_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\r\\nwhy the edits made under my use...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[explanation, why, the, edits, made, under, my...</td>\n",
       "      <td>[Explanation, Why, the, edits, made, under, my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>[d'aww!, he, matches, this, background, colour...</td>\n",
       "      <td>[d'aww, he, matches, this, background, colour,...</td>\n",
       "      <td>[D'aww, !, He, matches, this, background, colo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>[hey, man,, i'm, really, not, trying, to, edit...</td>\n",
       "      <td>[hey, man, i'm, really, not, trying, to, edit,...</td>\n",
       "      <td>[Hey, man, ,, I, 'm, really, not, trying, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions...</td>\n",
       "      <td>[\", more, i, can't, make, any, real, suggestio...</td>\n",
       "      <td>[, more, i, can't, make, any, real, suggestion...</td>\n",
       "      <td>[``, More, I, ca, n't, make, any, real, sugges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>[you,, sir,, are, my, hero., any, chance, you,...</td>\n",
       "      <td>[you, sir, are, my, hero, any, chance, you, re...</td>\n",
       "      <td>[You, ,, sir, ,, are, my, hero, ., Any, chance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\r\\nWhy the edits made under my use...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\r\\nMore\\r\\nI can't make any real suggestions...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                  comment_text_lower  \\\n",
       "0  explanation\\r\\nwhy the edits made under my use...   \n",
       "1  d'aww! he matches this background colour i'm s...   \n",
       "2  hey man, i'm really not trying to edit war. it...   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions...   \n",
       "4  you, sir, are my hero. any chance you remember...   \n",
       "\n",
       "                        comment_text_tokenized_space  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d'aww!, he, matches, this, background, colour...   \n",
       "2  [hey, man,, i'm, really, not, trying, to, edit...   \n",
       "3  [\", more, i, can't, make, any, real, suggestio...   \n",
       "4  [you,, sir,, are, my, hero., any, chance, you,...   \n",
       "\n",
       "                comment_text_tokenized_space_cleaned  \\\n",
       "0  [explanation, why, the, edits, made, under, my...   \n",
       "1  [d'aww, he, matches, this, background, colour,...   \n",
       "2  [hey, man, i'm, really, not, trying, to, edit,...   \n",
       "3  [, more, i, can't, make, any, real, suggestion...   \n",
       "4  [you, sir, are, my, hero, any, chance, you, re...   \n",
       "\n",
       "                                      nltk_tokenized  \n",
       "0  [Explanation, Why, the, edits, made, under, my...  \n",
       "1  [D'aww, !, He, matches, this, background, colo...  \n",
       "2  [Hey, man, ,, I, 'm, really, not, trying, to, ...  \n",
       "3  [``, More, I, ca, n't, make, any, real, sugges...  \n",
       "4  [You, ,, sir, ,, are, my, hero, ., Any, chance...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, see that word_tokenize not only separated the punctuation from the text corretly, but saved the punctuation inside the token (' or -)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ! Modify your function defined previously to save a pucntuation inside the token and delete it only if it glued to the token in the end or in the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "### Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def flat_nested(nested):\n",
    "    '''\n",
    "    Args: nested list: list ([[a], [b]])\n",
    "    Returns: flatten list: list ([a, b])\n",
    "    '''\n",
    "    flatten = []\n",
    "    for n in nested:\n",
    "        if type(n) == list:\n",
    "            flatten+=flat_nested(n)\n",
    "        else:\n",
    "            flatten.append(n)\n",
    "    return flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Frequency dict will accept only list of tokens, not list of lists of tokens, etc \n",
    "# Flat your list previously if needed \n",
    "# print(flat_nested(df['comment_text_tokenized_space_cleaned'].tolist())[:10])\n",
    "#print(df['comment_text_tokenized_space_cleaned'].head())\n",
    "#print(flat_nested(df['comment_text_tokenized_space_cleaned'].to_list())[:10])\n",
    "tokens = flat_nested(df['comment_text_tokenized_space_cleaned'].to_list())\n",
    "fdist = FreqDist(tokens)\n",
    "del fdist['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 495256),\n",
       " ('to', 296824),\n",
       " ('of', 224005),\n",
       " ('and', 222327),\n",
       " ('a', 214158),\n",
       " ('you', 204466),\n",
       " ('i', 200483),\n",
       " ('is', 175925),\n",
       " ('that', 154263),\n",
       " ('in', 144148),\n",
       " ('it', 129560),\n",
       " ('for', 102439),\n",
       " ('this', 97029),\n",
       " ('not', 93319),\n",
       " ('on', 89427),\n",
       " ('be', 83315),\n",
       " ('as', 77203),\n",
       " ('have', 72170),\n",
       " ('are', 71864),\n",
       " ('your', 63237)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyse the results. What are these words? Have you seen it previously in the 1 task?  \n",
    "Does these words contains a lot of meaningful information? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#### Most common words are stopwords, so they don't contain any meaningful information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "fdist.plot(30, cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"that'll\", 'don', 'such', 'an', \"isn't\", 'and', 'are', 'out', \"haven't\", 'that', 'theirs', 'before', 'were', 'me', \"didn't\", 'doing', 'doesn', 'yourselves', \"mustn't\", 'how', 'aren', 'you', \"shouldn't\", \"won't\", 'about', 'wouldn', 'a', \"it's\", \"you'd\", 'not', 'or', 'any', 'she', 'other', 'those', 'myself', 'am', 'isn', 'of', \"she's\", 'should', 'off', 'them', 'down', 'where', 'hers', 'very', 'didn', 'nor', 'same', 'can', 'y', \"don't\", 'again', 'some', 'most', 'below', 'will', 'been', 'it', 'while', 'herself', 'at', 'does', 'until', 'in', \"hadn't\", 'couldn', 'this', 'then', 'more', 'once', 'here', 'with', 'own', 'o', 'wasn', 'under', 'if', \"should've\", 'he', 'by', 'we', 'him', 'why', 'against', \"couldn't\", 'haven', 'ours', 'yourself', \"wasn't\", 'what', 'had', \"you've\", 'which', 'itself', 'was', 'have', 'between', 'do', 'from', 'now', \"aren't\", 'hadn', \"hasn't\", 'your', 'won', \"needn't\", 'as', 'there', 'too', 'shouldn', 'mustn', 'few', 'mightn', 'all', 'who', 'ourselves', 'did', 'his', 'm', 'during', 'after', 'both', \"shan't\", \"weren't\", 've', 'so', 'because', 'for', 'above', 'having', 'up', 'needn', 'the', 'than', \"you're\", 'is', 'to', 'd', 'our', 'whom', 'these', 'further', \"doesn't\", 'i', 'hasn', 'just', 'they', 'yours', 're', 'ma', 'but', 'each', 'their', 'her', 'into', \"wouldn't\", 'themselves', 'through', \"you'll\", 'on', 'only', 'shan', 'being', 'over', 'no', 'ain', 'my', 'its', 'has', \"mightn't\", 'himself', 'when', 'll', 't', 'weren', 'be', 's'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hedviga258\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete all of the stop words from the list of tokens created by nltk word_tokenize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('article', 55397),\n",
       " ('page', 45606),\n",
       " ('wikipedia', 35539),\n",
       " ('talk', 30901),\n",
       " ('please', 29601),\n",
       " ('would', 29211),\n",
       " ('one', 28052),\n",
       " ('like', 27702),\n",
       " ('see', 21482),\n",
       " ('also', 20545),\n",
       " ('think', 20036),\n",
       " ('know', 18986),\n",
       " (\"i'm\", 17773),\n",
       " ('people', 17685),\n",
       " ('edit', 17582),\n",
       " ('use', 16313),\n",
       " ('articles', 15883),\n",
       " ('may', 15550),\n",
       " ('time', 15386),\n",
       " ('thanks', 13742)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for w in stop_words:\n",
    "    del fdist[w]\n",
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are probably a lot of words such as 'apple'/'apples', etc whose presence extends our vocabulary a lot. \n",
    "Please, calculate the size of your vocabulary here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266450\n"
     ]
    }
   ],
   "source": [
    "### Define the size of your vocab - number of unique words from all of the texts \n",
    "\n",
    "vocab_size_init = len(fdist)\n",
    "print(vocab_size_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hedviga258\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "# Stemming will help us to reduce the numbed of uniq words in our vocabulary by deleting different forms of the same word\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stem = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word(word):\n",
    "    for l in word:\n",
    "        if not (l.isalpha() or l in \"'-\"):\n",
    "            #print(word)\n",
    "            return False\n",
    "            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please, apply stemming  and lemmatization to the tokenized words.  \n",
    "##### 1. Apply stemming first - calculate the number of the unique words after it \n",
    "##### 2. Apply lemmatization and calculate the same\n",
    "##### 3. Compare, analyse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183667\n",
      "147610 172029\n"
     ]
    }
   ],
   "source": [
    "### Your code here \n",
    "\n",
    "common_fdist = FreqDist()\n",
    "for w in fdist:\n",
    "    if is_word(w) and len(w)<100:\n",
    "        common_fdist[w] = fdist[w]\n",
    "    else:\n",
    "        pass\n",
    "        #print(w)\n",
    "print(len(common_fdist))\n",
    "stem_fdist = FreqDist()\n",
    "lemm_fdist = FreqDist()\n",
    "for w in common_fdist:\n",
    "    try:\n",
    "        stem_fdist[stem.stem(w)] += common_fdist[w]\n",
    "        lemm_fdist[lem.lemmatize(w, 'v')] += common_fdist[w]\n",
    "    except RecursionError:\n",
    "        print(w)\n",
    "vocab_size_stemmed = len(stem_fdist)\n",
    "vocab_size_lemmatized = len(lemm_fdist) \n",
    "print(vocab_size_stemmed, vocab_size_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sample_text = \"\"\"What is impeachment?\n",
    "Put simply, it's a process that allows senior figures in government to hold other officials (like judges, the president and cabinet members) to account if they're suspected of committing offences while in office.\n",
    "Those offences can include \"treason, bribery or other high crimes and misdemeanours\".\n",
    "After someone is impeached, they then go on trial in the Senate, the upper house of Congress, the members of which will decide whether they are guilty or not. It's a political trial, not a criminal one.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sample_text = sample_text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in d:\\app\\pyton\\python36\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in d:\\app\\pyton\\python36\\lib\\site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.1)\n",
      "Requirement already satisfied: setuptools in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (40.6.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in d:\\app\\pyton\\python36\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\app\\pyton\\python36\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\app\\pyton\\python36\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\app\\pyton\\python36\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\app\\pyton\\python36\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in d:\\app\\pyton\\python36\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in d:\\app\\pyton\\python36\\lib\\site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.42.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\app\\pyton\\python36\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.1)\n",
      "Requirement already satisfied: more-itertools in d:\\app\\pyton\\python36\\lib\\site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (8.1.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# You need to load the model firstly, once loaded, you can comment the line \n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['What', 'is', 'impeachment', '?', 'Put', 'simply', ',', 'it', \"'s\", 'a', 'process', 'that', 'allows', 'senior', 'figures', 'in', 'government', 'to', 'hold', 'other', 'officials', '(', 'like', 'judges', ',', 'the', 'president', 'and', 'cabinet', 'members', ')', 'to', 'account', 'if', 'they', \"'re\", 'suspected', 'of', 'committing', 'offences', 'while', 'in', 'office', '.', 'Those', 'offences', 'can', 'include', '\"', 'treason', ',', 'bribery', 'or', 'other', 'high', 'crimes', 'and', 'misdemeanours', '\"', '.', 'After', 'someone', 'is', 'impeached', ',', 'they', 'then', 'go', 'on', 'trial', 'in', 'the', 'Senate', ',', 'the', 'upper', 'house', 'of', 'Congress', ',', 'the', 'members', 'of', 'which', 'will', 'decide', 'whether', 'they', 'are', 'guilty', 'or', 'not', '.', 'It', \"'s\", 'a', 'political', 'trial', ',', 'not', 'a', 'criminal', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "doc = nlp(sample_text)\n",
    "spacy_words = [token.text for token in doc]\n",
    "\n",
    "print(f\"Tokenized words: {spacy_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes which spacy token has: \n",
      " ['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_kb_id', 'ent_kb_id_', 'ent_type', 'ent_type_', 'get_extension', 'has_extension', 'has_vector', 'head', 'i', 'idx', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'morph', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'shape', 'shape_', 'similarity', 'string', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'tensor', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n"
     ]
    }
   ],
   "source": [
    "print(\"Attributes which spacy token has: \\n {}\".format([dir(tok) for tok in doc][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words: ['What', 'be', 'impeachment', '?', 'Put', 'simply', ',', '-PRON-', 'have', 'a', 'process', 'that', 'allow', 'senior', 'figure', 'in', 'government', 'to', 'hold', 'other', 'official', '(', 'like', 'judge', ',', 'the', 'president', 'and', 'cabinet', 'member', ')', 'to', 'account', 'if', '-PRON-', 'be', 'suspect', 'of', 'commit', 'offence', 'while', 'in', 'office', '.', 'Those', 'offence', 'can', 'include', '\"', 'treason', ',', 'bribery', 'or', 'other', 'high', 'crime', 'and', 'misdemeanour', '\"', '.', 'After', 'someone', 'be', 'impeach', ',', 'they', 'then', 'go', 'on', 'trial', 'in', 'the', 'Senate', ',', 'the', 'upper', 'house', 'of', 'Congress', ',', 'the', 'member', 'of', 'which', 'will', 'decide', 'whether', 'they', 'be', 'guilty', 'or', 'not', '.', '-PRON-', 'have', 'a', 'political', 'trial', ',', 'not', 'a', 'criminal', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "# We can access lemmas: \n",
    "\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(f\"Lemmatized words: {lemmas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned words: ['impeachment', '?', 'simply', ',', 'process', 'allow', 'senior', 'figure', 'government', 'hold', 'official', '(', 'like', 'judge', ',', 'president', 'cabinet', 'member', ')', 'account', 'suspect', 'commit', 'offence', 'office', '.', 'offence', 'include', '\"', 'treason', ',', 'bribery', 'high', 'crime', 'misdemeanour', '\"', '.', 'impeach', ',', 'trial', 'Senate', ',', 'upper', 'house', 'Congress', ',', 'member', 'decide', 'guilty', '.', 'political', 'trial', ',', 'criminal', '.']\n"
     ]
    }
   ],
   "source": [
    "# We can filter stop words: \n",
    "\n",
    "cleaned_words = [token.lemma_ for token in doc if not token.is_stop]\n",
    "print(f\"Cleaned words: {cleaned_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned words: ['What', 'be', 'impeachment', 'Put', 'simply', '-PRON-', 'have', 'a', 'process', 'that', 'allow', 'senior', 'figure', 'in', 'government', 'to', 'hold', 'other', 'official', 'like', 'judge', 'the', 'president', 'and', 'cabinet', 'member', 'to', 'account', 'if', '-PRON-', 'be', 'suspect', 'of', 'commit', 'offence', 'while', 'in', 'office', 'Those', 'offence', 'can', 'include', 'treason', 'bribery', 'or', 'other', 'high', 'crime', 'and', 'misdemeanour', 'After', 'someone', 'be', 'impeach', 'they', 'then', 'go', 'on', 'trial', 'in', 'the', 'Senate', 'the', 'upper', 'house', 'of', 'Congress', 'the', 'member', 'of', 'which', 'will', 'decide', 'whether', 'they', 'be', 'guilty', 'or', 'not', '-PRON-', 'have', 'a', 'political', 'trial', 'not', 'a', 'criminal', 'one']\n"
     ]
    }
   ],
   "source": [
    "# We can filter punctuation tokens: \n",
    "\n",
    "cleaned_words = [token.lemma_ for token in doc if not token.is_punct]\n",
    "print(f\"Cleaned words: {cleaned_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_sample = df.sample(100, random_state=13) # fix random_state to make your experiments reproducible "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create columns [spacy_lemmas], [spacy_tokens], [spacy_filtered_stop_words], [spacy_filtered_punct], [spacy_filtered_stop_punct]  \n",
    "In spacy_filtered_stop_punct filter stop words AND punctuation \n",
    "\n",
    "TIP: Use lambda functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_sample['spacy_tokens'] = df_sample['comment_text'].apply(lambda x: [t.text for t in nlp(x.replace('\\n', ' ').replace('\\r', ' '))])\n",
    "df_sample['spacy_lemmas'] = df_sample['comment_text'].apply(lambda x: [t.lemma_ for t in nlp(x.replace('\\n', ' ').replace('\\r', ' '))])\n",
    "df_sample['spacy_filtered_stop_words'] = df_sample['comment_text'].apply(lambda x: [t.text for t in nlp(x.replace('\\n', ' ').replace('\\r', ' ')) if not t.is_stop])\n",
    "df_sample['spacy_filtered_punct'] = df_sample['comment_text'].apply(lambda x: [t.text for t in nlp(x.replace('\\n', ' ').replace('\\r', ' ')) if not t.is_punct])\n",
    "df_sample['spacy_filtered_stop_punct'] = df_sample['comment_text'].apply(lambda x: [t.text for t in nlp(x.replace('\\n', ' ').replace('\\r', ' ')) if not (t.is_stop or t.is_punct)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_lower</th>\n",
       "      <th>comment_text_tokenized_space</th>\n",
       "      <th>comment_text_tokenized_space_cleaned</th>\n",
       "      <th>nltk_tokenized</th>\n",
       "      <th>spacy_lemmas</th>\n",
       "      <th>spacy_tokens</th>\n",
       "      <th>spacy_filtered_stop_words</th>\n",
       "      <th>spacy_filtered_punct</th>\n",
       "      <th>spacy_filtered_stop_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25680</th>\n",
       "      <td>43fb0a70fae5057f</td>\n",
       "      <td>(incorrect, moronic allegations of)</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(incorrect, moronic allegations of)</td>\n",
       "      <td>[(incorrect,, moronic, allegations, of)]</td>\n",
       "      <td>[incorrect, moronic, allegations, of]</td>\n",
       "      <td>[(, incorrect, ,, moronic, allegations, of, )]</td>\n",
       "      <td>[(, incorrect, ,, moronic, allegation, of, )]</td>\n",
       "      <td>[(, incorrect, ,, moronic, allegations, of, )]</td>\n",
       "      <td>[(, incorrect, ,, moronic, allegations, )]</td>\n",
       "      <td>[incorrect, moronic, allegations, of]</td>\n",
       "      <td>[incorrect, moronic, allegations]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74202</th>\n",
       "      <td>c688777f515b665a</td>\n",
       "      <td>As the previous article lead already used, dig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>as the previous article lead already used, dig...</td>\n",
       "      <td>[as, the, previous, article, lead, already, us...</td>\n",
       "      <td>[as, the, previous, article, lead, already, us...</td>\n",
       "      <td>[As, the, previous, article, lead, already, us...</td>\n",
       "      <td>[As, the, previous, article, lead, already, us...</td>\n",
       "      <td>[As, the, previous, article, lead, already, us...</td>\n",
       "      <td>[previous, article, lead, ,, digital, system, ...</td>\n",
       "      <td>[As, the, previous, article, lead, already, us...</td>\n",
       "      <td>[previous, article, lead, digital, system, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87912</th>\n",
       "      <td>eb233cfbf51fd72f</td>\n",
       "      <td>▲ to ? \\r\\n\\r\\n...character encoding issues. O...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>▲ to ? \\r\\n\\r\\n...character encoding issues. o...</td>\n",
       "      <td>[▲, to, ?, ...character, encoding, issues., oo...</td>\n",
       "      <td>[▲, to, , character, encoding, issues, oops, f...</td>\n",
       "      <td>[▲, to, ?, ..., character, encoding, issues, ....</td>\n",
       "      <td>[▲, to, ?,     , ..., character, encode, issue...</td>\n",
       "      <td>[▲, to, ?,     , ..., character, encoding, iss...</td>\n",
       "      <td>[▲, ?,     , ..., character, encoding, issues,...</td>\n",
       "      <td>[▲, to,     , character, encoding, issues, Oop...</td>\n",
       "      <td>[▲,     , character, encoding, issues, Oops, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130308</th>\n",
       "      <td>b920ddbf9499a110</td>\n",
       "      <td>Yes. I know that was what Mikka did. And you k...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes. i know that was what mikka did. and you k...</td>\n",
       "      <td>[yes., i, know, that, was, what, mikka, did., ...</td>\n",
       "      <td>[yes, i, know, that, was, what, mikka, did, an...</td>\n",
       "      <td>[Yes, ., I, know, that, was, what, Mikka, did,...</td>\n",
       "      <td>[Yes, ., I, know, that, be, what, Mikka, do, ....</td>\n",
       "      <td>[Yes, ., I, know, that, was, what, Mikka, did,...</td>\n",
       "      <td>[Yes, ., know, Mikka, ., know, :, S, /, right,...</td>\n",
       "      <td>[Yes, I, know, that, was, what, Mikka, did, An...</td>\n",
       "      <td>[Yes, know, Mikka, know, S, right, Surprised, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147189</th>\n",
       "      <td>386b62dc67bcb66c</td>\n",
       "      <td>Son of a bitchSon of a bitch</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>son of a bitchson of a bitch</td>\n",
       "      <td>[son, of, a, bitchson, of, a, bitch]</td>\n",
       "      <td>[son, of, a, bitchson, of, a, bitch]</td>\n",
       "      <td>[Son, of, a, bitchSon, of, a, bitch]</td>\n",
       "      <td>[Son, of, a, bitchSon, of, a, bitch]</td>\n",
       "      <td>[Son, of, a, bitchSon, of, a, bitch]</td>\n",
       "      <td>[Son, bitchSon, bitch]</td>\n",
       "      <td>[Son, of, a, bitchSon, of, a, bitch]</td>\n",
       "      <td>[Son, bitchSon, bitch]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "25680   43fb0a70fae5057f                (incorrect, moronic allegations of)   \n",
       "74202   c688777f515b665a  As the previous article lead already used, dig...   \n",
       "87912   eb233cfbf51fd72f  ▲ to ? \\r\\n\\r\\n...character encoding issues. O...   \n",
       "130308  b920ddbf9499a110  Yes. I know that was what Mikka did. And you k...   \n",
       "147189  386b62dc67bcb66c                       Son of a bitchSon of a bitch   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "25680       1             0        0       0       0              0   \n",
       "74202       0             0        0       0       0              0   \n",
       "87912       0             0        0       0       0              0   \n",
       "130308      0             0        0       0       0              0   \n",
       "147189      1             0        1       0       1              0   \n",
       "\n",
       "                                       comment_text_lower  \\\n",
       "25680                 (incorrect, moronic allegations of)   \n",
       "74202   as the previous article lead already used, dig...   \n",
       "87912   ▲ to ? \\r\\n\\r\\n...character encoding issues. o...   \n",
       "130308  yes. i know that was what mikka did. and you k...   \n",
       "147189                       son of a bitchson of a bitch   \n",
       "\n",
       "                             comment_text_tokenized_space  \\\n",
       "25680            [(incorrect,, moronic, allegations, of)]   \n",
       "74202   [as, the, previous, article, lead, already, us...   \n",
       "87912   [▲, to, ?, ...character, encoding, issues., oo...   \n",
       "130308  [yes., i, know, that, was, what, mikka, did., ...   \n",
       "147189               [son, of, a, bitchson, of, a, bitch]   \n",
       "\n",
       "                     comment_text_tokenized_space_cleaned  \\\n",
       "25680               [incorrect, moronic, allegations, of]   \n",
       "74202   [as, the, previous, article, lead, already, us...   \n",
       "87912   [▲, to, , character, encoding, issues, oops, f...   \n",
       "130308  [yes, i, know, that, was, what, mikka, did, an...   \n",
       "147189               [son, of, a, bitchson, of, a, bitch]   \n",
       "\n",
       "                                           nltk_tokenized  \\\n",
       "25680      [(, incorrect, ,, moronic, allegations, of, )]   \n",
       "74202   [As, the, previous, article, lead, already, us...   \n",
       "87912   [▲, to, ?, ..., character, encoding, issues, ....   \n",
       "130308  [Yes, ., I, know, that, was, what, Mikka, did,...   \n",
       "147189               [Son, of, a, bitchSon, of, a, bitch]   \n",
       "\n",
       "                                             spacy_lemmas  \\\n",
       "25680       [(, incorrect, ,, moronic, allegation, of, )]   \n",
       "74202   [As, the, previous, article, lead, already, us...   \n",
       "87912   [▲, to, ?,     , ..., character, encode, issue...   \n",
       "130308  [Yes, ., I, know, that, be, what, Mikka, do, ....   \n",
       "147189               [Son, of, a, bitchSon, of, a, bitch]   \n",
       "\n",
       "                                             spacy_tokens  \\\n",
       "25680      [(, incorrect, ,, moronic, allegations, of, )]   \n",
       "74202   [As, the, previous, article, lead, already, us...   \n",
       "87912   [▲, to, ?,     , ..., character, encoding, iss...   \n",
       "130308  [Yes, ., I, know, that, was, what, Mikka, did,...   \n",
       "147189               [Son, of, a, bitchSon, of, a, bitch]   \n",
       "\n",
       "                                spacy_filtered_stop_words  \\\n",
       "25680          [(, incorrect, ,, moronic, allegations, )]   \n",
       "74202   [previous, article, lead, ,, digital, system, ...   \n",
       "87912   [▲, ?,     , ..., character, encoding, issues,...   \n",
       "130308  [Yes, ., know, Mikka, ., know, :, S, /, right,...   \n",
       "147189                             [Son, bitchSon, bitch]   \n",
       "\n",
       "                                     spacy_filtered_punct  \\\n",
       "25680               [incorrect, moronic, allegations, of]   \n",
       "74202   [As, the, previous, article, lead, already, us...   \n",
       "87912   [▲, to,     , character, encoding, issues, Oop...   \n",
       "130308  [Yes, I, know, that, was, what, Mikka, did, An...   \n",
       "147189               [Son, of, a, bitchSon, of, a, bitch]   \n",
       "\n",
       "                                spacy_filtered_stop_punct  \n",
       "25680                   [incorrect, moronic, allegations]  \n",
       "74202   [previous, article, lead, digital, system, pro...  \n",
       "87912   [▲,     , character, encoding, issues, Oops, F...  \n",
       "130308  [Yes, know, Mikka, know, S, right, Surprised, ...  \n",
       "147189                             [Son, bitchSon, bitch]  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Save the results (df and df_sample) in csv file using df.to_csv function. Share your csv files using google drive or email. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df.csv')\n",
    "df.to_csv('df_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud visualizations \n",
    "\n",
    "Create wordclouds for words cleaned from stop words and punctuation using NLTK library - as in previous task. \n",
    "(with spacy it would work slow, so do not apply it to the whole dataset, use only df_sample part) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms visualizations\n",
    "\n",
    "Create histograms of words frequency or counts for tokens cleaned from stop words and punctuation as in previous day task.  \n",
    "Compare the newly created visualizations to the visualisations from the previous day. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word counts plot \n",
    "\n",
    "Complete the plot as we did previously using FreqDict, but make plot larger (see how to set the plots size) and show 50 most common tokens withing the label and 50 most unfrequent. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

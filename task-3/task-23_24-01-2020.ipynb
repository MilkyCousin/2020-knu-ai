{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"task-23_24-01-2020.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"-9CahILYtutU","colab_type":"text"},"source":["## Prerequisites\n","\n"]},{"cell_type":"code","metadata":{"id":"mDnVxoOotutZ","colab_type":"code","colab":{}},"source":["import pandas as pd \n","import numpy as np \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_qwjnztRtutj","colab_type":"text"},"source":["### Note! Some of these models support only multiclass classification, please, while selecting your dataset,  \n","### be sure that for algorithms which does not support multilabel classification you use only examples with only one label. \n","### Examples without a label in any of the provided categories are clean messages, without any toxicity."]},{"cell_type":"code","metadata":{"id":"KXN9vr5Qtutm","colab_type":"code","outputId":"906ce5fb-82fc-48ed-f274-9b5123334d6c","executionInfo":{"status":"ok","timestamp":1580678048781,"user_tz":-120,"elapsed":2329,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","df = pd.read_csv(\"/content/drive/My Drive/train.csv\")"],"execution_count":114,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WUV5VkZDtuts","colab_type":"code","outputId":"78c2918a-3a54-417c-e74d-fff53a6867d9","executionInfo":{"status":"ok","timestamp":1580678051114,"user_tz":-120,"elapsed":663,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["df.head()"],"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id  ... identity_hate\n","0  0000997932d777bf  ...             0\n","1  000103f0d9cfb60f  ...             0\n","2  000113f07ec002fd  ...             0\n","3  0001b41b1c6bb37e  ...             0\n","4  0001d958c54c6e35  ...             0\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"code","metadata":{"id":"oXox1gXgtuty","colab_type":"code","outputId":"f6ab09f3-afd8-49f4-f8fe-0ad61e7bd9e3","executionInfo":{"status":"ok","timestamp":1580678054040,"user_tz":-120,"elapsed":748,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["df.shape"],"execution_count":116,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(159571, 8)"]},"metadata":{"tags":[]},"execution_count":116}]},{"cell_type":"markdown","metadata":{"id":"32zY7m7ctut4","colab_type":"text"},"source":["### As one of the methods to make the training simpier, use only examples, assigned to any category vs clean examples.  \n","For example:  \n","- Select only messages with obscene label == 1  \n","- Select all of the \"clean\" messages  \n","Implement a model which can perform a binary classification  - to understand whether your message is obscene or not.   "]},{"cell_type":"markdown","metadata":{"id":"5X_M1skLtut6","colab_type":"text"},"source":["##### If you want to perform a multilabel classification, please understand the difference between multilabel and multiclass classification and be sure that you are solving the correct task - choose only algorithms applicable for solving this type of problem."]},{"cell_type":"markdown","metadata":{"id":"Ey6dLTV8tut8","colab_type":"text"},"source":["#### To work with multiclass task:  \n","You only need to select messages which have only one label assigned: message cannot be assigned to 2 or more categories.  \n","\n","#### To work with multilabel task: \n","You can work with the whole dataset - some of your messages have only 1 label, some more than 1. "]},{"cell_type":"markdown","metadata":{"id":"w5eOUtAVtut9","colab_type":"text"},"source":["## Text vectorization"]},{"cell_type":"markdown","metadata":{"id":"ZErVlZ0YtuuA","colab_type":"text"},"source":["Previously we worked only with words vectorization. But we need to have a vector for each text, not only words from it. \n","\n","Before starting a text vectorization, please, make sure you are working with clean data - use the dataset created on the previous day. Cleaned from punctuation, stop words, lemmatized or stemmed, etc. "]},{"cell_type":"code","metadata":{"id":"41OIMTIjtuuD","colab_type":"code","outputId":"98960a9d-072a-469d-a87c-fb0a7f48c375","executionInfo":{"status":"ok","timestamp":1580678059220,"user_tz":-120,"elapsed":827,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["from string import punctuation\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer \n","import nltk\n","\n","nltk.download('stopwords')\n","  \n","lemmatizer = WordNetLemmatizer() \n","stop_words = set(stopwords.words('english'))"],"execution_count":117,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RVmVdsrptuuK","colab_type":"code","outputId":"30d3fb42-f049-47d2-aef9-95afbfe24451","executionInfo":{"status":"ok","timestamp":1580678222108,"user_tz":-120,"elapsed":162169,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["import nltk\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","def preprocess_text(tokenizer, lemmatizer, stop_words, punctuation, text): \n","    tokens = tokenizer(text.lower())\n","    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n","    return [token for token in lemmas if token not in stop_words and token not in punctuation]\n","\n","df['cleaned'] = df.comment_text.apply(lambda x: preprocess_text(word_tokenize, lemmatizer, stop_words, punctuation, x))"],"execution_count":118,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QrzHVQVYtuuP","colab_type":"code","outputId":"15f470eb-f459-47b7-b7e3-a2bea156f3bf","executionInfo":{"status":"ok","timestamp":1580678222112,"user_tz":-120,"elapsed":156677,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":195}},"source":["df.head()"],"execution_count":119,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>cleaned</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[explanation, edits, made, username, hardcore,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[d'aww, match, background, colour, 'm, seeming...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[hey, man, 'm, really, trying, edit, war, 's, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[``, ca, n't, make, real, suggestion, improvem...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[sir, hero, chance, remember, page, 's]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id  ...                                            cleaned\n","0  0000997932d777bf  ...  [explanation, edits, made, username, hardcore,...\n","1  000103f0d9cfb60f  ...  [d'aww, match, background, colour, 'm, seeming...\n","2  000113f07ec002fd  ...  [hey, man, 'm, really, trying, edit, war, 's, ...\n","3  0001b41b1c6bb37e  ...  [``, ca, n't, make, real, suggestion, improvem...\n","4  0001d958c54c6e35  ...            [sir, hero, chance, remember, page, 's]\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":119}]},{"cell_type":"code","metadata":{"id":"POMc1ofQtuuU","colab_type":"code","colab":{}},"source":["def flat_nested(nested):\n","    flatten = []\n","    for item in nested:\n","        if isinstance(item, list):\n","            flatten.extend(item)\n","        else:\n","            flatten.append(item)\n","    return flatten"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dUmZTXMktuua","colab_type":"code","colab":{}},"source":["vocab = set(flat_nested(df.cleaned.tolist()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"klafsszstuuf","colab_type":"code","outputId":"9029acb2-0d73-41db-ec40-8f9f7dfe7482","executionInfo":{"status":"ok","timestamp":1580678223105,"user_tz":-120,"elapsed":152063,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(vocab)"],"execution_count":122,"outputs":[{"output_type":"execute_result","data":{"text/plain":["249736"]},"metadata":{"tags":[]},"execution_count":122}]},{"cell_type":"markdown","metadata":{"id":"NFy9l1tmtuuk","colab_type":"text"},"source":["As we see, probably you vocabulary is too large.  \n","Let's try to make it smaller.  \n","For example, let's get rig of words, which has counts in our dataset less than some threshold."]},{"cell_type":"code","metadata":{"id":"5alGUlhVtuul","colab_type":"code","colab":{}},"source":["from collections import Counter, defaultdict \n","\n","cnt_vocab = Counter(flat_nested(df.cleaned.tolist()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhNQ037Ntuur","colab_type":"code","outputId":"2a3d2222-cdc6-493a-c6fb-d93669c3652f","executionInfo":{"status":"ok","timestamp":1580678224753,"user_tz":-120,"elapsed":151335,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["cnt_vocab.most_common(10)"],"execution_count":124,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(\"''\", 242528),\n"," ('``', 155370),\n"," ('article', 73284),\n"," (\"'s\", 66767),\n"," (\"n't\", 57144),\n"," ('wa', 56592),\n"," ('page', 56263),\n"," ('wikipedia', 45418),\n"," ('talk', 35356),\n"," ('ha', 31896)]"]},"metadata":{"tags":[]},"execution_count":124}]},{"cell_type":"markdown","metadata":{"id":"_9y4WUh7tuuw","colab_type":"text"},"source":["You can clean words which are shorter that particular length and occur less than N times. "]},{"cell_type":"code","metadata":{"id":"5ahC3DYQtuuy","colab_type":"code","colab":{}},"source":["threshold_count = 10\n","threshold_len = 4 \n","cleaned_vocab = [token for token, count in cnt_vocab.items() if count > threshold_count and len(token) > threshold_len]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Sxz9IEvtuu4","colab_type":"code","outputId":"39e5b164-cc60-4e29-f13b-39156bfb02e9","executionInfo":{"status":"ok","timestamp":1580678224760,"user_tz":-120,"elapsed":147152,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(cleaned_vocab)"],"execution_count":126,"outputs":[{"output_type":"execute_result","data":{"text/plain":["18696"]},"metadata":{"tags":[]},"execution_count":126}]},{"cell_type":"markdown","metadata":{"id":"O3Ef10Dgtuu-","colab_type":"text"},"source":["Much better!  \n","Let's try to vectorize the text summing one-hot vectors for each word. "]},{"cell_type":"code","metadata":{"id":"yHzNnwfttuu_","colab_type":"code","colab":{}},"source":["vocabulary = defaultdict()\n","\n","for i, token in enumerate(cleaned_vocab): \n","    empty_vec = np.zeros(len(cleaned_vocab))\n","    empty_vec[i] = 1 \n","    vocabulary[token] = empty_vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nq5MkzAytuvE","colab_type":"code","outputId":"93b4fbac-e745-49a0-a5b8-88547fcc563f","executionInfo":{"status":"ok","timestamp":1580678229679,"user_tz":-120,"elapsed":148205,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["vocabulary['source']"],"execution_count":128,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., ..., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":128}]},{"cell_type":"markdown","metadata":{"id":"gJOpb7nTtuvI","colab_type":"text"},"source":["Rigth now we have vectors for words (words are one-hot vectorized)  \n","Let's try to create vectors for texts: "]},{"cell_type":"code","metadata":{"id":"YrPV6MM0tuvK","colab_type":"code","outputId":"4eca3879-b61f-4fd9-ee10-08916a2d72cf","executionInfo":{"status":"ok","timestamp":1580678229682,"user_tz":-120,"elapsed":146810,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["sample_text = df.cleaned[10]\n","print(sample_text)"],"execution_count":129,"outputs":[{"output_type":"stream","text":["['``', 'fair', 'use', 'rationale', 'image', 'wonju.jpg', 'thanks', 'uploading', 'image', 'wonju.jpg', 'notice', 'image', 'page', 'specifies', 'image', 'used', 'fair', 'use', 'explanation', 'rationale', 'use', 'wikipedia', 'article', 'constitutes', 'fair', 'use', 'addition', 'boilerplate', 'fair', 'use', 'template', 'must', 'also', 'write', 'image', 'description', 'page', 'specific', 'explanation', 'rationale', 'using', 'image', 'article', 'consistent', 'fair', 'use', 'please', 'go', 'image', 'description', 'page', 'edit', 'include', 'fair', 'use', 'rationale', 'uploaded', 'fair', 'use', 'medium', 'consider', 'checking', 'specified', 'fair', 'use', 'rationale', 'page', 'find', 'list', \"'image\", 'page', 'edited', 'clicking', '``', \"''\", 'contribution', \"''\", \"''\", 'link', 'located', 'top', 'wikipedia', 'page', 'logged', 'selecting', '``', \"''\", 'image', \"''\", \"''\", 'dropdown', 'box', 'note', 'fair', 'use', 'image', 'uploaded', '4', 'may', '2006', 'lacking', 'explanation', 'deleted', 'one', 'week', 'uploaded', 'described', 'criterion', 'speedy', 'deletion', 'question', 'please', 'ask', 'medium', 'copyright', 'question', 'page', 'thank', 'talk', '•', 'contribs', '•', 'unspecified', 'source', 'image', 'wonju.jpg', 'thanks', 'uploading', 'image', 'wonju.jpg', 'noticed', 'file', \"'s\", 'description', 'page', 'currently', 'doe', \"n't\", 'specify', 'created', 'content', 'copyright', 'status', 'unclear', 'create', 'file', 'need', 'specify', 'owner', 'copyright', 'obtained', 'website', 'link', 'website', 'wa', 'taken', 'together', 'restatement', 'website', \"'s\", 'term', 'use', 'content', 'usually', 'sufficient', 'information', 'however', 'copyright', 'holder', 'different', 'website', \"'s\", 'publisher', 'copyright', 'also', 'acknowledged', 'well', 'adding', 'source', 'please', 'add', 'proper', 'copyright', 'licensing', 'tag', 'file', 'doe', \"n't\", 'one', 'already', 'created/took', 'picture', 'audio', 'video', 'tag', 'used', 'release', 'gfdl', 'believe', 'medium', 'meet', 'criterion', 'wikipedia', 'fair', 'use', 'use', 'tag', 'one', 'tag', 'listed', 'wikipedia', 'image', 'copyright', 'tag', 'fair', 'use', 'see', 'wikipedia', 'image', 'copyright', 'tag', 'full', 'list', 'copyright', 'tag', 'use', 'uploaded', 'file', 'consider', 'checking', 'specified', 'source', 'tagged', 'find', 'list', 'file', 'uploaded', 'following', 'link', 'unsourced', 'untagged', 'image', 'may', 'deleted', 'one', 'week', 'tagged', 'described', 'criterion', 'speedy', 'deletion', 'image', 'copyrighted', 'non-free', 'license', 'per', 'wikipedia', 'fair', 'use', 'image', 'deleted', '48', 'hour', 'question', 'please', 'ask', 'medium', 'copyright', 'question', 'page', 'thank', 'talk', '•', 'contribs', '•', '``']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eni7KIdNtuvS","colab_type":"text"},"source":["### One-hot vectorization and count vectorization"]},{"cell_type":"code","metadata":{"id":"XYH36vCUtuvU","colab_type":"code","colab":{}},"source":["sample_vector = np.zeros(len(cleaned_vocab))\n","\n","for token in sample_text: \n","    try: \n","        sample_vector += vocabulary[token]\n","    except KeyError: \n","        continue"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WMFHOc0ituvY","colab_type":"code","outputId":"c4953ca0-978b-402d-f138-62b5db2f4a5b","executionInfo":{"status":"ok","timestamp":1580678229687,"user_tz":-120,"elapsed":141057,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sample_vector"],"execution_count":131,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3., 0., 0., ..., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":131}]},{"cell_type":"markdown","metadata":{"id":"yvCKSQFOtuvd","colab_type":"text"},"source":["Right now we have count vectorization for our text.   \n","Use this pipeline to create vectors for all of the texts. Save them into np.array. i-th raw in np.array is a vector which represents i-th text from the dataframe.  "]},{"cell_type":"code","metadata":{"id":"AlAsyOe-tuvf","colab_type":"code","outputId":"3ee01204-b787-4206-d02c-8247a9a53888","executionInfo":{"status":"ok","timestamp":1580678230300,"user_tz":-120,"elapsed":137948,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["matrix_size = 1000\n","df_sample = df.cleaned.sample(matrix_size)\n","\n","def vect(size, dataset):\n","  c = 0\n","  text_matrix = np.zeros((size, len(cleaned_vocab)))\n","  for i in dataset:\n","    comment_matrix = np.zeros(len(cleaned_vocab))\n","    for token in i:\n","      try:\n","        comment_matrix = comment_matrix + vocabulary[token]\n","      except KeyError:\n","        continue\n","    text_matrix[c] = comment_matrix\n","    c = c + 1\n","  return text_matrix\n","\n","vectorized_text = vect(matrix_size, df_sample)\n","print(vectorized_text)"],"execution_count":132,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9TOmnJvatuvj","colab_type":"text"},"source":["### The next step is to train any classification model on top of the received vectors and report the quality. "]},{"cell_type":"markdown","metadata":{"id":"4lXwO_gTtuvk","colab_type":"text"},"source":["Please, select any of the proposed pipelines for performing a text classification task. (Binary, multiclass or multilabel).  "]},{"cell_type":"markdown","metadata":{"id":"X-GxyBWrtuvm","colab_type":"text"},"source":["The main task to calculate our models performance is to create a training and test sets. When you selected a texts for your task, please, use https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html to have at least two sets - train and test.  \n","\n","Train examples you will use to train your model on and test examples to evaluate your model - to understand how your model works on the unseen data. "]},{"cell_type":"markdown","metadata":{"id":"7MenGZKTtuvn","colab_type":"text"},"source":["### Train-test split "]},{"cell_type":"code","metadata":{"id":"wi9k_n8_tuvo","colab_type":"code","outputId":"67e2c1e1-49bf-4f6c-c46c-ab48aff3e9e8","executionInfo":{"status":"ok","timestamp":1580678768796,"user_tz":-120,"elapsed":656,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":554}},"source":["from sklearn.model_selection import train_test_split\n","\n","df_small = df[:2000]\n","test_data, train_data = train_test_split(df_small, random_state = 50, test_size = 0.5)\n","\n","\n","print('-------------------------------------test_data-------------------------------------','\\n',test_data,'\\n\\n')\n","print('-------------------------------------train_data-------------------------------------','\\n',train_data)"],"execution_count":147,"outputs":[{"output_type":"stream","text":["-------------------------------------test_data------------------------------------- \n","                     id  ...                                            cleaned\n","838   024859995cf6379e  ...  [thank, experimenting, wikipedia, test, worked...\n","1492  0400b22cadd183f4  ...  [external, link, 've, added, official, bobby, ...\n","515   01588a3bc3e2755d  ...  [``, =^_^=, watched, entire, series, agree, en...\n","1357  03b0df8e2f79ad61  ...  [understood, term, art, literal, enunciation, ...\n","1573  0439882932f5f3ca  ...  [``, muhammad, view, slavery, article, un-bala...\n","...                ...  ...                                                ...\n","1313  0392de34030477e6  ...  [``, agree, actual, usage, ``, '', russia, '',...\n","109   004a23742282fee4  ...  [welcome, wikipedia, bla, discover, ekopedia, ...\n","1931  053ceb31fc1a90a8  ...  [comment, password, hacking, really, n't, thin...\n","1504  040a95741d7c8d91  ...  [non-involved, people, definitely, understand,...\n","1712  04a67e3c97228690  ...  [``, oops, misunderstood, *blushes*, silly, n'...\n","\n","[1000 rows x 9 columns] \n","\n","\n","-------------------------------------train_data------------------------------------- \n","                     id  ...                                            cleaned\n","565   017dac2063a30a1a  ...  [vince, fyi, 'yellowfrogs, longdendale, counci...\n","1223  0349f1678a6d4c11  ...  [``, ==, oldlady, production, ==, history, old...\n","1581  043e7d852e8e64c5  ...  [modifying, content, section, way, allready, d...\n","959   02a276b30cb6ec23  ...  [``, read, beback, already, changed, structure...\n","1974  05566fed20751c92  ...  [thanks, note, richard, knew, wa, road, bridge...\n","...                ...  ...                                                ...\n","1056  02e287c1928ce29c  ...  [``, 're, offended, birther, analogy, consider...\n","1053  02e07f0130a68f2a  ...  [comment, wa, reverted, unable, see, n't, worr...\n","714   01ef0f942d024cf5  ...  [dar, ok, bear, penelopism, religion, 's, new,...\n","1971  055553141f143980  ...  [believe, 've, made, block, error, request, un...\n","1199  033d78bf5ab10000  ...  [``, justin, vaisse, 's, opinion, relative, ab...\n","\n","[1000 rows x 9 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V3PcJ94uxVmu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218},"outputId":"e9aa1240-9d56-442b-a478-69aab44e4705","executionInfo":{"status":"ok","timestamp":1580678738421,"user_tz":-120,"elapsed":817,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["test_data['cleaned']"],"execution_count":145,"outputs":[{"output_type":"execute_result","data":{"text/plain":["493    [go, away, revert, edit, blowdart, 's, user, t...\n","322    [total, population, 10, million, doe, match, e...\n","372    [upgraded, human, language, family, wikicolors...\n","735    [pkk, fighter, past, decade, entering, living,...\n","680    [procedure, ask, award, michelle, obama, alrea...\n","                             ...                        \n","289    ['m, focussing, science, moment, leading, revo...\n","109    [welcome, wikipedia, bla, discover, ekopedia, ...\n","907    [``, trying, measure, number, cafe, one, place...\n","480    [mark, ii, subsec, removed, mark, iix, subsect...\n","688    [``, past, master, vs., rarity, 'm, creating, ...\n","Name: cleaned, Length: 500, dtype: object"]},"metadata":{"tags":[]},"execution_count":145}]},{"cell_type":"markdown","metadata":{"id":"FCAOMUYGtuvt","colab_type":"text"},"source":["### TF-IDF score "]},{"cell_type":"markdown","metadata":{"id":"_05enO31tuvv","colab_type":"text"},"source":["#### Please, review again this article or read it if you have not done it before. \n","\n","https://medium.com/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7"]},{"cell_type":"markdown","metadata":{"id":"FBZK9-MNtuvw","colab_type":"text"},"source":["#### Implement calculating a tf-idf score for each of the words from your vocabulary. \n","\n","The main goal of this taks is to create a dictionary - keys of the dictionary would be tokens and values would be corresponding tf-idf score of the token.\n","\n","#### Calculate it MANUALLY and compare the received scores for words with the sklearn implementation:  \n","from sklearn.feature_extraction.text import TfidfTransformer "]},{"cell_type":"markdown","metadata":{"id":"ogAbktZhtuvx","colab_type":"text"},"source":["#### Tip: \n","\n","##### TF = (Number of time the word occurs in the current text) / (Total number of words in the current text)  \n","\n","##### IDF = (Total number of documents / Number of documents with word t in it)\n","\n","##### TF-IDF = TF*IDF "]},{"cell_type":"markdown","metadata":{"id":"gb_S3mfhtuvy","colab_type":"text"},"source":["When you calculated a tf-idf score for each of the words in your vocabulary - revectorize the texts.  \n","Instead of using number of occurences of the i-th word in the i-th cell of the text vector, use it's tf-idf score.   \n","\n","Revectorize the documents, save vectors into np.array. "]},{"cell_type":"code","metadata":{"id":"miUQQ6zItuv0","colab_type":"code","outputId":"e1e2b54d-d630-4f49-9e29-5e1bbdced15f","executionInfo":{"status":"ok","timestamp":1580678795762,"user_tz":-120,"elapsed":19810,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["def tfscore(word, text, c): #(Number of time the word occurs in the current text) / (Total number of words in the current text)\n","  occurs = 0\n","  words = text.split()\n","  for i in words:\n","    if i == word:\n","      occurs = occurs + 1\n","    if occurs != 0:\n","      c = c + 1\n","  tf = occurs / len(words)\n","  return tf, c\n","\n","def tfidf(df, documents): #((Number of time the word occurs ...) / (Total number of words...))*((Total number of documents / Number of documents with word t in it))\n","  word_row = []\n","  for word in documents:\n","    c = 0\n","    word_column = []\n","    for comment_text in df['comment_text']:\n","      probability, c = tfscore(word, comment_text, c)\n","      word_column.append(probability)\n","    mult = c/len(documents)\n","    word_row.append(np.array(word_column) * mult)\n","  return np.array(word_row)\n","\n","docs = []\n","for j in cnt_vocab.most_common(1000):\n","  docs.append(j[0])\n","result = tfidf(df_small, docs)\n","print(result)"],"execution_count":148,"outputs":[{"output_type":"stream","text":["[[0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.26109091 0.         0.        ]\n"," ...\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6LINxjZAtuv5","colab_type":"text"},"source":["### Training the model "]},{"cell_type":"markdown","metadata":{"id":"Txst18wDtuv7","colab_type":"text"},"source":["As it was said before, select any of the text classification models for the selected task and train the model. \n","\n","When the model is trained, you need to evaluate it somehow. \n","\n","Read about True positive, False positive, False negative and True negative counts and how to calculate them:   \n","\n","https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative \n","\n","##### Calculate TP, FP, FN and TN on the test set for your model to measure its performance. \n"]},{"cell_type":"code","metadata":{"id":"vkaovd6ctuwD","colab_type":"code","outputId":"44e8ee8d-7bb0-43e6-8216-e0e27f47e8a3","executionInfo":{"status":"ok","timestamp":1580678801519,"user_tz":-120,"elapsed":1015,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import confusion_matrix\n","\n","model = KNeighborsClassifier()\n","tfidf = TfidfVectorizer(vocabulary = cleaned_vocab)\n","x = tfidf.fit_transform(train_data.cleaned.str.join(' '))\n","x_test = tfidf.fit_transform(test_data.cleaned.str.join(' '))\n","y = train_data.drop(columns = ['id','comment_text','cleaned']).values\n","y_test = test_data.drop(columns = ['id','comment_text','cleaned']).values\n","model.fit(x,y)\n","prediction = model.predict(x_test)\n","\n","TP = np.sum(np.logical_and(prediction == 1, y_test == 1)) #True positive\n","FP = np.sum(np.logical_and(prediction == 1, y_test == 0)) #False positive\n","TN = np.sum(np.logical_and(prediction == 0, y_test == 0)) #True negative\n","FN = np.sum(np.logical_and(prediction == 0, y_test == 1)) #False negative\n","\n","print(TP, FP, TN, FN)"],"execution_count":149,"outputs":[{"output_type":"stream","text":["5 43 5743 209\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tdidufVgtuwI","colab_type":"text"},"source":["#### The next step is to calculate  Precision, Recall, F1 and F2 score \n","\n","https://en.wikipedia.org/wiki/Sensitivity_and_specificity"]},{"cell_type":"code","metadata":{"id":"HWLfkYFBtuwJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a33fa8e4-1ce6-44b9-e453-10a52392288e","executionInfo":{"status":"ok","timestamp":1580678837329,"user_tz":-120,"elapsed":742,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["prec = TP/(FP+TP)\n","rec = TP/(TP+FN)\n","F1 = (2*TP)/(2*TP + FP + FN)\n","F2 = (5*prec*rec)/(4*prec + rec)\n","print(prec, rec, F1, F2)"],"execution_count":151,"outputs":[{"output_type":"stream","text":["0.10416666666666667 0.02336448598130841 0.03816793893129771 0.027654867256637166\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mfi-TnwktuwP","colab_type":"text"},"source":["Calculate these metrics for the vectorization created using count vectorizing and for tf-idf vectorization.  \n","Compare them. "]},{"cell_type":"markdown","metadata":{"id":"dBHyCC9ftuwQ","colab_type":"text"},"source":["### Conclusions and improvements "]},{"cell_type":"markdown","metadata":{"id":"3FmxtPybtuwS","colab_type":"text"},"source":["For all of the vectorization pipelines we used all of the words, which were available in our dictionary, as experiment try to use the most meaningful words - select them using TF-IDF score. (for example for each text you can select not more than 10 words for vectorization, or less). \n","\n","Compare this approach with the first and second ones. Did your model improve? \n","\n"]},{"cell_type":"markdown","metadata":{"id":"UTOsl0lgtuwT","colab_type":"text"},"source":["### Additionally, visualisations "]},{"cell_type":"markdown","metadata":{"id":"uGt0aq96tuwU","colab_type":"text"},"source":["For now you have a vector for each word from your vocabulary. \n","You have vectors with lenght > 18000, so the dimension of your space is more than 18000 - it's impossible to visualise it in 2d space. \n","\n","So try to research and look for algorithms which perform dimensionality reduction. (t-SNE, PCA) \n","Try to visualise obtained vectors in a vectorspace, only subset from the vocabulary, don't plot all of the words. (100) \n","\n","Probably on this step you will realise how this type of vectorization using these techniques is not the best way to vectorize words. \n","\n","Please, analyse the obtained results and explain why visualisation looks like this. "]}]}
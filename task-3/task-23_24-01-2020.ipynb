{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note! Some of these models support only multiclass classification, please, while selecting your dataset,  \n",
    "### be sure that for algorithms which does not support multilabel classification you use only examples with only one label. \n",
    "### Examples without a label in any of the provided categories are clean messages, without any toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As one of the methods to make the training simpier, use only examples, assigned to any category vs clean examples.  \n",
    "For example:  \n",
    "- Select only messages with obscene label == 1  \n",
    "- Select all of the \"clean\" messages  \n",
    "Implement a model which can perform a binary classification  - to understand whether your message is obscene or not.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If you want to perform a multilabel classification, please understand the difference between multilabel and multiclass classification and be sure that you are solving the correct task - choose only algorithms applicable for solving this type of problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To work with multiclass task:  \n",
    "You only need to select messages which have only one label assigned: message cannot be assigned to 2 or more categories.  \n",
    "\n",
    "#### To work with multilabel task: \n",
    "You can work with the whole dataset - some of your messages have only 1 label, some more than 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we worked only with words vectorization. But we need to have a vector for each text, not only words from it. \n",
    "\n",
    "Before starting a text vectorization, please, make sure you are working with clean data - use the dataset created on the previous day. Cleaned from punctuation, stop words, lemmatized or stemmed, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#from replacers import\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tokenizer, lemmatizer, stop_words, punctuation, text): # cleaned words that \n",
    "    tokens = tokenizer(text.lower())\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return [token for token in lemmas if token not in stop_words and token not in punctuation and token]\n",
    "\n",
    "df['cleaned'] = df.comment_text.apply(lambda x: preprocess_text(word_tokenize, lemmatizer, stop_words, punctuation, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[d'aww, match, background, colour, 'm, seeming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, 'm, really, trying, edit, war, 's, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[``, ca, n't, make, real, suggestion, improvem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page, 's]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                             cleaned  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [d'aww, match, background, colour, 'm, seeming...  \n",
       "2  [hey, man, 'm, really, trying, edit, war, 's, ...  \n",
       "3  [``, ca, n't, make, real, suggestion, improvem...  \n",
       "4            [sir, hero, chance, remember, page, 's]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_nested(nested):\n",
    "    flatten = []\n",
    "    for item in nested:\n",
    "        if isinstance(item, list):\n",
    "            flatten.extend(item)\n",
    "        else:\n",
    "            flatten.append(item)\n",
    "    return flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(flat_nested(df.cleaned.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249531"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, probably you vocabulary is too large.  \n",
    "Let's try to make it smaller.  \n",
    "For example, let's get rig of words, which has counts in our dataset less than some threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict \n",
    "\n",
    "cnt_vocab = Counter(flat_nested(df.cleaned.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"''\", 241319),\n",
       " ('``', 156982),\n",
       " ('article', 73264),\n",
       " (\"'s\", 66766),\n",
       " (\"n't\", 57144),\n",
       " ('wa', 56590),\n",
       " ('page', 56239),\n",
       " ('wikipedia', 45413),\n",
       " ('talk', 35356),\n",
       " ('ha', 31896)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vocab.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clean words which are shorter that particular length and occur less than N times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_count = 10\n",
    "threshold_len = 4 \n",
    "cleaned_vocab = [token for token, count in cnt_vocab.items() if count > threshold_count and len(token) > threshold_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18705"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!  \n",
    "Let's try to vectorize the text summing one-hot vectors for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = defaultdict()\n",
    "\n",
    "for i, token in enumerate(cleaned_vocab): \n",
    "    empty_vec = np.zeros(len(cleaned_vocab))\n",
    "    empty_vec[i] = 1 \n",
    "    vocabulary[token] = empty_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rigth now we have vectors for words (words are one-hot vectorized)  \n",
    "Let's try to create vectors for texts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'fair', 'use', 'rationale', 'image', 'wonju.jpg', 'thanks', 'uploading', 'image', 'wonju.jpg', 'notice', 'image', 'page', 'specifies', 'image', 'used', 'fair', 'use', 'explanation', 'rationale', 'use', 'wikipedia', 'article', 'constitutes', 'fair', 'use', 'addition', 'boilerplate', 'fair', 'use', 'template', 'must', 'also', 'write', 'image', 'description', 'page', 'specific', 'explanation', 'rationale', 'using', 'image', 'article', 'consistent', 'fair', 'use', 'please', 'go', 'image', 'description', 'page', 'edit', 'include', 'fair', 'use', 'rationale', 'uploaded', 'fair', 'use', 'medium', 'consider', 'checking', 'specified', 'fair', 'use', 'rationale', 'page', 'find', 'list', \"'image\", 'page', 'edited', 'clicking', '``', \"''\", 'contribution', \"''\", \"''\", 'link', 'located', 'top', 'wikipedia', 'page', 'logged', 'selecting', '``', \"''\", 'image', \"''\", \"''\", 'dropdown', 'box', 'note', 'fair', 'use', 'image', 'uploaded', '4', 'may', '2006', 'lacking', 'explanation', 'deleted', 'one', 'week', 'uploaded', 'described', 'criterion', 'speedy', 'deletion', 'question', 'please', 'ask', 'medium', 'copyright', 'question', 'page', 'thank', 'talk', '•', 'contribs', '•', 'unspecified', 'source', 'image', 'wonju.jpg', 'thanks', 'uploading', 'image', 'wonju.jpg', 'noticed', 'file', \"'s\", 'description', 'page', 'currently', 'doe', \"n't\", 'specify', 'created', 'content', 'copyright', 'status', 'unclear', 'create', 'file', 'need', 'specify', 'owner', 'copyright', 'obtained', 'website', 'link', 'website', 'wa', 'taken', 'together', 'restatement', 'website', \"'s\", 'term', 'use', 'content', 'usually', 'sufficient', 'information', 'however', 'copyright', 'holder', 'different', 'website', \"'s\", 'publisher', 'copyright', 'also', 'acknowledged', 'well', 'adding', 'source', 'please', 'add', 'proper', 'copyright', 'licensing', 'tag', 'file', 'doe', \"n't\", 'one', 'already', 'created/took', 'picture', 'audio', 'video', 'tag', 'used', 'release', 'gfdl', 'believe', 'medium', 'meet', 'criterion', 'wikipedia', 'fair', 'use', 'use', 'tag', 'one', 'tag', 'listed', 'wikipedia', 'image', 'copyright', 'tag', 'fair', 'use', 'see', 'wikipedia', 'image', 'copyright', 'tag', 'full', 'list', 'copyright', 'tag', 'use', 'uploaded', 'file', 'consider', 'checking', 'specified', 'source', 'tagged', 'find', 'list', 'file', 'uploaded', 'following', 'link', 'unsourced', 'untagged', 'image', 'may', 'deleted', 'one', 'week', 'tagged', 'described', 'criterion', 'speedy', 'deletion', 'image', 'copyrighted', 'non-free', 'license', 'per', 'wikipedia', 'fair', 'use', 'image', 'deleted', '48', 'hour', 'question', 'please', 'ask', 'medium', 'copyright', 'question', 'page', 'thank', 'talk', '•', 'contribs', '•', '``']\n"
     ]
    }
   ],
   "source": [
    "sample_text = df.cleaned[10]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot vectorization and count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vector = np.zeros(len(cleaned_vocab))\n",
    "\n",
    "for token in sample_text: \n",
    "    try: \n",
    "        sample_vector += vocabulary[token]\n",
    "    except KeyError: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have count vectorization for our text.   \n",
    "Use this pipeline to create vectors for all of the texts. Save them into np.array. i-th raw in np.array is a vector which represents i-th text from the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['cleaned_cleaned'] = df.cleaned.apply(lambda x: [w for w in x if cnt_vocab[w] > threshold_count and len(w)>threshold_len])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([1., 1., 1., ..., 0., 0., 0.]),\n",
       "       array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "       array([0., 1., 0., ..., 0., 0., 0.]), ...,\n",
       "       array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "       array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "       array([0., 0., 0., ..., 0., 0., 0.])], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Your code here\n",
    "def vectorize(x):\n",
    "    vector = np.zeros(len(cleaned_vocab))\n",
    "    \n",
    "    for token in x: \n",
    "        try: \n",
    "            vector += vocabulary[token]\n",
    "        except KeyError: \n",
    "            continue\n",
    "    return vector\n",
    "\n",
    "text_vectorize = np.array(df.cleaned.apply(lambda x: vectorize(x)))\n",
    "text_vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to train any classification model on top of the received vectors and report the quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, select any of the proposed pipelines for performing a text classification task. (Binary, multiclass or multilabel).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main task to calculate our models performance is to create a training and test sets. When you selected a texts for your task, please, use https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html to have at least two sets - train and test.  \n",
    "\n",
    "Train examples you will use to train your model on and test examples to evaluate your model - to understand how your model works on the unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here, splitting your dataset into train and test parts. there another label is toxic,in which i train\n",
    "from sklearn.model_selection import train_test_split\n",
    "corpus = df.cleaned.apply(lambda x: ' '.join(x))\n",
    "X = corpus\n",
    "y = df.obscene.tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please, review again this article or read it if you have not done it before. \n",
    "\n",
    "https://medium.com/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement calculating a tf-idf score for each of the words from your vocabulary. \n",
    "\n",
    "The main goal of this taks is to create a dictionary - keys of the dictionary would be tokens and values would be corresponding tf-idf score of the token.\n",
    "\n",
    "#### Calculate it MANUALLY and compare the received scores for words with the sklearn implementation:  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tip: \n",
    "\n",
    "##### TF = (Number of time the word occurs in the current text) / (Total number of words in the current text)  \n",
    "\n",
    "##### IDF = (Total number of documents / Number of documents with word t in it)\n",
    "\n",
    "##### TF-IDF = TF*IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you calculated a tf-idf score for each of the words in your vocabulary - revectorize the texts.  \n",
    "Instead of using number of occurences of the i-th word in the i-th cell of the text vector, use it's tf-idf score.   \n",
    "\n",
    "Revectorize the documents, save vectors into np.array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 18705)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Your code here for obtaining a tf-idf vectorized documents.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "vocabulary = cleaned_vocab\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
    "                 ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus)\n",
    "\n",
    "pipe['tfid'].idf_\n",
    "\n",
    "pipe.transform(corpus).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>explanation</th>\n",
       "      <td>5.563271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edits</th>\n",
       "      <td>4.063610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>username</th>\n",
       "      <td>5.856578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hardcore</th>\n",
       "      <td>8.405540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metallica</th>\n",
       "      <td>9.648046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "explanation  5.563271\n",
       "edits        4.063610\n",
       "username     5.856578\n",
       "hardcore     8.405540\n",
       "metallica    9.648046"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf = pd.DataFrame(pipe['tfid'].idf_, index=pipe['count'].get_feature_names())\n",
    "df_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18705"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_voc = {token: count for token, count in cnt_vocab.items() if count > threshold_count and len(token) > threshold_len}\n",
    "len(cleaned_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf is a matrix \n",
    "def tf(word, i):\n",
    "    comment_voc = Counter(df.cleaned[i])\n",
    "    return comment_voc[word]/len(comment_voc)\n",
    "TF = {(word, i): (tf(word,i)) for i in range(len(df.cleaned)) for word in df.cleaned[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038461538461538464"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF['explanation', 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_doc = len(df.cleaned)\n",
    "number_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_comments = df.cleaned.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_cleaned'] = df.cleaned.apply(lambda x: [w for w in x if cnt_vocab[w] > threshold_count and len(w)>threshold_len])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_(token):\n",
    "    count = 0\n",
    "    for comment in df.cleaned_cleaned.tolist():\n",
    "        if token in comment:\n",
    "            count+=1\n",
    "    return count\n",
    "DF = {token: df_(token) for token in cleaned_vocab} # document frequency for every word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explanation': 5.571114066090075,\n",
       " 'edits': 4.076707074345483,\n",
       " 'username': 5.863856365916739,\n",
       " 'hardcore': 8.502913695531998,\n",
       " 'metallica': 9.648045999835,\n",
       " 'reverted': 4.92859095316825,\n",
       " 'vandalism': 4.595218222196305,\n",
       " 'closure': 8.525903213756695,\n",
       " 'voted': 7.393001851609954,\n",
       " 'please': 2.9774607564002133,\n",
       " 'remove': 4.541451386021977,\n",
       " 'template': 5.018531694028839,\n",
       " 'since': 4.065221237650293,\n",
       " 'retired': 7.722755137982422,\n",
       " 'match': 6.3844699960488915,\n",
       " 'background': 6.357514186060364,\n",
       " 'colour': 7.707250951446457,\n",
       " 'seemingly': 7.892654174777819,\n",
       " 'stuck': 7.196425327680466,\n",
       " 'thanks': 3.6178759821785227,\n",
       " 'january': 6.0957638579674205,\n",
       " 'really': 4.015554954694744,\n",
       " 'trying': 4.584321406087005,\n",
       " 'constantly': 6.996314229323013,\n",
       " 'removing': 5.315903877911587,\n",
       " 'relevant': 5.297307340131912,\n",
       " 'information': 3.894453473108621,\n",
       " 'talking': 5.463273285405882,\n",
       " 'instead': 4.876151453866621,\n",
       " 'seems': 4.381108735946799,\n",
       " 'formatting': 7.205698964465795,\n",
       " 'actual': 5.671707712471013,\n",
       " 'suggestion': 5.674390477326194,\n",
       " 'improvement': 6.443558912418898,\n",
       " 'wondered': 8.326290159852679,\n",
       " 'section': 4.006899096170283,\n",
       " 'statistic': 7.227677871184571,\n",
       " 'later': 5.309355678648086,\n",
       " 'subsection': 7.6819331434621665,\n",
       " 'accident': 7.491312783853517,\n",
       " 'think': 3.2745433893305758,\n",
       " 'reference': 4.217760962638623,\n",
       " 'tidying': 9.648045999835,\n",
       " 'exact': 6.450831671747978,\n",
       " 'format': 6.372249884714116,\n",
       " 'no-one': 7.956369989163927,\n",
       " 'first': 3.9044706314297155,\n",
       " 'preference': 7.23085752410195,\n",
       " 'style': 5.266912621138333,\n",
       " 'appears': 5.3168428451167244,\n",
       " 'backlog': 8.375080324022111,\n",
       " 'article': 2.45556838123514,\n",
       " 'review': 5.067461289319523,\n",
       " 'guess': 5.4616432931949515,\n",
       " 'delay': 6.771660483913575,\n",
       " 'reviewer': 7.160167579657842,\n",
       " 'listed': 5.194945327470342,\n",
       " 'wikipedia': 2.8321319245184426,\n",
       " 'transport': 8.385130659875614,\n",
       " 'chance': 5.9627443670689475,\n",
       " 'remember': 5.386372665405085,\n",
       " 'congratulation': 7.2436782125310115,\n",
       " 'cocksucker': 8.760742804834097,\n",
       " 'around': 4.811480686334933,\n",
       " 'banned': 5.785813658909869,\n",
       " 'sorry': 4.660776817568017,\n",
       " 'offensive': 6.6346141491816075,\n",
       " 'anyway': 5.218505525351291,\n",
       " 'intending': 9.009958596458082,\n",
       " 'write': 4.88282421141299,\n",
       " 'anything': 4.400833975413834,\n",
       " 'would': 2.972853494083564,\n",
       " 'merely': 6.042936428786522,\n",
       " 'requesting': 6.009520431866678,\n",
       " 'encyclopedic': 6.422052707197935,\n",
       " 'school': 5.392940003987588,\n",
       " 'selective': 8.136063423551612,\n",
       " 'breeding': 9.242580891726835,\n",
       " 'almost': 5.543044143138911,\n",
       " 'point': 3.9760736693135463,\n",
       " 'short': 5.709241971729211,\n",
       " 'messy': 8.717570632968888,\n",
       " 'someone': 4.107062605960097,\n",
       " 'expertise': 7.686945685285711,\n",
       " 'eugenics': 9.761374685142004,\n",
       " 'alignment': 9.453889985394042,\n",
       " 'subject': 4.64051074399106,\n",
       " 'contrary': 6.688681370451883,\n",
       " 'rationale': 6.5156622063202425,\n",
       " 'image': 4.530052187418244,\n",
       " 'uploading': 6.387205975867767,\n",
       " 'notice': 5.014704936880212,\n",
       " 'specifies': 7.2436782125310115,\n",
       " 'constitutes': 7.064048447402769,\n",
       " 'addition': 5.370883972055992,\n",
       " 'boilerplate': 8.760742804834097,\n",
       " 'description': 5.597504060271292,\n",
       " 'specific': 5.290879402480513,\n",
       " 'using': 4.415982911208037,\n",
       " 'consistent': 6.789935104157056,\n",
       " 'include': 5.139544058260804,\n",
       " 'uploaded': 5.820181302414077,\n",
       " 'medium': 5.338206107136946,\n",
       " 'consider': 4.942707324891507,\n",
       " 'checking': 6.3736003238119885,\n",
       " 'specified': 6.825392415993786,\n",
       " \"'image\": 7.385539130408365,\n",
       " 'edited': 5.411871242173681,\n",
       " 'clicking': 6.465537819137674,\n",
       " 'contribution': 4.6452988785877505,\n",
       " 'located': 6.6451962585121445,\n",
       " 'logged': 6.601824326358617,\n",
       " 'selecting': 7.341895840676457,\n",
       " 'dropdown': 7.491312783853517,\n",
       " 'lacking': 7.083096642373463,\n",
       " 'deleted': 4.230042647484494,\n",
       " 'described': 5.961848710941003,\n",
       " 'criterion': 5.197860174422744,\n",
       " 'speedy': 5.256688037732234,\n",
       " 'deletion': 4.441491540679405,\n",
       " 'question': 4.052007832191901,\n",
       " 'copyright': 5.3102885145366265,\n",
       " 'thank': 3.88689349351984,\n",
       " 'contribs': 5.368408110429787,\n",
       " 'unspecified': 8.405539531506822,\n",
       " 'source': 3.506392692581538,\n",
       " 'noticed': 5.457309591937833,\n",
       " 'currently': 5.311689401994307,\n",
       " 'specify': 7.151304892399996,\n",
       " 'created': 4.8846518086320145,\n",
       " 'content': 4.339071338812975,\n",
       " 'status': 5.675062294617166,\n",
       " 'unclear': 6.947164288211402,\n",
       " 'create': 5.207497793541462,\n",
       " 'owner': 6.791986386927614,\n",
       " 'obtained': 7.529212056444503,\n",
       " 'website': 4.931781766341322,\n",
       " 'taken': 5.311689401994307,\n",
       " 'together': 5.776844988927109,\n",
       " 'restatement': 8.458461932961164,\n",
       " 'usually': 5.802468093815007,\n",
       " 'sufficient': 6.650529604487507,\n",
       " 'however': 4.337306113288406,\n",
       " 'holder': 6.918793591082187,\n",
       " 'different': 4.718465830495451,\n",
       " 'publisher': 7.145439772947598,\n",
       " 'acknowledged': 7.491312783853517,\n",
       " 'adding': 4.83615204667168,\n",
       " 'proper': 5.7310354528958145,\n",
       " 'licensing': 7.550904881055763,\n",
       " 'already': 4.503879313114221,\n",
       " 'created/took': 7.936825393090957,\n",
       " 'picture': 5.178859189718717,\n",
       " 'audio': 7.470862173382226,\n",
       " 'video': 5.941466968621662,\n",
       " 'release': 5.974461490756701,\n",
       " 'believe': 4.4369997842746765,\n",
       " 'tagged': 5.89267680445223,\n",
       " 'following': 5.037888272335869,\n",
       " 'unsourced': 6.315841489659795,\n",
       " 'untagged': 7.86826272165366,\n",
       " 'copyrighted': 6.7556920807348435,\n",
       " 'non-free': 6.618948032437208,\n",
       " 'license': 6.353532760761179,\n",
       " 'discus': 5.250954835699722,\n",
       " 'phone': 7.411906006249106,\n",
       " 'exclusive': 7.917655476983237,\n",
       " 'group': 4.995106578811583,\n",
       " 'taliban': 9.088430211899578,\n",
       " 'destroying': 8.176229465276947,\n",
       " 'self-appointed': 9.088430211899578,\n",
       " 'purist': 10.341193180394946,\n",
       " 'destructive': 8.298119282885985,\n",
       " 'clean': 6.606930720433191,\n",
       " 'behavior': 5.985400524177132,\n",
       " 'issue': 4.2764117903199566,\n",
       " 'nonsensical': 8.491614140278063,\n",
       " 'warning': 5.137971730892851,\n",
       " 'start': 5.03221851937292,\n",
       " 'throwing': 7.759894684931878,\n",
       " 'accusation': 6.054655312899736,\n",
       " 'hominem': 8.082410710059293,\n",
       " 'attack': 4.744889866256855,\n",
       " 'going': 4.235284498896094,\n",
       " 'strengthen': 9.579053128348049,\n",
       " 'argument': 5.167063242488787,\n",
       " 'abusing': 7.4234224483106654,\n",
       " 'power': 5.518610117801629,\n",
       " 'admin': 5.070393842740801,\n",
       " 'probably': 4.7306753600100055,\n",
       " 'single': 5.400571686919748,\n",
       " 'talked': 7.4830822847170015,\n",
       " 'event': 5.642012359944614,\n",
       " 'absence': 7.727977081963574,\n",
       " 'notable': 5.112379019613882,\n",
       " 'living': 5.874464380528932,\n",
       " 'attend': 8.097448587423834,\n",
       " 'certainly': 5.33437468482539,\n",
       " 'carrier': 8.954898819275055,\n",
       " 'intend': 7.139608852636805,\n",
       " 'revert': 5.081468153039895,\n",
       " 'attracting': 9.761374685142004,\n",
       " 'attention': 5.679102704154172,\n",
       " 'willing': 6.254017107651457,\n",
       " 'throw': 7.069453865969677,\n",
       " 'quite': 4.9171875986834115,\n",
       " 'liberally': 9.684413644005874,\n",
       " 'perhaps': 4.916243162913542,\n",
       " 'achieve': 7.431174425114984,\n",
       " 'level': 5.790328339264396,\n",
       " 'civility': 6.819043188315127,\n",
       " 'rational': 7.6919834793156685,\n",
       " 'discussion': 4.04132523802193,\n",
       " 'topic': 5.038243701520346,\n",
       " 'resolve': 6.875457277595219,\n",
       " 'matter': 4.7148575791579805,\n",
       " 'peacefully': 9.761374685142004,\n",
       " 'started': 5.230497103735766,\n",
       " 'belong': 6.381741481395688,\n",
       " 'situation': 5.770910253407294,\n",
       " 'settled': 7.600353156469744,\n",
       " 'apologized': 8.355277696725931,\n",
       " 'santana': 10.582355237211832,\n",
       " 'february': 6.2844515929517115,\n",
       " 'making': 4.664439378126663,\n",
       " 'diplomat': 9.42490244852079,\n",
       " 'third': 5.924937666670452,\n",
       " 'signed': 6.921127314428407,\n",
       " 'label': 6.771660483913575,\n",
       " 'fella': 9.42490244852079,\n",
       " 'coming': 5.986317534787014,\n",
       " '1983.': 10.495343860222203,\n",
       " 'could': 3.7172228157550142,\n",
       " 'older': 7.040079257289772,\n",
       " 'lloyd': 9.453889985394042,\n",
       " 'birthday': 7.930394502760667,\n",
       " 'passed': 6.947164288211402,\n",
       " 'homie': 9.844756294081053,\n",
       " 'death': 5.857383851411121,\n",
       " 'forbid': 8.991266463445928,\n",
       " 'thinking': 6.00017456944844,\n",
       " 'equal': 6.884425947577979,\n",
       " 'changing': 5.930995668754367,\n",
       " 'birth': 6.898031599633757,\n",
       " 'comming': 9.984518236456212,\n",
       " 'tosser': 9.546263305525057,\n",
       " 'redirect': 5.244380190057637,\n",
       " 'sense': 5.198277275575819,\n",
       " 'argue': 6.329971461422781,\n",
       " 'hindi': 8.57353126274595,\n",
       " 'bother': 6.436338664445412,\n",
       " 'writing': 5.2749880351438785,\n",
       " 'something': 4.192877521278327,\n",
       " 'regarding': 5.093417511055146,\n",
       " 'posted': 5.505478327612333,\n",
       " 'better': 4.377981146238847,\n",
       " 'closer': 7.300077900993136,\n",
       " 'premature': 8.731755267960844,\n",
       " 'wrestling': 7.787293659119993,\n",
       " 'catagory': 10.677665417016158,\n",
       " 'surely': 6.520346055632669,\n",
       " 'besides': 6.4804634693543495,\n",
       " 'recent': 5.272738315409863,\n",
       " 'editing': 3.961918460417519,\n",
       " 'simply': 4.756355129831382,\n",
       " 'entirely': 6.039060454941829,\n",
       " 'unnecessary': 6.844685618928464,\n",
       " 'detail': 5.402616677407475,\n",
       " 'damage': 6.983798421391183,\n",
       " 'studying': 8.031490619632034,\n",
       " 'always': 4.926045612945797,\n",
       " 'symmetrical': 10.900808968330368,\n",
       " 'geometry': 8.885905947788103,\n",
       " 'stated': 5.596261052031695,\n",
       " 'symmetric': 10.677665417016158,\n",
       " 'assertion': 6.672152068500673,\n",
       " 'according': 5.381350053138794,\n",
       " 'kenneth': 9.242580891726835,\n",
       " 'rather': 4.62698901027633,\n",
       " 'irregular': 9.93572807228678,\n",
       " 'crystal': 8.279770144217787,\n",
       " 'common': 5.21171720908417,\n",
       " 'variety': 7.289891055686143,\n",
       " 'perfection': 9.369332597365979,\n",
       " 'still': 4.1419888271245515,\n",
       " 'decent': 7.331276271848997,\n",
       " 'number': 4.8294940397346515,\n",
       " 'falsity': 10.341193180394946,\n",
       " 'forgive': 7.600353156469744,\n",
       " 'signpost': 8.405539531506822,\n",
       " 'september': 6.200328602537952,\n",
       " 'single-page': 9.483742948543723,\n",
       " 'unsubscribe': 8.869376645836892,\n",
       " 'paragraph': 5.466541262170499,\n",
       " 'understand': 4.617374678978323,\n",
       " 'reason': 4.160732969405315,\n",
       " 'necessarily': 6.827517815306099,\n",
       " 'wrong': 4.571756735517306,\n",
       " 'persuaded': 9.453889985394042,\n",
       " 'strategy': 7.815464536086689,\n",
       " 'introducing': 7.8743050361096225,\n",
       " 'academic': 6.327387480656856,\n",
       " 'honor': 7.573078738550085,\n",
       " 'unhelpful': 8.024423452408943,\n",
       " 'approach': 6.449372882284318,\n",
       " 'sitting': 7.652374341220622,\n",
       " 'justice': 7.083096642373463,\n",
       " 'similarly': 7.400520684023981,\n",
       " 'enhanced': 9.369332597365979,\n",
       " 'change': 4.186941882513651,\n",
       " 'support': 4.846076237872301,\n",
       " 'invite': 7.269823492635334,\n",
       " 'anyone': 4.579141797614768,\n",
       " 'written': 4.993405608848821,\n",
       " 'jurist': 10.272200308907994,\n",
       " 'benjamin': 8.885905947788103,\n",
       " 'learned': 6.9665353539674015,\n",
       " 'marshall': 9.088430211899578,\n",
       " 'harlan': 10.495343860222203,\n",
       " 'becomes': 7.001364745109082,\n",
       " 'current': 5.111613615826037,\n",
       " 'version': 5.038599257079648,\n",
       " 'either': 4.756086997372343,\n",
       " 'improved': 6.8799315579901394,\n",
       " 'credential': 8.120438105648532,\n",
       " 'introductory': 8.549433711166891,\n",
       " 'repeat': 6.759660339910464,\n",
       " 'kathleen': 10.582355237211832,\n",
       " 'sullivan': 9.844756294081053,\n",
       " 'stanford': 9.21905039431664,\n",
       " 'suggests': 6.916465301322595,\n",
       " 'harvard': 8.28890262778106,\n",
       " 'faculty': 8.805863240114567,\n",
       " 'wonder': 6.139703980721516,\n",
       " 'avoided': 7.697046781272215,\n",
       " 'learning': 6.93999579873279,\n",
       " 'others': 4.615742406259614,\n",
       " 'managed': 7.385539130408365,\n",
       " 'grasp': 7.9498125886177675,\n",
       " 'process': 5.442287850241995,\n",
       " 'judging': 7.815464536086689,\n",
       " 'anecdote': 9.242580891726835,\n",
       " 'gently': 9.844756294081053,\n",
       " 'illustrates': 8.136063423551612,\n",
       " 'humorous': 8.954898819275055,\n",
       " 'stronger': 8.159968944405167,\n",
       " 'clarence': 10.272200308907994,\n",
       " 'thomas': 7.385539130408365,\n",
       " 'mention': 4.8722292886334575,\n",
       " 'wanting': 7.454797570878419,\n",
       " 'return': 6.365524909806442,\n",
       " 'degree': 6.624142849314312,\n",
       " 'minimum': 7.427290925088586,\n",
       " 'questioning': 7.8743050361096225,\n",
       " 'deserves': 6.98131394806352,\n",
       " 'radial': 9.984518236456212,\n",
       " 'symmetry': 10.035811530843763,\n",
       " 'several': 5.009510120003108,\n",
       " 'extinct': 9.068227504582058,\n",
       " 'lineage': 8.805863240114567,\n",
       " 'included': 5.242198212320887,\n",
       " 'apologize': 6.579993064701382,\n",
       " 'knowledge': 5.655760530661671,\n",
       " 'history': 4.416173732695116,\n",
       " 'study': 6.00017456944844,\n",
       " 'archaeology': 9.196060876091943,\n",
       " 'e-mail': 6.888940627932505,\n",
       " 'translate': 7.555300492528801,\n",
       " 'mother': 6.3913240324766845,\n",
       " 'child': 5.861424260948126,\n",
       " 'michael': 6.6813012631542605,\n",
       " 'jackson': 7.781753478744378,\n",
       " 'studied': 7.815464536086689,\n",
       " 'motif': 7.642712430308886,\n",
       " 'reasoning': 6.739974664839434,\n",
       " 'judged': 8.447651016856948,\n",
       " 'character': 5.779825617065247,\n",
       " 'harshly': 10.035811530843763,\n",
       " 'wacko': 9.93572807228678,\n",
       " 'ignore': 6.226812591412424,\n",
       " 'continue': 4.646499503056624,\n",
       " 'refuting': 9.579053128348049,\n",
       " 'bullshit': 6.9665353539674015,\n",
       " 'jayjg': 8.585801355337765,\n",
       " 'example': 4.659802396053645,\n",
       " 'barnstar': 6.584988911894754,\n",
       " 'block': 4.560890678942729,\n",
       " 'expires': 8.082410710059293,\n",
       " 'funny': 6.327387480656856,\n",
       " 'thing': 3.8593973045355567,\n",
       " 'uncivil': 7.160167579657842,\n",
       " 'heading': 6.84252345592397,\n",
       " 'freedom': 6.73027526775072,\n",
       " 'contain': 6.652313726281008,\n",
       " 'praise': 8.045776576879511,\n",
       " 'looked': 6.0409965639686956,\n",
       " 'month': 5.418088878784552,\n",
       " 'quickly': 6.600127973110438,\n",
       " 'drive': 6.947164288211402,\n",
       " 'meaning': 5.811670612746168,\n",
       " 'updating': 7.591178780193703,\n",
       " 'sound': 5.5083184317650815,\n",
       " 'generating': 9.196060876091943,\n",
       " 'interest': 5.222344301658456,\n",
       " 'spent': 6.777714992822281,\n",
       " 'freely': 7.283157023504799,\n",
       " 'licensed': 7.618958344300778,\n",
       " 'length': 6.816935705975562,\n",
       " 'classical': 7.7438085471802545,\n",
       " 'music': 6.006707490490063,\n",
       " 'unfortunately': 6.034236518910976,\n",
       " 'attempt': 5.486933261148058,\n",
       " 'failed': 6.486496670158517,\n",
       " 'effectively': 7.60961248188254,\n",
       " 'wikiproject': 5.902752456440972,\n",
       " 'interested': 5.30563301264584,\n",
       " 'wikipedia_talk': 8.491614140278063,\n",
       " 'given': 4.865626623590105,\n",
       " 'featured': 6.335159540504559,\n",
       " 'impressive': 8.235318381646954,\n",
       " 'subpages': 8.869376645836892,\n",
       " 'difference': 5.680453143252043,\n",
       " 'surprised': 6.668515700857289,\n",
       " 'straw': 7.943297907596574,\n",
       " 'never': 4.407811844367988,\n",
       " 'claimed': 6.379020391281327,\n",
       " 'position': 5.286768869175027,\n",
       " 'practitioner': 8.775557890619236,\n",
       " 'researcher': 7.5597155107379175,\n",
       " 'field': 5.9010661154005355,\n",
       " 'ignored': 6.763644408925338,\n",
       " 'exactly': 5.439098054873895,\n",
       " 'quote': 5.303776863621047,\n",
       " 'agrees': 7.614274494988352,\n",
       " 'combating': 10.495343860222203,\n",
       " 'notion': 7.411906006249106,\n",
       " 'absurd': 7.273140245261327,\n",
       " 'claim': 4.612718093148372,\n",
       " 'pedophilia': 9.196060876091943,\n",
       " 'sexual': 7.128048030235729,\n",
       " 'orientation': 8.426373618409663,\n",
       " 'unfair': 7.23404731947005,\n",
       " 'disorder': 7.911346307789972,\n",
       " 'divided': 7.983038236246088,\n",
       " 'value': 6.196925309406243,\n",
       " 'judgment': 7.503786958078693,\n",
       " 'cantor': 10.272200308907994,\n",
       " 'pointed': 6.514105785772585,\n",
       " 'earlier': 5.980828042502242,\n",
       " 'thread': 6.657685270082919,\n",
       " 'scientific': 6.360177303479848,\n",
       " 'judgement': 7.211929514216432,\n",
       " 'choose': 6.452292592387653,\n",
       " 'clearly': 4.888623098409133,\n",
       " 'pretend': 7.396754201228505,\n",
       " 'mainland': 9.048424877285878,\n",
       " 'includes': 6.4305997677763935,\n",
       " 'lower': 7.0064408981409425,\n",
       " 'basin': 9.802196679662257,\n",
       " 'china': 6.840365957783948,\n",
       " 'river': 7.215059407225358,\n",
       " 'korea': 7.657240530871795,\n",
       " 'found': 4.704874135173798,\n",
       " 'citation': 5.214681428912887,\n",
       " 'comprehensive': 7.555300492528801,\n",
       " 'hammer': 8.4369557277402,\n",
       " 'speculation': 7.080353156427712,\n",
       " 'culture': 6.115402732039343,\n",
       " 'brought': 6.300651324165821,\n",
       " 'japan': 7.0559947125956715,\n",
       " 'migrant': 10.147037165953988,\n",
       " 'trace': 7.917655476983237,\n",
       " 'southeast': 9.029006791428776,\n",
       " 'describes': 7.276468035354003,\n",
       " 'migration': 8.775557890619236,\n",
       " 'based': 5.0937932390325145,\n",
       " 'close': 5.909526402749926,\n",
       " 'entire': 5.605621494791259,\n",
       " 'haplogroup': 10.207661787770423,\n",
       " 'proposed': 6.031353287696891,\n",
       " 'asian': 7.396754201228505,\n",
       " 'origin': 6.337763708642948,\n",
       " 'definition': 5.708546803122836,\n",
       " 'southern': 7.199506994217874,\n",
       " 'dispersal': 10.495343860222203,\n",
       " 'neolithic': 10.415301152548667,\n",
       " 'farmer': 8.972917324777733,\n",
       " 'eventually': 6.684984508570557,\n",
       " 'concluding': 9.579053128348049,\n",
       " 'state': 4.407244253774753,\n",
       " 'propose': 6.749769062431722,\n",
       " 'chromosome': 9.889208056651887,\n",
       " 'descend': 9.684413644005874,\n",
       " 'prehistoric': 9.684413644005874,\n",
       " 'southeastern': 10.272200308907994,\n",
       " 'agriculture': 9.173588020239883,\n",
       " 'region': 6.507904215509303,\n",
       " 'global': 6.898031599633757,\n",
       " 'sample': 7.798466959718119,\n",
       " 'consisted': 8.919807499463783,\n",
       " 'population': 6.328678636420476,\n",
       " 'including': 5.1825484744935135,\n",
       " 'sampled': 10.415301152548667,\n",
       " 'across': 6.085580470576722,\n",
       " 'japanese': 6.808549912599288,\n",
       " 'archipelago': 10.495343860222203,\n",
       " 'pretty': 5.359055347200359,\n",
       " 'everyone': 5.3065623807424736,\n",
       " 'warren': 8.760742804834097,\n",
       " 'hospital': 7.8743050361096225,\n",
       " 'qualifies': 7.749141893155617,\n",
       " 'native': 6.806464406108267,\n",
       " 'rachel': 9.151609113521108,\n",
       " 'actually': 4.402715089587804,\n",
       " 'preceding': 6.859953091059253,\n",
       " 'unsigned': 6.193533559405123,\n",
       " 'comment': 3.8808416987413024,\n",
       " 'added': 4.314982200528602,\n",
       " 'august': 6.029435741567619,\n",
       " 'explicit': 7.809766514972051,\n",
       " 'fenian': 9.889208056651887,\n",
       " 'edit-warring': 7.776243822933408,\n",
       " 'giant': 7.821195210795675,\n",
       " 'terrorism': 7.427290925088586,\n",
       " 'notability': 5.448698128602914,\n",
       " 'placed': 5.5943994318849946,\n",
       " 'speedily': 6.159143037753739,\n",
       " 'person': 4.375779310486905,\n",
       " 'people': 3.491369723499523,\n",
       " 'company': 5.573539779832563,\n",
       " 'indicate': 5.994608692370996,\n",
       " 'assert': 6.787888020535331,\n",
       " 'guideline': 5.012970331068003,\n",
       " 'generally': 5.5741471287731885,\n",
       " 'accepted': 5.77535799980553,\n",
       " 'contest': 6.0675076895170275,\n",
       " 'tagging': 6.618948032437208,\n",
       " 'existing': 5.924937666670452,\n",
       " 'leave': 4.767139812413528,\n",
       " 'explaining': 5.86223430554487,\n",
       " 'hesitate': 6.331265959985427,\n",
       " 'confirm': 6.377662617820867,\n",
       " 'check': 4.728586586404614,\n",
       " 'biography': 5.7518620584366,\n",
       " 'briefly': 7.662130516165987,\n",
       " 'summarize': 7.9763042040647445,\n",
       " 'armenia': 8.415902318542368,\n",
       " 'necessary': 5.794863494429787,\n",
       " 'sentence': 5.191624444385172,\n",
       " 'redundant': 7.3454609068409535,\n",
       " 'welcome': 4.62440046900273,\n",
       " 'responded': 6.884425947577979,\n",
       " 'without': 4.239273971992416,\n",
       " 'seeing': 6.206026623652589,\n",
       " 'response': 5.3420522657244245,\n",
       " 't/c//wp': 9.068227504582058,\n",
       " 'chicago/wp': 9.068227504582058,\n",
       " 'white': 5.917202346622031,\n",
       " 'tiger': 8.57353126274595,\n",
       " 'erased': 7.936825393090957,\n",
       " 'holocaust': 7.507979836338729,\n",
       " 'brutally': 9.93572807228678,\n",
       " 'shave': 10.08987875211404,\n",
       " 'skinhead': 10.415301152548667,\n",
       " 'meeting': 7.224508296423291,\n",
       " 'doubt': 5.724659235756539,\n",
       " 'bible': 7.266517704500834,\n",
       " 'homosexuality': 8.00351676758963,\n",
       " 'deadly': 9.068227504582058,\n",
       " 'forehead': 9.889208056651887,\n",
       " 'fucking': 5.548358593202403,\n",
       " 'appreciate': 5.4753084416140325,\n",
       " 'anymore': 6.301908395355872,\n",
       " 'beware': 8.972917324777733,\n",
       " 'filthy': 8.610802657543182,\n",
       " 'screwed': 8.63644508815652,\n",
       " 'dominance': 9.369332597365979,\n",
       " 'almighty': 9.266678443305896,\n",
       " 'administrator': 5.148236329504734,\n",
       " 'outside': 6.032313441395234,\n",
       " 'criticism': 5.872006370278663,\n",
       " 'present': 5.556682065751037,\n",
       " 'conforms': 9.109049499102312,\n",
       " 'neutral': 5.367419479602847,\n",
       " 'begin': 6.5234808544380405,\n",
       " 'offer': 6.2993958312199885,\n",
       " 'polygraph': 11.188491040782148,\n",
       " 'concerned': 6.266079980100732,\n",
       " 'result': 5.320136190836275,\n",
       " 'shock': 8.405539531506822,\n",
       " 'complainant': 10.207661787770423,\n",
       " 'uncovered': 9.722153971988721,\n",
       " 'perfectly': 6.507904215509303,\n",
       " 'valid': 5.785813658909869,\n",
       " 'telling': 6.304427288375355,\n",
       " 'truth': 5.520335743769099,\n",
       " 'machine': 7.215059407225358,\n",
       " 'investigator': 9.291371055896267,\n",
       " 'research': 5.084814503067238,\n",
       " 'followup': 9.579053128348049,\n",
       " 'story': 5.650500820968691,\n",
       " 'possible': 5.080355186870474,\n",
       " 'verify': 6.6137800622787655,\n",
       " 'false': 5.661710961442302,\n",
       " 'matched': 9.291371055896267,\n",
       " 'accused': 6.534530690624625,\n",
       " 'happened': 5.7569548304478895,\n",
       " 'arguing': 6.907205975909799,\n",
       " 'respected': 7.359849644293053,\n",
       " 'baseless': 8.136063423551612,\n",
       " 'agree': 4.6204131293701725,\n",
       " 'though': 4.5328362132018825,\n",
       " 'appropriate': 5.2772428275309675,\n",
       " 'stalking': 7.246909233112458,\n",
       " 'absolute': 7.3454609068409535,\n",
       " 'rubbish': 7.7123923509468755,\n",
       " 'assumed': 7.199506994217874,\n",
       " 'faith': 5.486933261148058,\n",
       " 'intention': 6.381741481395688,\n",
       " 'suggested': 6.256418069188995,\n",
       " 'suggest': 5.148236329504734,\n",
       " 'might': 4.318783829437542,\n",
       " 'ulterior': 9.514514607210478,\n",
       " 'motive': 8.385130659875614,\n",
       " 'administrative': 7.313823821897771,\n",
       " 'mentioned': 5.132097423810678,\n",
       " 'party': 5.58085242667885,\n",
       " 'disagreement': 7.058672090366388,\n",
       " 'conflict': 5.712027488850638,\n",
       " 'extend': 7.996643888301867,\n",
       " 'toward': 7.110953596876429,\n",
       " 'spurious': 8.298119282885985,\n",
       " 'unfounded': 7.936825393090957,\n",
       " 'chatspy': 10.341193180394946,\n",
       " 'regard': 5.204974663523341,\n",
       " 'predominant': 9.844756294081053,\n",
       " 'scholary': 10.900808968330368,\n",
       " 'consensus': 4.936908339568595,\n",
       " 'allegedly': 8.326290159852679,\n",
       " 'despite': 6.109159215399658,\n",
       " 'rhetoric': 8.261751638715108,\n",
       " 'fascism': 8.4369557277402,\n",
       " 'consistently': 7.727977081963574,\n",
       " 'right-wing': 8.326290159852679,\n",
       " 'force': 5.951162945860541,\n",
       " 'aware': 5.5790192455971885,\n",
       " 'owning': 9.266678443305896,\n",
       " 'numerous': 6.474466449881975,\n",
       " 'developed': 7.080353156427712,\n",
       " 'scholar': 6.561885574073992,\n",
       " 'manner': 6.24091388265303,\n",
       " 'roger': 8.218076575212446,\n",
       " 'griffin': 9.151609113521108,\n",
       " 'mcdonald': 9.396731571554094,\n",
       " 'dissenter': 9.802196679662257,\n",
       " 'leftist': 8.537599253519886,\n",
       " 'connection': 6.365524909806442,\n",
       " 'radical': 7.838586953507543,\n",
       " 'right': 4.024802387662811,\n",
       " 'system': 5.550729667223742,\n",
       " 'street': 6.779741335967514,\n",
       " 'socialist': 7.798466959718119,\n",
       " 'distance': 7.6670445309684165,\n",
       " 'movement': 6.350887256572754,\n",
       " 'course': 5.0937932390325145,\n",
       " 'educated': 7.9763042040647445,\n",
       " 'foremost': 9.151609113521108,\n",
       " 'expert': 6.227980133868462,\n",
       " 'former': 6.211757298361574,\n",
       " 'member': 5.36150813233979,\n",
       " 'communist': 7.139608852636805,\n",
       " 'italy': 7.6819331434621665,\n",
       " 'cover': 6.049755744058577,\n",
       " 'wrote': 5.378348550135038,\n",
       " 'definitive': 8.279770144217787,\n",
       " 'seven': 7.029607957422477,\n",
       " 'volume': 7.411906006249106,\n",
       " 'piece': 5.817078119163561,\n",
       " 'mussolini': 9.984518236456212,\n",
       " 'wanted': 5.355631523850805,\n",
       " 'bottom': 6.269727400557775,\n",
       " 'promoter': 9.369332597365979,\n",
       " 'speculated': 9.342664350283817,\n",
       " 'skyhook': 10.900808968330368,\n",
       " 'concept': 6.236191323698856,\n",
       " 'competitive': 9.029006791428776,\n",
       " 'realistically': 9.984518236456212,\n",
       " 'thought': 4.666888558875803,\n",
       " 'space': 6.0517126918455375,\n",
       " 'elevator': 9.684413644005874,\n",
       " 'rotating': 9.242580891726835,\n",
       " 'tether': 11.03434036095489,\n",
       " 'deemed': 7.754503836297002,\n",
       " 'feasible': 9.316688863880557,\n",
       " 'presently': 8.168066154637785,\n",
       " 'available': 5.810900493339604,\n",
       " 'material': 4.959980037190965,\n",
       " 'statement': 4.87796688553013,\n",
       " 'alone': 5.659723547737464,\n",
       " 'exceed': 9.196060876091943,\n",
       " 'payload': 10.341193180394946,\n",
       " 'indication': 7.491312783853517,\n",
       " 'particular': 5.514595199876147,\n",
       " 'scenario': 8.375080324022111,\n",
       " 'considered': 5.000226917699559,\n",
       " 'although': 5.232653671317317,\n",
       " 'application': 7.154250402629753,\n",
       " 'become': 5.5560852289681755,\n",
       " 'future': 5.434332358800881,\n",
       " 'higher': 6.6120633236597115,\n",
       " 'strength': 7.83275603319675,\n",
       " 'operational': 8.853116124965112,\n",
       " 'temperature': 8.105053186809052,\n",
       " 'shall': 6.452292592387653,\n",
       " 'commercial': 6.900317314914613,\n",
       " 'suffice': 8.22666031890384,\n",
       " 'needed': 5.557279258960783,\n",
       " 'primary': 6.152621275507351,\n",
       " 'message': 4.682208848638638,\n",
       " 'reader': 5.476409763311252,\n",
       " 'magic': 8.082410710059293,\n",
       " 'facility': 8.08990138178845,\n",
       " 'appear': 5.631019685396869,\n",
       " 'misread': 8.805863240114567,\n",
       " 'upper': 7.754503836297002,\n",
       " 'limit': 6.84252345592397,\n",
       " 'problem': 4.307935681726666,\n",
       " 'built': 7.119464286544337,\n",
       " 'reinforce': 9.889208056651887,\n",
       " 'conclusion': 6.342992478725747,\n",
       " 'report': 5.284492519455442,\n",
       " 'fundamental': 7.454797570878419,\n",
       " 'phase': 8.010437210434203,\n",
       " 'effort': 5.712725082182032,\n",
       " 'technically': 7.105319779158173,\n",
       " 'evaluated': 9.514514607210478,\n",
       " 'alternate': 7.697046781272215,\n",
       " 'configuration': 9.483742948543723,\n",
       " 'allow': 6.018007045743997,\n",
       " 'vehicle': 7.385539130408365,\n",
       " 'technology': 7.128048030235729,\n",
       " 'combined': 7.7123923509468755,\n",
       " 'orbiting': 10.582355237211832,\n",
       " 'spinning': 9.42490244852079,\n",
       " 'provide': 5.226626963450457,\n",
       " 'method': 6.465537819137674,\n",
       " 'moving': 6.419219844113631,\n",
       " 'surface': 7.86826272165366,\n",
       " 'earth': 6.491045578684886,\n",
       " 'orbit': 8.821367426650532,\n",
       " 'architecture': 8.105053186809052,\n",
       " 'design': 6.947164288211402,\n",
       " 'solution': 6.588333396617601,\n",
       " 'expect': 6.185663929133704,\n",
       " 'prove': 5.941466968621662,\n",
       " 'subjected': 8.902713066104484,\n",
       " 'detailed': 6.763644408925338,\n",
       " 'completely': 5.378348550135038,\n",
       " 'potential': 6.8023063959596035,\n",
       " 'drastically': 9.316688863880557,\n",
       " 'reducing': 8.63644508815652,\n",
       " 'access': 6.285688451489108,\n",
       " 'boeing': 9.109049499102312,\n",
       " 'combining': 8.821367426650532,\n",
       " 'plane': 7.917655476983237,\n",
       " 'concluded': 8.052996824852999,\n",
       " 'general': 5.151415982422114,\n",
       " 'mismatch': 10.582355237211832,\n",
       " 'atmospheric': 9.802196679662257,\n",
       " 'speed': 7.320768294250582,\n",
       " 'airplane': 9.009958596458082,\n",
       " 'orbital': 9.722153971988721,\n",
       " 'decrease': 9.242580891726835,\n",
       " 'increasing': 7.9763042040647445,\n",
       " 'fucked': 7.943297907596574,\n",
       " 'drink': 7.905076694776376,\n",
       " 'threatening': 7.224508296423291,\n",
       " 'disputing': 8.469391003493353,\n",
       " 'neutrality': 6.836064875884558,\n",
       " 'country': 5.202457883671373,\n",
       " 'bully': 7.396754201228505,\n",
       " 'outcome': 7.495453576519549,\n",
       " 'russia': 7.050661366620309,\n",
       " 'undeletion': 9.196060876091943,\n",
       " 'hoped': 8.561409902213605,\n",
       " 'researching': 8.105053186809052,\n",
       " 'texas': 7.662130516165987,\n",
       " 'government': 5.707852117440157,\n",
       " 'local': 6.3222394641394555,\n",
       " 'apparently': 5.890173674234112,\n",
       " 'florida': 7.856286530606945,\n",
       " 'public': 5.4698199536321965,\n",
       " 'domain': 6.79610161907272,\n",
       " 'similar': 5.455689283724844,\n",
       " 'old-fashioned': 10.035811530843763,\n",
       " 'reliable': 4.779687713001639,\n",
       " 'meantime': 7.503786958078693,\n",
       " 'awesome': 7.524929394652502,\n",
       " 'disregard': 7.9763042040647445,\n",
       " 'stupid': 5.665697677686123,\n",
       " 'peace': 6.849024020527063,\n",
       " 'deleting': 5.563872030817276,\n",
       " 'stuff': 5.358565511285592,\n",
       " 'asshole': 6.459629382451507,\n",
       " 'sidaway': 8.954898819275055,\n",
       " 'obviously': 5.389903564407637,\n",
       " 'major': 5.585142963447718,\n",
       " 'previous': 5.7310354528958145,\n",
       " 'assessment': 7.289891055686143,\n",
       " 'issued': 7.618958344300778,\n",
       " 'longer': 5.738168150753242,\n",
       " 'student': 6.267294309333134,\n",
       " 'demographic': 8.060269584182079,\n",
       " 'trivia': 7.393001851609954,\n",
       " 'additional': 6.370901266842823,\n",
       " 'deficient': 9.722153971988721,\n",
       " 'integrated': 8.537599253519886,\n",
       " 'wiaga': 10.147037165953988,\n",
       " 'b-class': 9.316688863880557,\n",
       " \"'talk\": 9.42490244852079,\n",
       " 'delete': 4.8449035611034965,\n",
       " 'feeling': 6.353532760761179,\n",
       " 'hasty': 8.954898819275055,\n",
       " 'decision': 5.9388388462153925,\n",
       " 'douche': 8.128220246090585,\n",
       " 'second': 5.050763986695914,\n",
       " 'front': 6.5521452373256075,\n",
       " 'computer': 6.118539169529473,\n",
       " 'masturbating': 9.761374685142004,\n",
       " 'fictional': 7.600353156469744,\n",
       " 'creature': 8.176229465276947,\n",
       " 'mediocre': 9.342664350283817,\n",
       " 'sucking': 8.48044083967994,\n",
       " 'failure': 7.108132720534788,\n",
       " 'personal': 4.510778054805378,\n",
       " 'ahead': 6.164610519935872,\n",
       " 'profile': 7.102514728230564,\n",
       " 'another': 4.286921520887097,\n",
       " 'insult': 6.477460464094579,\n",
       " 'dream': 7.5597155107379175,\n",
       " 'innocent': 7.266517704500834,\n",
       " 'eighteen': 10.495343860222203,\n",
       " 'college': 6.467020399097897,\n",
       " 'freshman': 9.93572807228678,\n",
       " 'happy': 5.207497793541462,\n",
       " 'overweight': 9.844756294081053,\n",
       " 'follow': 5.607504143605875,\n",
       " 'somebody': 6.2600303548749086,\n",
       " 'suffer': 7.989817923231468,\n",
       " 'empty': 7.765314752401218,\n",
       " 'threat': 6.121685475218838,\n",
       " 'saying': 4.92350673503489,\n",
       " 'along': 5.612541937635832,\n",
       " 'within': 5.380849176594388,\n",
       " 'sister': 7.32425869919035,\n",
       " 'sucker': 8.885905947788103,\n",
       " 'drown': 10.035811530843763,\n",
       " 'sorrow': 9.93572807228678,\n",
       " 'headed': 8.426373618409663,\n",
       " 'maybe': 4.900942317958242,\n",
       " 'couple': 5.891841731334809,\n",
       " 'horny': 9.984518236456212,\n",
       " 'convenience': 8.731755267960844,\n",
       " 'store': 7.623664235338191,\n",
       " 'little': 4.5861293161839605,\n",
       " 'three': 5.186251420506208,\n",
       " 'minute': 6.182310097035273,\n",
       " 'tonight': 7.564150107805784,\n",
       " 'sincerely': 7.085847675745352,\n",
       " 'every': 4.791283646361329,\n",
       " 'appearence': 9.984518236456212,\n",
       " 'absolutely': 6.027521865385335,\n",
       " 'awful': 7.892654174777819,\n",
       " 'money': 6.43346509924968,\n",
       " 'lange': 10.341193180394946,\n",
       " 'candidate': 6.777714992822281,\n",
       " 'kindly': 7.040079257289772,\n",
       " 'malicious': 7.983038236246088,\n",
       " 'finger': 7.637916258045393,\n",
       " 'everytime': 8.469391003493353,\n",
       " 'repost': 8.184459964413463,\n",
       " 'locking': 9.342664350283817,\n",
       " 'violate': 6.767644414258685,\n",
       " 'newbie': 7.749141893155617,\n",
       " 'whether': 4.980907557296922,\n",
       " 'conservative': 7.105319779158173,\n",
       " 'wikipedians': 6.394078855155529,\n",
       " 'bisexual': 9.396731571554094,\n",
       " 'homosexual': 7.520664995866045,\n",
       " 'heterosexual': 9.316688863880557,\n",
       " 'defined': 6.823271524424648,\n",
       " 'activity': 6.374952589062001,\n",
       " 'attracted': 8.853116124965112,\n",
       " 'sexually': 9.009958596458082,\n",
       " 'straight': 6.686831231163722,\n",
       " 'opposite': 7.0374511348835025,\n",
       " 'frank': 7.438986964851777,\n",
       " 'herbert': 9.316688863880557,\n",
       " 'mason': 8.902713066104484,\n",
       " 'christian': 6.150456772497778,\n",
       " 'enforcing': 8.525903213756695,\n",
       " 'category': 5.43380423626418,\n",
       " 'arabic': 7.770764357168782,\n",
       " 'middle': 6.385837050260425,\n",
       " 'ethnicity': 7.446861021282683,\n",
       " 'forced': 7.058672090366388,\n",
       " 'called': 4.932740999028781,\n",
       " 'european': 6.537710343542005,\n",
       " 'syriac': 9.453889985394042,\n",
       " 'assyrian': 8.537599253519886,\n",
       " 'ancestry': 7.969615215913947,\n",
       " 'speak': 5.919774144010402,\n",
       " 'aramaic': 9.546263305525057,\n",
       " 'forgetting': 8.837115783618671,\n",
       " 'speaks': 7.47491897407784,\n",
       " 'respect': 5.824854208113469,\n",
       " 'appease': 9.61295468002373,\n",
       " 'muslim': 6.419219844113631,\n",
       " 'closely': 7.042714304927777,\n",
       " 'related': 5.364952170184389,\n",
       " 'forcing': 8.345521521780569,\n",
       " 'alien': 8.052996824852999,\n",
       " 'oppression': 9.048424877285878,\n",
       " '|talk': 8.610802657543182,\n",
       " 'officially': 7.094146478560048,\n",
       " 'episode': 6.224481588025948,\n",
       " 'everybody': 6.875457277595219,\n",
       " 'decided': 6.03519944628437,\n",
       " 'playground': 8.919807499463783,\n",
       " 'grudge': 8.649517169723872,\n",
       " 'realise': 7.438986964851777,\n",
       " 'noble': 8.316811415898137,\n",
       " 'project': 5.2575729935422,\n",
       " '201.215.187.159': 10.341193180394946,\n",
       " 'neiln': 9.291371055896267,\n",
       " 'harassed': 8.22666031890384,\n",
       " 'disruptively': 9.396731571554094,\n",
       " 'stopped': 7.034829901403628,\n",
       " 'taking': 5.529008825022528,\n",
       " 'young': 6.666702463733108,\n",
       " 'chemical': 8.105053186809052,\n",
       " 'imbalance': 9.93572807228678,\n",
       " 'brain': 7.001364745109082,\n",
       " 'gibberish': 8.937199242175653,\n",
       " 'legal': 6.140774071781361,\n",
       " 'profession': 8.235318381646954,\n",
       " 'scare': 8.33585961086883,\n",
       " 'insulting': 7.0374511348835025,\n",
       " 'original': 4.812898522954134,\n",
       " 'right.': 10.147037165953988,\n",
       " 'oppose': 6.615499753158292,\n",
       " 'stand': 5.757684491188033,\n",
       " 'properly': 6.139703980721516,\n",
       " 'resolved': 7.01924517038693,\n",
       " 'moved': 5.710633760402034,\n",
       " 'mainspace': 8.4369557277402,\n",
       " 'apology': 6.28940823259164,\n",
       " 'unlikely': 7.151304892399996,\n",
       " 'waste': 6.560255581863061,\n",
       " 'justify': 6.798165603293572,\n",
       " 'plasma': 9.722153971988721,\n",
       " 'physic': 7.317290029874258,\n",
       " 'transgression': 10.035811530843763,\n",
       " 'deserve': 6.664892508487869,\n",
       " 'considerate': 9.453889985394042,\n",
       " 'ambiguous': 7.83275603319675,\n",
       " 'irish': 6.988785962902221,\n",
       " 'qoute': 10.495343860222203,\n",
       " 'invented': 7.462797613545496,\n",
       " 'establishment': 8.244052061615708,\n",
       " 'depends': 7.6919834793156685,\n",
       " 'context': 5.743191483885466,\n",
       " 'applicable': 7.642712430308886,\n",
       " 'citizenship': 8.261751638715108,\n",
       " 'nationality': 7.266517704500834,\n",
       " 'predominantly': 8.991266463445928,\n",
       " 'refers': 6.495615274374952,\n",
       " 'republic': 6.726421698434731,\n",
       " 'ireland': 7.088606298184432,\n",
       " 'whose': 6.366866291630644,\n",
       " 'official': 5.442820473423695,\n",
       " 'whats': 7.438986964851777,\n",
       " 'inane': 9.173588020239883,\n",
       " 'conversation': 6.745839784291832,\n",
       " 'reading': 5.463273285405882,\n",
       " 'magazine': 6.4895269755076965,\n",
       " 'corporate': 8.017405879750296,\n",
       " 'easily': 6.147218777224003,\n",
       " 'well.': 9.130102908300145,\n",
       " 'belated': 9.068227504582058,\n",
       " 'elsewhere': 6.5234808544380405,\n",
       " 'looking': 5.232653671317317,\n",
       " 'nothing': 4.531550315039267,\n",
       " 'inability': 8.525903213756695,\n",
       " 'express': 6.8821762278439635,\n",
       " 'concern': 5.56447353459481,\n",
       " 'cheer': 5.515167773610656,\n",
       " 'direction': 6.951971989779505,\n",
       " 'recognize': 7.119464286544337,\n",
       " 'american': 5.297768063475698,\n",
       " ...}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDF = {token: np.log((1+number_doc)/(1+DF[token]))+1 for token in cleaned_vocab}\n",
    "IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = {(word, i): TF[word,i]*IDF[word] for i in range(len(df.cleaned)) for word in df.cleaned[i] if word in cleaned_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21427361792654137,\n",
       " 0.15679642593636475,\n",
       " 0.22553293715064382,\n",
       " 0.32703514213584606,\n",
       " 0.3710786923013462,\n",
       " 0.18956119050647116,\n",
       " 0.17673916239216558,\n",
       " 0.32791935437525754,\n",
       " 0.2843462250619213,\n",
       " 0.11451772140000821]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[TF_IDF[w, 0] for w in cleaned_vocab[:10]] # for first comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it was said before, select any of the text classification models for the selected task and train the model. \n",
    "\n",
    "When the model is trained, you need to evaluate it somehow. \n",
    "\n",
    "Read about True positive, False positive, False negative and True negative counts and how to calculate them:   \n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative \n",
    "\n",
    "##### Calculate TP, FP, FN and TN on the test set for your model to measure its performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106912, 300)\n",
      "(52659, 300)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression().fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc_pred = lg.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is: \n",
      "0.9692831487578569\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "print(\"The training accuracy is: \")\n",
    "print(accuracy_score(labels_train, lg.predict(features_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: \n",
      "0.9684194534647449\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "print(\"The test accuracy is: \")\n",
    "print(accuracy_score(labels_test, lrc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step is to calculate  Precision, Recall, F1 and F2 score \n",
    "\n",
    "https://en.wikipedia.org/wiki/Sensitivity_and_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     49828\n",
      "           1       0.94      0.44      0.60      2831\n",
      "\n",
      "    accuracy                           0.97     52659\n",
      "   macro avg       0.96      0.72      0.79     52659\n",
      "weighted avg       0.97      0.97      0.96     52659\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(\"Classification report\")\n",
    "print(classification_report(labels_test,lrc_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate these metrics for the vectorization created using count vectorizing and for tf-idf vectorization.  \n",
    "Compare them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and improvements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all of the vectorization pipelines we used all of the words, which were available in our dictionary, as experiment try to use the most meaningful words - select them using TF-IDF score. (for example for each text you can select not more than 10 words for vectorization, or less). \n",
    "\n",
    "Compare this approach with the first and second ones. Did your model improve? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additionally, visualisations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now you have a vector for each word from your vocabulary. \n",
    "You have vectors with lenght > 18000, so the dimension of your space is more than 18000 - it's impossible to visualise it in 2d space. \n",
    "\n",
    "So try to research and look for algorithms which perform dimensionality reduction. (t-SNE, PCA) \n",
    "Try to visualise obtained vectors in a vectorspace, only subset from the vocabulary, don't plot all of the words. (100) \n",
    "\n",
    "Probably on this step you will realise how this type of vectorization using these techniques is not the best way to vectorize words. \n",
    "\n",
    "Please, analyse the obtained results and explain why visualisation looks like this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

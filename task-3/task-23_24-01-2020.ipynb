{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"task-23_24-01-2020.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"-9CahILYtutU","colab_type":"text"},"source":["## Prerequisites\n","\n"]},{"cell_type":"code","metadata":{"id":"mDnVxoOotutZ","colab_type":"code","colab":{}},"source":["import pandas as pd \n","import numpy as np \n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_qwjnztRtutj","colab_type":"text"},"source":["### Note! Some of these models support only multiclass classification, please, while selecting your dataset,  \n","### be sure that for algorithms which does not support multilabel classification you use only examples with only one label. \n","### Examples without a label in any of the provided categories are clean messages, without any toxicity."]},{"cell_type":"code","metadata":{"id":"KXN9vr5Qtutm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c5b0d05e-c60c-4d7c-9cd7-b6c5e74fd7fb","executionInfo":{"status":"ok","timestamp":1580639453966,"user_tz":-120,"elapsed":1846,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","df = pd.read_csv(\"/content/drive/My Drive/train.csv\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WUV5VkZDtuts","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"07a3c3c5-9f0b-4953-87d6-222f650f860a","executionInfo":{"status":"ok","timestamp":1580639458292,"user_tz":-120,"elapsed":802,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["df.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id  ... identity_hate\n","0  0000997932d777bf  ...             0\n","1  000103f0d9cfb60f  ...             0\n","2  000113f07ec002fd  ...             0\n","3  0001b41b1c6bb37e  ...             0\n","4  0001d958c54c6e35  ...             0\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"oXox1gXgtuty","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"537defd8-1aee-44ba-fbfb-ef3a9bedecdd","executionInfo":{"status":"ok","timestamp":1580639461905,"user_tz":-120,"elapsed":661,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["df.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(159571, 8)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"32zY7m7ctut4","colab_type":"text"},"source":["### As one of the methods to make the training simpier, use only examples, assigned to any category vs clean examples.  \n","For example:  \n","- Select only messages with obscene label == 1  \n","- Select all of the \"clean\" messages  \n","Implement a model which can perform a binary classification  - to understand whether your message is obscene or not.   "]},{"cell_type":"markdown","metadata":{"id":"5X_M1skLtut6","colab_type":"text"},"source":["##### If you want to perform a multilabel classification, please understand the difference between multilabel and multiclass classification and be sure that you are solving the correct task - choose only algorithms applicable for solving this type of problem."]},{"cell_type":"markdown","metadata":{"id":"Ey6dLTV8tut8","colab_type":"text"},"source":["#### To work with multiclass task:  \n","You only need to select messages which have only one label assigned: message cannot be assigned to 2 or more categories.  \n","\n","#### To work with multilabel task: \n","You can work with the whole dataset - some of your messages have only 1 label, some more than 1. "]},{"cell_type":"markdown","metadata":{"id":"w5eOUtAVtut9","colab_type":"text"},"source":["## Text vectorization"]},{"cell_type":"markdown","metadata":{"id":"ZErVlZ0YtuuA","colab_type":"text"},"source":["Previously we worked only with words vectorization. But we need to have a vector for each text, not only words from it. \n","\n","Before starting a text vectorization, please, make sure you are working with clean data - use the dataset created on the previous day. Cleaned from punctuation, stop words, lemmatized or stemmed, etc. "]},{"cell_type":"code","metadata":{"id":"41OIMTIjtuuD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"3cb843e8-46e0-464e-bb60-aec142588dd9","executionInfo":{"status":"ok","timestamp":1580639470816,"user_tz":-120,"elapsed":1397,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["from string import punctuation\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer \n","import nltk\n","\n","nltk.download('stopwords')\n","  \n","lemmatizer = WordNetLemmatizer() \n","stop_words = set(stopwords.words('english'))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RVmVdsrptuuK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"ac50262e-39d6-4376-f164-1317e023f28e","executionInfo":{"status":"ok","timestamp":1580639646501,"user_tz":-120,"elapsed":170080,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["import nltk\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","def preprocess_text(tokenizer, lemmatizer, stop_words, punctuation, text): \n","    tokens = tokenizer(text.lower())\n","    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n","    return [token for token in lemmas if token not in stop_words and token not in punctuation]\n","\n","df['cleaned'] = df.comment_text.apply(lambda x: preprocess_text(word_tokenize, lemmatizer, stop_words, punctuation, x))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QrzHVQVYtuuP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"8ca42b68-617f-4b60-9022-7ef2d3badd39","executionInfo":{"status":"ok","timestamp":1580639658877,"user_tz":-120,"elapsed":669,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["df.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>cleaned</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\r\\nWhy the edits made under my use...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[explanation, edits, made, username, hardcore,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[d'aww, match, background, colour, 'm, seeming...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[hey, man, 'm, really, trying, edit, war, 's, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[``, ca, n't, make, real, suggestion, improvem...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[sir, hero, chance, remember, page, 's]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id  ...                                            cleaned\n","0  0000997932d777bf  ...  [explanation, edits, made, username, hardcore,...\n","1  000103f0d9cfb60f  ...  [d'aww, match, background, colour, 'm, seeming...\n","2  000113f07ec002fd  ...  [hey, man, 'm, really, trying, edit, war, 's, ...\n","3  0001b41b1c6bb37e  ...  [``, ca, n't, make, real, suggestion, improvem...\n","4  0001d958c54c6e35  ...            [sir, hero, chance, remember, page, 's]\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"POMc1ofQtuuU","colab_type":"code","colab":{}},"source":["def flat_nested(nested):\n","    flatten = []\n","    for item in nested:\n","        if isinstance(item, list):\n","            flatten.extend(item)\n","        else:\n","            flatten.append(item)\n","    return flatten"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dUmZTXMktuua","colab_type":"code","colab":{}},"source":["vocab = set(flat_nested(df.cleaned.tolist()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"klafsszstuuf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7ff944b9-8fec-467b-918f-3a8527d714d0","executionInfo":{"status":"ok","timestamp":1580639672199,"user_tz":-120,"elapsed":780,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["len(vocab)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["249736"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"NFy9l1tmtuuk","colab_type":"text"},"source":["As we see, probably you vocabulary is too large.  \n","Let's try to make it smaller.  \n","For example, let's get rig of words, which has counts in our dataset less than some threshold."]},{"cell_type":"code","metadata":{"id":"5alGUlhVtuul","colab_type":"code","colab":{}},"source":["from collections import Counter, defaultdict \n","\n","cnt_vocab = Counter(flat_nested(df.cleaned.tolist()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhNQ037Ntuur","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"outputId":"e447a5d3-8314-4b14-fb7d-55075ce20e47","executionInfo":{"status":"ok","timestamp":1580639684308,"user_tz":-120,"elapsed":768,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["cnt_vocab.most_common(10)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(\"''\", 242528),\n"," ('``', 155370),\n"," ('article', 73284),\n"," (\"'s\", 66767),\n"," (\"n't\", 57144),\n"," ('wa', 56592),\n"," ('page', 56263),\n"," ('wikipedia', 45418),\n"," ('talk', 35356),\n"," ('ha', 31896)]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"_9y4WUh7tuuw","colab_type":"text"},"source":["You can clean words which are shorter that particular length and occur less than N times. "]},{"cell_type":"code","metadata":{"id":"5ahC3DYQtuuy","colab_type":"code","colab":{}},"source":["threshold_count = 10\n","threshold_len = 4 \n","cleaned_vocab = [token for token, count in cnt_vocab.items() if count > threshold_count and len(token) > threshold_len]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Sxz9IEvtuu4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0815a783-bb2a-422a-bc23-0a280b724cb5","executionInfo":{"status":"ok","timestamp":1580639692988,"user_tz":-120,"elapsed":631,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["len(cleaned_vocab)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["18696"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"O3Ef10Dgtuu-","colab_type":"text"},"source":["Much better!  \n","Let's try to vectorize the text summing one-hot vectors for each word. "]},{"cell_type":"code","metadata":{"id":"yHzNnwfttuu_","colab_type":"code","colab":{}},"source":["vocabulary = defaultdict()\n","\n","for i, token in enumerate(cleaned_vocab): \n","    empty_vec = np.zeros(len(cleaned_vocab))\n","    empty_vec[i] = 1 \n","    vocabulary[token] = empty_vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nq5MkzAytuvE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"45299391-5db7-4557-f8eb-6b6b53910bf8","executionInfo":{"status":"ok","timestamp":1580639703319,"user_tz":-120,"elapsed":617,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["vocabulary['source']"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., ..., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"gJOpb7nTtuvI","colab_type":"text"},"source":["Rigth now we have vectors for words (words are one-hot vectorized)  \n","Let's try to create vectors for texts: "]},{"cell_type":"code","metadata":{"id":"YrPV6MM0tuvK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"6bfa30a7-e6ff-4698-f7c8-ea20223d5021","executionInfo":{"status":"ok","timestamp":1580639709430,"user_tz":-120,"elapsed":685,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["sample_text = df.cleaned[10]\n","print(sample_text)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["['``', 'fair', 'use', 'rationale', 'image', 'wonju.jpg', 'thanks', 'uploading', 'image', 'wonju.jpg', 'notice', 'image', 'page', 'specifies', 'image', 'used', 'fair', 'use', 'explanation', 'rationale', 'use', 'wikipedia', 'article', 'constitutes', 'fair', 'use', 'addition', 'boilerplate', 'fair', 'use', 'template', 'must', 'also', 'write', 'image', 'description', 'page', 'specific', 'explanation', 'rationale', 'using', 'image', 'article', 'consistent', 'fair', 'use', 'please', 'go', 'image', 'description', 'page', 'edit', 'include', 'fair', 'use', 'rationale', 'uploaded', 'fair', 'use', 'medium', 'consider', 'checking', 'specified', 'fair', 'use', 'rationale', 'page', 'find', 'list', \"'image\", 'page', 'edited', 'clicking', '``', \"''\", 'contribution', \"''\", \"''\", 'link', 'located', 'top', 'wikipedia', 'page', 'logged', 'selecting', '``', \"''\", 'image', \"''\", \"''\", 'dropdown', 'box', 'note', 'fair', 'use', 'image', 'uploaded', '4', 'may', '2006', 'lacking', 'explanation', 'deleted', 'one', 'week', 'uploaded', 'described', 'criterion', 'speedy', 'deletion', 'question', 'please', 'ask', 'medium', 'copyright', 'question', 'page', 'thank', 'talk', '•', 'contribs', '•', 'unspecified', 'source', 'image', 'wonju.jpg', 'thanks', 'uploading', 'image', 'wonju.jpg', 'noticed', 'file', \"'s\", 'description', 'page', 'currently', 'doe', \"n't\", 'specify', 'created', 'content', 'copyright', 'status', 'unclear', 'create', 'file', 'need', 'specify', 'owner', 'copyright', 'obtained', 'website', 'link', 'website', 'wa', 'taken', 'together', 'restatement', 'website', \"'s\", 'term', 'use', 'content', 'usually', 'sufficient', 'information', 'however', 'copyright', 'holder', 'different', 'website', \"'s\", 'publisher', 'copyright', 'also', 'acknowledged', 'well', 'adding', 'source', 'please', 'add', 'proper', 'copyright', 'licensing', 'tag', 'file', 'doe', \"n't\", 'one', 'already', 'created/took', 'picture', 'audio', 'video', 'tag', 'used', 'release', 'gfdl', 'believe', 'medium', 'meet', 'criterion', 'wikipedia', 'fair', 'use', 'use', 'tag', 'one', 'tag', 'listed', 'wikipedia', 'image', 'copyright', 'tag', 'fair', 'use', 'see', 'wikipedia', 'image', 'copyright', 'tag', 'full', 'list', 'copyright', 'tag', 'use', 'uploaded', 'file', 'consider', 'checking', 'specified', 'source', 'tagged', 'find', 'list', 'file', 'uploaded', 'following', 'link', 'unsourced', 'untagged', 'image', 'may', 'deleted', 'one', 'week', 'tagged', 'described', 'criterion', 'speedy', 'deletion', 'image', 'copyrighted', 'non-free', 'license', 'per', 'wikipedia', 'fair', 'use', 'image', 'deleted', '48', 'hour', 'question', 'please', 'ask', 'medium', 'copyright', 'question', 'page', 'thank', 'talk', '•', 'contribs', '•', '``']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eni7KIdNtuvS","colab_type":"text"},"source":["### One-hot vectorization and count vectorization"]},{"cell_type":"code","metadata":{"id":"XYH36vCUtuvU","colab_type":"code","colab":{}},"source":["sample_vector = np.zeros(len(cleaned_vocab))\n","\n","for token in sample_text: \n","    try: \n","        sample_vector += vocabulary[token]\n","    except KeyError: \n","        continue"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WMFHOc0ituvY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9f0e0319-1e0c-4247-d283-b4ef0a0e754f","executionInfo":{"status":"ok","timestamp":1580639719599,"user_tz":-120,"elapsed":726,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["sample_vector"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([3., 0., 0., ..., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"yvCKSQFOtuvd","colab_type":"text"},"source":["Right now we have count vectorization for our text.   \n","Use this pipeline to create vectors for all of the texts. Save them into np.array. i-th raw in np.array is a vector which represents i-th text from the dataframe.  "]},{"cell_type":"code","metadata":{"id":"AlAsyOe-tuvf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"67071a1c-6c6b-430b-fc87-19758079a153","executionInfo":{"status":"ok","timestamp":1580641162879,"user_tz":-120,"elapsed":1025,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["matrix_size = 1000\n","df_sample = df.cleaned.sample(matrix_size)\n","\n","def vect(size, dataset):\n","  c = 0\n","  text_matrix = np.zeros((size, len(cleaned_vocab)))\n","  for i in dataset:\n","    comment_matrix = np.zeros(len(cleaned_vocab))\n","    for token in i:\n","      try:\n","        comment_matrix = comment_matrix + vocabulary[token]\n","      except KeyError:\n","        continue\n","    text_matrix[c] = comment_matrix\n","    c = c + 1\n","  return text_matrix\n","\n","vectorized_text = vect(matrix_size, df_sample)\n","print(vectorized_text)"],"execution_count":50,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 1. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9TOmnJvatuvj","colab_type":"text"},"source":["### The next step is to train any classification model on top of the received vectors and report the quality. "]},{"cell_type":"markdown","metadata":{"id":"4lXwO_gTtuvk","colab_type":"text"},"source":["Please, select any of the proposed pipelines for performing a text classification task. (Binary, multiclass or multilabel).  "]},{"cell_type":"markdown","metadata":{"id":"X-GxyBWrtuvm","colab_type":"text"},"source":["The main task to calculate our models performance is to create a training and test sets. When you selected a texts for your task, please, use https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html to have at least two sets - train and test.  \n","\n","Train examples you will use to train your model on and test examples to evaluate your model - to understand how your model works on the unseen data. "]},{"cell_type":"markdown","metadata":{"id":"7MenGZKTtuvn","colab_type":"text"},"source":["### Train-test split "]},{"cell_type":"code","metadata":{"id":"wi9k_n8_tuvo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":924},"outputId":"4a3c53f1-ee83-4bcd-f8d2-512881c04740","executionInfo":{"status":"ok","timestamp":1580642059126,"user_tz":-120,"elapsed":961,"user":{"displayName":"Артем Сторожук","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCZGpq74qqfwzFGvgKj5tyyXYEzNgKMs9At6vvu=s64","userId":"07400695560797901351"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","test_data = train_test_split(df, random_state = 100, test_size = 0.70)\n","train_data = train_test_split(df, random_state = 100, test_size = 0.70)\n","\n","print(test_data)\n","print(train_data)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["[                      id  ...                                            cleaned\n","42889   726309d1e5e136b5  ...  [really, error, notice, say, also, 85, etc, wa...\n","151680  810b217d94985b30  ...  [please, delete, user, talk, page, contributio...\n","157991  e6fa18ba13ff84a8  ...  [disgusting, finally, stopped, harassing, thou...\n","23886   3f1438b884901af7  ...  [replied, ..., talk, page, courtesy, note, cas...\n","120706  85bab4f16ab29eba  ...                           [corresponding, section]\n","...                  ...  ...                                                ...\n","82270   dc1a61190e0475dd  ...  [joy, turning, bavarian, creampuff, mass, twit...\n","65615   af8962ff812c1748  ...  [image, garma03.jpg, tagged, image, garma03.jp...\n","77655   d0058dd19c233604  ...  [``, tesla, stuff, notice, ``, '', ugly, ameri...\n","56088   95df37d4a69b607d  ...  [assuming, point, trying, explain, mr, bracket...\n","38408   668ba87c1b6a3f31  ...  [``, plus, take, look, made, outing, edits, si...\n","\n","[47871 rows x 9 columns],                       id  ...                                            cleaned\n","42252   70bbc3e96dd459b1  ...                                    [hammed, cheer]\n","4197    0b2e86f819b4b9a4  ...  [problem, sorry, inconvenience, thank, notific...\n","94042   fb7a63a8e287b2d1  ...  [source, gambia, 2000, summer, olympics, hello...\n","43020   72beff75685cb8dc  ...  [added, criticism, added, much, needed, critic...\n","92647   f7c526a05d03f697  ...  [care, 's, song, 's, great, song, 's, still, s...\n","...                  ...  ...                                                ...\n","156759  d31b791e473683f9  ...  [information, included, article, ankit, fadia,...\n","128061  acf2ec1627c8fb62  ...  [blog, press, release, considered, reliable, r...\n","90111   f12567e549b38e18  ...  [car, wreck, phenomenon, use, detailing, discu...\n","87153   e928fa021aaac6ba  ...  [yes, wanderer, say, 10, word, take, 100, word...\n","145140  17ccfa12203001d4  ...  [``, n't, want, start, edit, war, wo, n't, put...\n","\n","[111700 rows x 9 columns]]\n","[                      id  ...                                            cleaned\n","42889   726309d1e5e136b5  ...  [really, error, notice, say, also, 85, etc, wa...\n","151680  810b217d94985b30  ...  [please, delete, user, talk, page, contributio...\n","157991  e6fa18ba13ff84a8  ...  [disgusting, finally, stopped, harassing, thou...\n","23886   3f1438b884901af7  ...  [replied, ..., talk, page, courtesy, note, cas...\n","120706  85bab4f16ab29eba  ...                           [corresponding, section]\n","...                  ...  ...                                                ...\n","82270   dc1a61190e0475dd  ...  [joy, turning, bavarian, creampuff, mass, twit...\n","65615   af8962ff812c1748  ...  [image, garma03.jpg, tagged, image, garma03.jp...\n","77655   d0058dd19c233604  ...  [``, tesla, stuff, notice, ``, '', ugly, ameri...\n","56088   95df37d4a69b607d  ...  [assuming, point, trying, explain, mr, bracket...\n","38408   668ba87c1b6a3f31  ...  [``, plus, take, look, made, outing, edits, si...\n","\n","[47871 rows x 9 columns],                       id  ...                                            cleaned\n","42252   70bbc3e96dd459b1  ...                                    [hammed, cheer]\n","4197    0b2e86f819b4b9a4  ...  [problem, sorry, inconvenience, thank, notific...\n","94042   fb7a63a8e287b2d1  ...  [source, gambia, 2000, summer, olympics, hello...\n","43020   72beff75685cb8dc  ...  [added, criticism, added, much, needed, critic...\n","92647   f7c526a05d03f697  ...  [care, 's, song, 's, great, song, 's, still, s...\n","...                  ...  ...                                                ...\n","156759  d31b791e473683f9  ...  [information, included, article, ankit, fadia,...\n","128061  acf2ec1627c8fb62  ...  [blog, press, release, considered, reliable, r...\n","90111   f12567e549b38e18  ...  [car, wreck, phenomenon, use, detailing, discu...\n","87153   e928fa021aaac6ba  ...  [yes, wanderer, say, 10, word, take, 100, word...\n","145140  17ccfa12203001d4  ...  [``, n't, want, start, edit, war, wo, n't, put...\n","\n","[111700 rows x 9 columns]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FCAOMUYGtuvt","colab_type":"text"},"source":["### TF-IDF score "]},{"cell_type":"markdown","metadata":{"id":"_05enO31tuvv","colab_type":"text"},"source":["#### Please, review again this article or read it if you have not done it before. \n","\n","https://medium.com/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7"]},{"cell_type":"markdown","metadata":{"id":"FBZK9-MNtuvw","colab_type":"text"},"source":["#### Implement calculating a tf-idf score for each of the words from your vocabulary. \n","\n","The main goal of this taks is to create a dictionary - keys of the dictionary would be tokens and values would be corresponding tf-idf score of the token.\n","\n","#### Calculate it MANUALLY and compare the received scores for words with the sklearn implementation:  \n","from sklearn.feature_extraction.text import TfidfTransformer "]},{"cell_type":"markdown","metadata":{"id":"ogAbktZhtuvx","colab_type":"text"},"source":["#### Tip: \n","\n","##### TF = (Number of time the word occurs in the current text) / (Total number of words in the current text)  \n","\n","##### IDF = (Total number of documents / Number of documents with word t in it)\n","\n","##### TF-IDF = TF*IDF "]},{"cell_type":"markdown","metadata":{"id":"gb_S3mfhtuvy","colab_type":"text"},"source":["When you calculated a tf-idf score for each of the words in your vocabulary - revectorize the texts.  \n","Instead of using number of occurences of the i-th word in the i-th cell of the text vector, use it's tf-idf score.   \n","\n","Revectorize the documents, save vectors into np.array. "]},{"cell_type":"code","metadata":{"id":"miUQQ6zItuv0","colab_type":"code","colab":{}},"source":["### Your code here for obtaining a tf-idf vectorized documents. "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6LINxjZAtuv5","colab_type":"text"},"source":["### Training the model "]},{"cell_type":"markdown","metadata":{"id":"Txst18wDtuv7","colab_type":"text"},"source":["As it was said before, select any of the text classification models for the selected task and train the model. \n","\n","When the model is trained, you need to evaluate it somehow. \n","\n","Read about True positive, False positive, False negative and True negative counts and how to calculate them:   \n","\n","https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative \n","\n","##### Calculate TP, FP, FN and TN on the test set for your model to measure its performance. \n"]},{"cell_type":"code","metadata":{"id":"vkaovd6ctuwD","colab_type":"code","colab":{}},"source":["TP = 0  ## Your code here \n","FP = 0  ## Your code here \n","FN = 0  ## Your code here \n","TN = 0  ## Your code here "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tdidufVgtuwI","colab_type":"text"},"source":["#### The next step is to calculate  Precision, Recall, F1 and F2 score \n","\n","https://en.wikipedia.org/wiki/Sensitivity_and_specificity"]},{"cell_type":"code","metadata":{"id":"HWLfkYFBtuwJ","colab_type":"code","colab":{}},"source":["prec = 0  ## Your code here \n","rec = 0  ## Your code here \n","F1 = 0  ## Your code here \n","F2 = 0  ## Your code here "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mfi-TnwktuwP","colab_type":"text"},"source":["Calculate these metrics for the vectorization created using count vectorizing and for tf-idf vectorization.  \n","Compare them. "]},{"cell_type":"markdown","metadata":{"id":"dBHyCC9ftuwQ","colab_type":"text"},"source":["### Conclusions and improvements "]},{"cell_type":"markdown","metadata":{"id":"3FmxtPybtuwS","colab_type":"text"},"source":["For all of the vectorization pipelines we used all of the words, which were available in our dictionary, as experiment try to use the most meaningful words - select them using TF-IDF score. (for example for each text you can select not more than 10 words for vectorization, or less). \n","\n","Compare this approach with the first and second ones. Did your model improve? \n","\n"]},{"cell_type":"markdown","metadata":{"id":"UTOsl0lgtuwT","colab_type":"text"},"source":["### Additionally, visualisations "]},{"cell_type":"markdown","metadata":{"id":"uGt0aq96tuwU","colab_type":"text"},"source":["For now you have a vector for each word from your vocabulary. \n","You have vectors with lenght > 18000, so the dimension of your space is more than 18000 - it's impossible to visualise it in 2d space. \n","\n","So try to research and look for algorithms which perform dimensionality reduction. (t-SNE, PCA) \n","Try to visualise obtained vectors in a vectorspace, only subset from the vocabulary, don't plot all of the words. (100) \n","\n","Probably on this step you will realise how this type of vectorization using these techniques is not the best way to vectorize words. \n","\n","Please, analyse the obtained results and explain why visualisation looks like this. "]}]}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "task_23_24_01_2020_(1) (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j31YzvXZ75J1",
        "colab_type": "text"
      },
      "source": [
        "## Prerequisites\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC1NtMfM75J7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-MXlJCb75KC",
        "colab_type": "text"
      },
      "source": [
        "### Note! Some of these models support only multiclass classification, please, while selecting your dataset,  \n",
        "### be sure that for algorithms which does not support multilabel classification you use only examples with only one label. \n",
        "### Examples without a label in any of the provided categories are clean messages, without any toxicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ_3sTkG75KF",
        "colab_type": "code",
        "outputId": "e5834c03-42cc-46ec-f0aa-64ccf7138880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(\"/content/drive/My Drive/train.csv\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wabH4jHe75KJ",
        "colab_type": "code",
        "outputId": "fc149109-4fb0-4803-cd6a-b20380843090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmSEu1HA75KP",
        "colab_type": "code",
        "outputId": "e11d64c6-2cb6-40b7-a607-cc2b23bfaf48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(159571, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9bkLmbC75KT",
        "colab_type": "text"
      },
      "source": [
        "### As one of the methods to make the training simpier, use only examples, assigned to any category vs clean examples.  \n",
        "For example:  \n",
        "- Select only messages with obscene label == 1  \n",
        "- Select all of the \"clean\" messages  \n",
        "Implement a model which can perform a binary classification  - to understand whether your message is obscene or not.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_xYhONc75KU",
        "colab_type": "text"
      },
      "source": [
        "##### If you want to perform a multilabel classification, please understand the difference between multilabel and multiclass classification and be sure that you are solving the correct task - choose only algorithms applicable for solving this type of problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0LyAEyZ75KV",
        "colab_type": "text"
      },
      "source": [
        "#### To work with multiclass task:  \n",
        "You only need to select messages which have only one label assigned: message cannot be assigned to 2 or more categories.  \n",
        "\n",
        "#### To work with multilabel task: \n",
        "You can work with the whole dataset - some of your messages have only 1 label, some more than 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryS_V69375KX",
        "colab_type": "text"
      },
      "source": [
        "## Text vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aGY_gux75KZ",
        "colab_type": "text"
      },
      "source": [
        "Previously we worked only with words vectorization. But we need to have a vector for each text, not only words from it. \n",
        "\n",
        "Before starting a text vectorization, please, make sure you are working with clean data - use the dataset created on the previous day. Cleaned from punctuation, stop words, lemmatized or stemmed, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W2P3Lfd75Kb",
        "colab_type": "code",
        "outputId": "1520c3ff-3cee-4a1b-f945-6a1be3b9e0b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "  \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iaXLMQ775Kh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(tokenizer, lemmatizer, stop_words, punctuation, text): \n",
        "    tokens = tokenizer(text.lower())\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return [token for token in lemmas if token not in stop_words and token not in punctuation]\n",
        "\n",
        "df['cleaned'] = df.comment_text.apply(lambda x: preprocess_text(word_tokenize, lemmatizer, stop_words, punctuation, x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuqcjP_N75Kn",
        "colab_type": "code",
        "outputId": "baf785fe-6632-43a7-da55-43c4523fa120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[d'aww, match, background, colour, 'm, seeming...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[hey, man, 'm, really, trying, edit, war, 's, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[``, ca, n't, make, real, suggestion, improvem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[sir, hero, chance, remember, page, 's]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ...                                            cleaned\n",
              "0  0000997932d777bf  ...  [explanation, edits, made, username, hardcore,...\n",
              "1  000103f0d9cfb60f  ...  [d'aww, match, background, colour, 'm, seeming...\n",
              "2  000113f07ec002fd  ...  [hey, man, 'm, really, trying, edit, war, 's, ...\n",
              "3  0001b41b1c6bb37e  ...  [``, ca, n't, make, real, suggestion, improvem...\n",
              "4  0001d958c54c6e35  ...            [sir, hero, chance, remember, page, 's]\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajDpbwBE75Ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_nested(nested):\n",
        "    flatten = []\n",
        "    for item in nested:\n",
        "        if isinstance(item, list):\n",
        "            flatten.extend(item)\n",
        "        else:\n",
        "            flatten.append(item)\n",
        "    return flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo9hEekH75Ky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = set(flat_nested(df.cleaned.tolist()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrrxsxzV75K3",
        "colab_type": "code",
        "outputId": "4d41dd29-3c8a-4648-dc26-b53174103c62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "249736"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L3e57qI75K9",
        "colab_type": "text"
      },
      "source": [
        "As we see, probably you vocabulary is too large.  \n",
        "Let's try to make it smaller.  \n",
        "For example, let's get rig of words, which has counts in our dataset less than some threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-XDaKLZ75K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter, defaultdict \n",
        "\n",
        "cnt_vocab = Counter(flat_nested(df.cleaned.tolist()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S-PXafS75LD",
        "colab_type": "code",
        "outputId": "97eacea0-015d-42cb-82db-199f05429adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "cnt_vocab.most_common(10)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"''\", 242528),\n",
              " ('``', 155370),\n",
              " ('article', 73284),\n",
              " (\"'s\", 66767),\n",
              " (\"n't\", 57144),\n",
              " ('wa', 56592),\n",
              " ('page', 56263),\n",
              " ('wikipedia', 45418),\n",
              " ('talk', 35356),\n",
              " ('ha', 31896)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNKVw79w75LJ",
        "colab_type": "text"
      },
      "source": [
        "You can clean words which are shorter that particular length and occur less than N times. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bkitny475LL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold_count = 10\n",
        "threshold_len = 4 \n",
        "cleaned_vocab = [token for token, count in cnt_vocab.items() if count > threshold_count and len(token) > threshold_len]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfL1AVzS75LQ",
        "colab_type": "code",
        "outputId": "d45417f4-7249-46f9-9809-21330229d8fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(cleaned_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18696"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_pb_D2B75LV",
        "colab_type": "text"
      },
      "source": [
        "Much better!  \n",
        "Let's try to vectorize the text summing one-hot vectors for each word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZPYw-fi75LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary = defaultdict()\n",
        "\n",
        "for i, token in enumerate(cleaned_vocab): \n",
        "    empty_vec = np.zeros(len(cleaned_vocab))\n",
        "    empty_vec[i] = 1 \n",
        "    vocabulary[token] = empty_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiVb8YD375Lc",
        "colab_type": "code",
        "outputId": "6dee8203-1d70-420f-e0de-53058d925895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocabulary['source']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TEZmt3l75Lg",
        "colab_type": "text"
      },
      "source": [
        "Rigth now we have vectors for words (words are one-hot vectorized)  \n",
        "Let's try to create vectors for texts: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o2XqsSR75Lg",
        "colab_type": "code",
        "outputId": "69a28434-129a-464b-c771-6705bdfcbb2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sample_text = df.cleaned[10]\n",
        "print(sample_text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['``', 'fair', 'use', 'rationale', 'image', 'wonju.jpg', 'thanks', 'uploading', 'image', 'wonju.jpg', 'notice', 'image', 'page', 'specifies', 'image', 'used', 'fair', 'use', 'explanation', 'rationale', 'use', 'wikipedia', 'article', 'constitutes', 'fair', 'use', 'addition', 'boilerplate', 'fair', 'use', 'template', 'must', 'also', 'write', 'image', 'description', 'page', 'specific', 'explanation', 'rationale', 'using', 'image', 'article', 'consistent', 'fair', 'use', 'please', 'go', 'image', 'description', 'page', 'edit', 'include', 'fair', 'use', 'rationale', 'uploaded', 'fair', 'use', 'medium', 'consider', 'checking', 'specified', 'fair', 'use', 'rationale', 'page', 'find', 'list', \"'image\", 'page', 'edited', 'clicking', '``', \"''\", 'contribution', \"''\", \"''\", 'link', 'located', 'top', 'wikipedia', 'page', 'logged', 'selecting', '``', \"''\", 'image', \"''\", \"''\", 'dropdown', 'box', 'note', 'fair', 'use', 'image', 'uploaded', '4', 'may', '2006', 'lacking', 'explanation', 'deleted', 'one', 'week', 'uploaded', 'described', 'criterion', 'speedy', 'deletion', 'question', 'please', 'ask', 'medium', 'copyright', 'question', 'page', 'thank', 'talk', '•', 'contribs', '•', 'unspecified', 'source', 'image', 'wonju.jpg', 'thanks', 'uploading', 'image', 'wonju.jpg', 'noticed', 'file', \"'s\", 'description', 'page', 'currently', 'doe', \"n't\", 'specify', 'created', 'content', 'copyright', 'status', 'unclear', 'create', 'file', 'need', 'specify', 'owner', 'copyright', 'obtained', 'website', 'link', 'website', 'wa', 'taken', 'together', 'restatement', 'website', \"'s\", 'term', 'use', 'content', 'usually', 'sufficient', 'information', 'however', 'copyright', 'holder', 'different', 'website', \"'s\", 'publisher', 'copyright', 'also', 'acknowledged', 'well', 'adding', 'source', 'please', 'add', 'proper', 'copyright', 'licensing', 'tag', 'file', 'doe', \"n't\", 'one', 'already', 'created/took', 'picture', 'audio', 'video', 'tag', 'used', 'release', 'gfdl', 'believe', 'medium', 'meet', 'criterion', 'wikipedia', 'fair', 'use', 'use', 'tag', 'one', 'tag', 'listed', 'wikipedia', 'image', 'copyright', 'tag', 'fair', 'use', 'see', 'wikipedia', 'image', 'copyright', 'tag', 'full', 'list', 'copyright', 'tag', 'use', 'uploaded', 'file', 'consider', 'checking', 'specified', 'source', 'tagged', 'find', 'list', 'file', 'uploaded', 'following', 'link', 'unsourced', 'untagged', 'image', 'may', 'deleted', 'one', 'week', 'tagged', 'described', 'criterion', 'speedy', 'deletion', 'image', 'copyrighted', 'non-free', 'license', 'per', 'wikipedia', 'fair', 'use', 'image', 'deleted', '48', 'hour', 'question', 'please', 'ask', 'medium', 'copyright', 'question', 'page', 'thank', 'talk', '•', 'contribs', '•', '``']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSIjPYJe75Ln",
        "colab_type": "text"
      },
      "source": [
        "### One-hot vectorization and count vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSDPz5Ft75Lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_vector = np.zeros(len(cleaned_vocab))\n",
        "\n",
        "for token in sample_text: \n",
        "    try: \n",
        "        sample_vector += vocabulary[token]\n",
        "    except KeyError: \n",
        "        continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOURMODd75Lr",
        "colab_type": "code",
        "outputId": "bc16971d-ac33-41ae-8955-5f43fdcb8faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_vector"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lJE1Hmu75Lw",
        "colab_type": "text"
      },
      "source": [
        "Right now we have count vectorization for our text.   \n",
        "Use this pipeline to create vectors for all of the texts. Save them into np.array. i-th raw in np.array is a vector which represents i-th text from the dataframe.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvNy9Jmg75Ly",
        "colab_type": "code",
        "outputId": "2501e234-2bf8-43f2-bfff-3f714e3511c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "def vectt(text_list):\n",
        "    tvector = np.zeros(len(cleaned_vocab))\n",
        "    for token in text_list: \n",
        "        try: \n",
        "            tvector += vocabulary[token]\n",
        "        except KeyError: \n",
        "            continue\n",
        "    return tvector\n",
        "\n",
        "def dfvect(dataf):\n",
        "  ou = []\n",
        "  for text in dataf.cleaned[0:500]:\n",
        "    ou.append(vectt(text))\n",
        "  return np.array(ou)\n",
        "\n",
        "fift = dfvect(df)\n",
        "print(fift)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPfct7eq75L1",
        "colab_type": "text"
      },
      "source": [
        "### The next step is to train any classification model on top of the received vectors and report the quality. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtPOig5k75L2",
        "colab_type": "text"
      },
      "source": [
        "Please, select any of the proposed pipelines for performing a text classification task. (Binary, multiclass or multilabel).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwoSqztE75L3",
        "colab_type": "text"
      },
      "source": [
        "The main task to calculate our models performance is to create a training and test sets. When you selected a texts for your task, please, use https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html to have at least two sets - train and test.  \n",
        "\n",
        "Train examples you will use to train your model on and test examples to evaluate your model - to understand how your model works on the unseen data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEtjLnz275L4",
        "colab_type": "text"
      },
      "source": [
        "### Train-test split "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfe63Nvj75L5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "103f0d4b-5833-4833-c6ff-f5f036cf0d02"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.33, random_state=42)\n",
        "\n",
        "print(test.head())\n",
        "print(train.head())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      id  ...                                            cleaned\n",
            "119105  7ca72b5b9c688e9e  ...  [geez, forgetful, 've, already, discussed, mar...\n",
            "131631  c03f72fd8f8bf54f  ...  [carioca, rfa, thanks, support, request, admin...\n",
            "125326  9e5b8e8fc1ff2e84  ...  [``, birthday, worry, 's, enjoy, ur, day|talk|...\n",
            "111256  5332799e706665a6  ...  [pseudoscience, category, 'm, assuming, articl...\n",
            "83590   dfa7d8f0b4366680  ...  [phrase, exists, would, provided, search, engi...\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "                      id  ...                                            cleaned\n",
            "29614   4e8d3d4418fea47e  ...  [``, sockpuppetry, case, accused, sockpuppetry...\n",
            "109036  46f7af1e6b292845  ...  ['ve, read, archive, various, national, anarch...\n",
            "110790  50a9e047dc52064c  ...  [wikipedia, encyclopedia, yes, lyric, part, pa...\n",
            "80583   d79990110b82ce70  ...  [mention, “, azeri, genetically, close, armeni...\n",
            "30047   4fc8d57b1e8f2f2a  ...  [``, werdna, 's, rfa, hi, 'm, still, slightly,...\n",
            "\n",
            "[5 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In_SCgN575L8",
        "colab_type": "text"
      },
      "source": [
        "### TF-IDF score "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF-YnjeF75L9",
        "colab_type": "text"
      },
      "source": [
        "#### Please, review again this article or read it if you have not done it before. \n",
        "\n",
        "https://medium.com/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXg2DkTR75L-",
        "colab_type": "text"
      },
      "source": [
        "#### Implement calculating a tf-idf score for each of the words from your vocabulary. \n",
        "\n",
        "The main goal of this taks is to create a dictionary - keys of the dictionary would be tokens and values would be corresponding tf-idf score of the token.\n",
        "\n",
        "#### Calculate it MANUALLY and compare the received scores for words with the sklearn implementation:  \n",
        "from sklearn.feature_extraction.text import TfidfTransformer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hN1L0eH75L_",
        "colab_type": "text"
      },
      "source": [
        "#### Tip: \n",
        "\n",
        "##### TF = (Number of time the word occurs in the current text) / (Total number of words in the current text)  \n",
        "\n",
        "##### IDF = (Total number of documents / Number of documents with word t in it)\n",
        "\n",
        "##### TF-IDF = TF*IDF "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lbvaP8H75MA",
        "colab_type": "text"
      },
      "source": [
        "When you calculated a tf-idf score for each of the words in your vocabulary - revectorize the texts.  \n",
        "Instead of using number of occurences of the i-th word in the i-th cell of the text vector, use it's tf-idf score.   \n",
        "\n",
        "Revectorize the documents, save vectors into np.array. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV1bQH-b75MB",
        "colab_type": "code",
        "outputId": "729fae64-562b-45cd-b8ac-f8043670d01a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "def tf(word, txt, c):\n",
        "  r = 0\n",
        "  t = txt.split()\n",
        "  for i in t:\n",
        "    if i == word:\n",
        "      r += 1\n",
        "  if r != 0:\n",
        "    c += 1\n",
        "  return r/len(t), c\n",
        "\n",
        "def tfidfarr(df,docs):\n",
        "  res = []\n",
        "  for word in docs:\n",
        "    c = 0\n",
        "    wtfl = []\n",
        "    for comment in df['comment_text'][0:1000]:\n",
        "      wtf, c = tf(word,comment,c)\n",
        "      wtfl.append(wtf)\n",
        "    widf = c/len(docs)\n",
        "    res.append(np.array(wtfl) * widf)\n",
        "  return np.array(res)\n",
        "\n",
        "dl = []\n",
        "for j in cnt_vocab.most_common(1000):\n",
        "  dl.append(j[0])\n",
        "\n",
        "npt = tfidfarr(df,dl)\n",
        "npt\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.00278351, 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5vG5rWv75MH",
        "colab_type": "text"
      },
      "source": [
        "### Training the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5y5NgvE75MJ",
        "colab_type": "text"
      },
      "source": [
        "As it was said before, select any of the text classification models for the selected task and train the model. \n",
        "\n",
        "When the model is trained, you need to evaluate it somehow. \n",
        "\n",
        "Read about True positive, False positive, False negative and True negative counts and how to calculate them:   \n",
        "\n",
        "https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative \n",
        "\n",
        "##### Calculate TP, FP, FN and TN on the test set for your model to measure its performance. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsesJm4jYR4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "model = KNeighborsClassifier()\n",
        "\n",
        "tfidf = TfidfVectorizer(vocabulary=cleaned_vocab)\n",
        "x = tfidf.fit_transform(train.cleaned.str.join(' '))\n",
        "y = train.drop(columns = ['id','comment_text','cleaned']).values\n",
        "x_test = tfidf.fit_transform(test.cleaned.str.join(' '))\n",
        "y_test = test.drop(columns = ['id','comment_text','cleaned']).values\n",
        "\n",
        "model.fit(x,y)\n",
        "\n",
        "pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa8SV0_yNLpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a46471b3-8242-4e27-c9ae-4cd72a633e02"
      },
      "source": [
        "pred"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 0, 1, 0, 1, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1TDzYLQ75MK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57706ea7-2ba1-4516-d61e-4f283f107a03"
      },
      "source": [
        "TP = np.sum(np.logical_and(pred == 1, y_test == 1))\n",
        "TN = np.sum(np.logical_and(pred == 0, y_test == 0))\n",
        "FP = np.sum(np.logical_and(pred == 1, y_test == 0))\n",
        "FN = np.sum(np.logical_and(pred == 0, y_test == 1))\n",
        "\n",
        "print(TP, FP, FN, TN)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3553 8117 8153 296131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w-2kyWb75MO",
        "colab_type": "text"
      },
      "source": [
        "#### The next step is to calculate  Precision, Recall, F1 and F2 score \n",
        "\n",
        "https://en.wikipedia.org/wiki/Sensitivity_and_specificity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FPR6RIG75MP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prec = TP/(FP+TP)\n",
        "rec = TP/(TP+FN)\n",
        "F1 = (2*TP)/(2*TP + FP + FN)\n",
        "F2 = (5*prec*rec)/(4*prec + rec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_4UL6L-75MV",
        "colab_type": "text"
      },
      "source": [
        "Calculate these metrics for the vectorization created using count vectorizing and for tf-idf vectorization.  \n",
        "Compare them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqw8UZB575MW",
        "colab_type": "text"
      },
      "source": [
        "### Conclusions and improvements "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zs53_VY75MX",
        "colab_type": "text"
      },
      "source": [
        "For all of the vectorization pipelines we used all of the words, which were available in our dictionary, as experiment try to use the most meaningful words - select them using TF-IDF score. (for example for each text you can select not more than 10 words for vectorization, or less). \n",
        "\n",
        "Compare this approach with the first and second ones. Did your model improve? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhyk43dfnrFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf7365fd-d941-462e-a6be-b41c3475f708"
      },
      "source": [
        "tfidflst = []\n",
        "nptt = npt.transpose()\n",
        "\n",
        "tfidflst = list(nptt[0])\n",
        "\n",
        "topten = []\n",
        "for co in range(10):\n",
        "  top = max(tfidflst)\n",
        "  topten.append(dl[tfidflst.index(top)])\n",
        "  tfidflst.pop(tfidflst.index(top))\n",
        "\n",
        "print(topten)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['page', 'wikipedia', 'u', 'would', 'used', \"'re\", \"'d\", 'number', 'white', \"''\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJYaENPc75MY",
        "colab_type": "text"
      },
      "source": [
        "### Additionally, visualisations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYgw0ZFK75MY",
        "colab_type": "text"
      },
      "source": [
        "For now you have a vector for each word from your vocabulary. \n",
        "You have vectors with lenght > 18000, so the dimension of your space is more than 18000 - it's impossible to visualise it in 2d space. \n",
        "\n",
        "So try to research and look for algorithms which perform dimensionality reduction. (t-SNE, PCA) \n",
        "Try to visualise obtained vectors in a vectorspace, only subset from the vocabulary, don't plot all of the words. (100) \n",
        "\n",
        "Probably on this step you will realise how this type of vectorization using these techniques is not the best way to vectorize words. \n",
        "\n",
        "Please, analyse the obtained results and explain why visualisation looks like this. "
      ]
    }
  ]
}
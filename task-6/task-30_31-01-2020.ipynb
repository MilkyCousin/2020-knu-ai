{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "task-30_31-01-2020.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7gOfbyhNN6y",
        "colab_type": "text"
      },
      "source": [
        "## Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYSZxrGcNN60",
        "colab_type": "code",
        "outputId": "9dd248c9-feeb-44f9-8295-c4b54129d31d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import torch\n",
        "import torch.nn as nn \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from string import punctuation\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOk1OmpfNN63",
        "colab_type": "text"
      },
      "source": [
        "## RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBbzzFZuNN63",
        "colab_type": "text"
      },
      "source": [
        "Please, read about RNNs (Recurrent Neural Networks).  \n",
        "\n",
        "1. Understand it's difference from the FFNNs. (Write your answer down below)  \n",
        "\n",
        "https://towardsdatascience.com/recurrent-neural-networks-rnn-explained-the-eli5-way-3956887e8b75\n",
        "\n",
        "https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7\n",
        "\n",
        "2. Why do we need recurrent neural networks? \n",
        "3. For which tasks it would work better? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRuIFp_bJemE",
        "colab_type": "text"
      },
      "source": [
        "1. Recurrent nets are using their outputs as inputs\n",
        "2. With recurrent networks we can process sequences with different input shape. Also recurrent nets providing some kind of memory.\n",
        "3. Recurrent networks are often applied to NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnGw1BQZNN6-",
        "colab_type": "text"
      },
      "source": [
        "## Load data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhbQRjjCNN6_",
        "colab_type": "code",
        "outputId": "66fc4b18-6609-44b1-cff3-03bc40aa5c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the DF created during the previous task\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(\"/content/drive/My Drive/train.csv\")\n",
        "\n",
        "def preprocess_text(tokenizer, lemmatizer, stop_words, punctuation, text): \n",
        "    tokens = tokenizer(text.lower())\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return [token for token in lemmas if token not in stop_words and token not in punctuation and len(token) > 4 and len(token) < 20]\n",
        "\n",
        "df['cleaned'] = df.comment_text.apply(lambda x: preprocess_text(word_tokenize, lemmatizer, stop_words, punctuation, x))\n",
        "\n",
        "for column in df.columns: \n",
        "    if column not in ['id', 'comment_text', 'cleaned']:\n",
        "        df[column] = df[column].astype('int32')\n",
        "\n",
        "df['toxicity'] = df.iloc[:,2:8].sum(axis=1)\n",
        "clean = df[df['toxicity'] == 0]\n",
        "obscene = df[df['obscene'] == 1]\n",
        "df_binary = clean.append(obscene, ignore_index=True, sort=False)\n",
        "df_binary = df_binary.sample(frac=1)\n",
        "df_binary.reset_index(inplace=True)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loONHu_DNN7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Work with small amount of this data: \n",
        "df_sample, _ = train_test_split(df_binary, test_size=0.7, stratify=df_binary['obscene'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONmg1IKpNN7E",
        "colab_type": "code",
        "outputId": "951cff8d-200b-477f-9d56-0af528a30f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def flat_nested(nested):\n",
        "    flatten = []\n",
        "    for item in nested:\n",
        "        if isinstance(item, list):\n",
        "            flatten.extend(item)\n",
        "        else:\n",
        "            flatten.append(item)\n",
        "    return flatten\n",
        "\n",
        "cnt_vocab = Counter(flat_nested(df_sample.cleaned.tolist()))\n",
        "\n",
        "print(\"Vocab size before filtering: {}\".format(len(cnt_vocab)))\n",
        "\n",
        "threshold_count_l = 1\n",
        "threshold_count_h = 500\n",
        "threshold_len = 2\n",
        "\n",
        "cleaned_vocab = [token for token, count in cnt_vocab.items() if \n",
        "                     threshold_count_h > count > threshold_count_l and len(token) > threshold_len\n",
        "                ]\n",
        "print(\"Vocab size after filtering: {}\".format(len(cleaned_vocab)))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size before filtering: 90423\n",
            "Vocab size after filtering: 35199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sPLlcu8NN7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_vocab.append(\" \")\n",
        "# Convert list to set \n",
        "cleaned_vocab = set(cleaned_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YmssmoyNN7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_to_id = {v: k for k, v in enumerate(sorted(cleaned_vocab))}\n",
        "id_to_token = {v: k for k, v in token_to_id.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt2Iu9X0NN7K",
        "colab_type": "text"
      },
      "source": [
        "Before passing our raw text to the model we need to represent each raw text by a vector.   \n",
        "Let's do this by creating an empty list with all of the tokens in it represented by its id. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXFGI48hNN7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize(data, token_to_id, max_len=None, dtype='int32', batch_first=True):\n",
        "    \"\"\"\n",
        "    Casts a list of tokens into rnn-digestable matrix\n",
        "        \"data\" contains only sequences represented by tokens from the dictionary, filter noise before \n",
        "    \"\"\"\n",
        "    seq_lengths = list(map(len, data))\n",
        "    max_len = max_len or max(map(len, data))\n",
        "    # Create a marix with a shape [batch size, max number of tokens in sequence]\n",
        "    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        line_ix = [token_to_id[c] for c in data[i]]\n",
        "        data_ix[i, :len(line_ix)] = line_ix\n",
        "\n",
        "    return data_ix, seq_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc7np-N8NN7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_noise_tokens(df, cleaned_vocab): \n",
        "    df['filtered_tokens'] = df.cleaned.apply(lambda x: [tok for tok in x if tok in cleaned_vocab])\n",
        "    return df "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrA_BuVMNN7Q",
        "colab_type": "code",
        "outputId": "aafadc0b-1dd3-445d-d120-d314cd3b88c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# After applying this function there would be sentences with all tokens filtered - empty lists. \n",
        "df_sample = filter_noise_tokens(df_sample, cleaned_vocab)\n",
        "\n",
        "# Remove examples without any tokens assigned \n",
        "df_filtered = df_sample[df_sample.astype(str)['filtered_tokens'] != '[]']"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZO6vPLRNN7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform train-test split stratified (would be imbalanced)\n",
        "df_train, df_test = train_test_split(df_filtered, test_size=0.4, stratify=df_filtered['obscene'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwng9RYhNN7U",
        "colab_type": "code",
        "outputId": "bda957b5-fd1d-4015-ec51-ca0c827629e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Train shape: {}\".format(df_train.shape))\n",
        "print(\"Test shape: {}\".format(df_test.shape))"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (25755, 12)\n",
            "Test shape: (17170, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KdNQPWENN7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNLoop(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_tokens, emb_size=200, hid_size=128):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
        "        self.rnn = nn.RNN(emb_size, hid_size, batch_first=True)\n",
        "        self.logits = nn.Linear(hid_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, seq_lengths):\n",
        "        # Embed the obtained sequence \n",
        "        emb = self.emb(x)\n",
        "        # Pack padded sequence - why do we need this, refer to:\n",
        "        # https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "        \n",
        "        pack = torch.nn.utils.rnn.pack_padded_sequence(emb,\n",
        "                                                   seq_lengths,\n",
        "                                                   batch_first=True,\n",
        "                                                   enforce_sorted=False\n",
        "                                                  ) \n",
        "        all_hidden_states, hidden = self.rnn(pack)\n",
        "        logits = self.logits(hidden)\n",
        "        # Cast logits to the range from 0 to 1 \n",
        "        output = self.sigmoid(logits)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYP3ApznNN7X",
        "colab_type": "code",
        "outputId": "8176c874-a745-47ab-a2c1-eceb209ac08a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Initialise the model \n",
        "model = RNNLoop(num_tokens=len(cleaned_vocab))\n",
        "# specify loss function\n",
        "criterion = nn.BCELoss()\n",
        "# specify optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
        "history = []\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "n_iters = df_train.shape[0] // batch_size\n",
        "print(\"Number of iterations for 1 epoch: {}\".format(n_iters))\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0 \n",
        "    for step in range(n_iters):\n",
        "\n",
        "        optimizer.zero_grad()    # Forward pass\n",
        "        # Make a random sample from the dataframe \n",
        "        sample = df_train.sample(batch_size)\n",
        "\n",
        "        # Vectorize the obtained sample \n",
        "        batch_ix, seq_lengths = vectorize(sample.filtered_tokens.tolist(), token_to_id)\n",
        "        # Convert vectorized batch to tensor \n",
        "        batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "        # Select true labels \n",
        "        y_true = sample.obscene.tolist()\n",
        "        # Convert true labels to tensor \n",
        "        y_true = torch.tensor(y_true, dtype=torch.float)\n",
        "\n",
        "        # Make prediction \n",
        "        y_pred = model(batch_ix, seq_lengths)\n",
        "\n",
        "        loss = criterion(y_pred.squeeze(), y_true)\n",
        "\n",
        "        epoch_loss += loss.item() / n_iters\n",
        "        loss.backward()   # Backward pass \n",
        "        optimizer.step()\n",
        "            \n",
        "    print('Epoch {}: train loss: {}'.format(epoch, epoch_loss))    "
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of iterations for 1 epoch: 402\n",
            "Epoch 0: train loss: 0.16399330706497775\n",
            "Epoch 1: train loss: 0.09334906738770389\n",
            "Epoch 2: train loss: 0.07606762232124549\n",
            "Epoch 3: train loss: 0.0681611931122914\n",
            "Epoch 4: train loss: 0.050832275790625665\n",
            "Epoch 5: train loss: 0.04252213836370715\n",
            "Epoch 6: train loss: 0.03742606459554414\n",
            "Epoch 7: train loss: 0.031569908581852484\n",
            "Epoch 8: train loss: 0.026241310798943564\n",
            "Epoch 9: train loss: 0.024904278009142144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgJrpc63NN7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions for test dataset splitting on batches \n",
        "\n",
        "def index_marks(nrows, chunk_size):\n",
        "    return range(1 * chunk_size, (nrows // chunk_size + 1) * chunk_size, chunk_size)\n",
        "\n",
        "def split(df, chunk_size):\n",
        "    indices = index_marks(df.shape[0], chunk_size)\n",
        "    return np.split(df, indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSFEAFnDNN7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_predictions(model, df_test, batch_size, threshold): \n",
        "    n_prints = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    # Split data in batches \n",
        "    test_batches = split(df_test, batch_size)\n",
        "    for batch in test_batches:\n",
        "        if not batch.empty:\n",
        "            batch_ix, seq_lengths = vectorize(batch.filtered_tokens.tolist(), token_to_id)\n",
        "            # Convert vectorized batch to tensor \n",
        "            batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "            # Select true labels \n",
        "            y_true = batch.obscene.tolist()\n",
        "\n",
        "            # Make prediction \n",
        "            y_pred = model(batch_ix, seq_lengths).detach().squeeze().numpy()\n",
        "            # Convert it to binaries \n",
        "            y_pred = [int(pred.item() > threshold) for pred in y_pred]\n",
        "        \n",
        "            # Add them to parallel lists \n",
        "            predictions.extend(y_pred)\n",
        "            true_labels.extend(y_true)\n",
        "        \n",
        "            # Print some examples with obscene documents texts and predicted and true labels \n",
        "            for true, pred, document in zip(y_true, y_pred, batch.comment_text):\n",
        "                if true == 1.0 and n_prints < 10:\n",
        "                    print(\"Predicted label: {}\".format(pred))\n",
        "                    print(\"True label: {}\".format(true))\n",
        "                    print(\"Document: {}\".format(document))\n",
        "                    print(\"*-*-\"*20)\n",
        "                    n_prints += 1\n",
        "        \n",
        "    return true_labels, predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoI4hCJqNN7c",
        "colab_type": "code",
        "outputId": "7d8aceec-acae-49fa-d7fc-02a83c7c82d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "true_labels, predictions = make_predictions(model, df_test, batch_size=64, threshold=0.3)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted label: 0\n",
            "True label: 1\n",
            "Document: A proposal to cut down on AIDS \n",
            "\n",
            "Less AIDS would be spread if you were to stop inserting your minuscule penis into little boys.\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "Document: HEY \n",
            "Nigger. Get a real job you cocksucking jew.\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "Document: \"\n",
            "\n",
            " A barnstar for you! \n",
            "\n",
            "  The Photographer's Barnstar your photos are horrible. You are an idiot. Fuck you    \"\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "Document: Hey \n",
            "\n",
            "Hey guess what.  Not that I told anyone this yet but Larry Sanger sucked my c*****shhh during carnival in Rio back in '03 and I nutted in his mouth so basically I get to do whatever I want on Wikipedia for life.  Sucks for you eh?\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "Document: Dayuum, bitch. , Baby!\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "Document: Faggots, Freaks, Drunks, and Junkies \n",
            "\n",
            "Jim sucks donkey balls\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "Document: Layne Staley\n",
            "who told you that you crackhead, i think someone would hae known whether he had a kid or not which he didnt, DUMB ASS!\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "Document: \"\n",
            "\n",
            "So I've been blocked for this which was to make a point that using rude, aggressive language is not right but Bali ultimate doesnt get blocked for \"\"what the fuck are you on about\"\"?\n",
            "\n",
            "\"\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "Document: \"\n",
            "\n",
            " Tamar Braxton \n",
            "\n",
            "U R retarded!!How can youtube not be a verifiable source. I put a clip of her hitting a D and E one octave above Soprano C(D7 and E7) and you say it's not a good source? What is then! I bet if I even got a video of her hitting whistle notes you'd say it wasn't verifiable. Of course it it. if you see or hear her hitting a whistle note, then she's a whistle registre singer. The same applies to all whsitle register singers!!!! U R just stupid!!!!! U and Mr. \"\"I'll bring the food\"\" who totally destroyed the Whsitle register singers category.\"\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "Document: FUCKIN' HELL! \n",
            "You are such an idiot. Stop capitalising all your words!\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HufYpQdfNN7e",
        "colab_type": "code",
        "outputId": "ad5730e1-2a03-4a18-ca0a-e9f2f1720279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Pring a classification report: \n",
        "\n",
        "print(classification_report(true_labels, predictions))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96     16264\n",
            "           1       0.36      0.47      0.41       906\n",
            "\n",
            "    accuracy                           0.93     17170\n",
            "   macro avg       0.67      0.71      0.69     17170\n",
            "weighted avg       0.94      0.93      0.93     17170\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD9p0fMc2tgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMnet(nn.Module):\n",
        "    def __init__(self, num_tokens, emb_size=200, hid_size=128):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
        "        self.rnn = nn.LSTM(emb_size, hid_size, batch_first=True)\n",
        "        self.logits = nn.Linear(hid_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, seq_lengths): \n",
        "        emb = self.emb(x)\n",
        "        pack = torch.nn.utils.rnn.pack_padded_sequence(emb, seq_lengths, batch_first=True, enforce_sorted=False) \n",
        "        packed_output, (hidden, cell) = self.rnn(pack)\n",
        "        logits = self.logits(hidden.squeeze(0))\n",
        "        output = self.sigmoid(logits)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--zDJFDzI0Fw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e804b1e0-f495-4964-e4b9-058e67398588"
      },
      "source": [
        "#train LSTM on obscene\n",
        "model = LSTMnet(num_tokens=len(cleaned_vocab))\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
        "history = []\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "n_iters = df_train.shape[0] // batch_size\n",
        "print(\"Number of iterations for 1 epoch: {}\".format(n_iters))\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0 \n",
        "    for step in range(n_iters):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        sample = df_train.sample(batch_size)\n",
        "\n",
        "        batch_ix, seq_lengths = vectorize(sample.filtered_tokens.tolist(), token_to_id) \n",
        "        batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "        y_true = sample.obscene.tolist()\n",
        "        y_true = torch.tensor(y_true, dtype=torch.float)\n",
        "\n",
        "        y_pred = model(batch_ix, seq_lengths)\n",
        "\n",
        "        loss = criterion(y_pred.squeeze(), y_true)\n",
        "\n",
        "        epoch_loss += loss.item() / n_iters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "            \n",
        "    print('Epoch {}: train loss: {}'.format(epoch, epoch_loss))    "
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of iterations for 1 epoch: 402\n",
            "Epoch 0: train loss: 0.13710353038834408\n",
            "Epoch 1: train loss: 0.06930935842017477\n",
            "Epoch 2: train loss: 0.04380234466826273\n",
            "Epoch 3: train loss: 0.02714419715463842\n",
            "Epoch 4: train loss: 0.02467759663588945\n",
            "Epoch 5: train loss: 0.016561179003673633\n",
            "Epoch 6: train loss: 0.01505775541404568\n",
            "Epoch 7: train loss: 0.010431133473226979\n",
            "Epoch 8: train loss: 0.009237416144064791\n",
            "Epoch 9: train loss: 0.008229222731683673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afCZM87CI8JQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04b354ac-ea15-49a7-d4ee-b8ab3ad25927"
      },
      "source": [
        "true_labels, predictions = make_predictions(model, df_test, batch_size=64, threshold=0.3)\n",
        "print(classification_report(true_labels, predictions))"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted label: 0\n",
            "True label: 1\n",
            "Document: A proposal to cut down on AIDS \n",
            "\n",
            "Less AIDS would be spread if you were to stop inserting your minuscule penis into little boys.\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "Document: HEY \n",
            "Nigger. Get a real job you cocksucking jew.\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "Document: \"\n",
            "\n",
            " A barnstar for you! \n",
            "\n",
            "  The Photographer's Barnstar your photos are horrible. You are an idiot. Fuck you    \"\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "Document: Hey \n",
            "\n",
            "Hey guess what.  Not that I told anyone this yet but Larry Sanger sucked my c*****shhh during carnival in Rio back in '03 and I nutted in his mouth so basically I get to do whatever I want on Wikipedia for life.  Sucks for you eh?\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "Document: Dayuum, bitch. , Baby!\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "Document: Faggots, Freaks, Drunks, and Junkies \n",
            "\n",
            "Jim sucks donkey balls\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "Document: Layne Staley\n",
            "who told you that you crackhead, i think someone would hae known whether he had a kid or not which he didnt, DUMB ASS!\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "Document: \"\n",
            "\n",
            "So I've been blocked for this which was to make a point that using rude, aggressive language is not right but Bali ultimate doesnt get blocked for \"\"what the fuck are you on about\"\"?\n",
            "\n",
            "\"\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 0\n",
            "True label: 1\n",
            "Document: \"\n",
            "\n",
            " Tamar Braxton \n",
            "\n",
            "U R retarded!!How can youtube not be a verifiable source. I put a clip of her hitting a D and E one octave above Soprano C(D7 and E7) and you say it's not a good source? What is then! I bet if I even got a video of her hitting whistle notes you'd say it wasn't verifiable. Of course it it. if you see or hear her hitting a whistle note, then she's a whistle registre singer. The same applies to all whsitle register singers!!!! U R just stupid!!!!! U and Mr. \"\"I'll bring the food\"\" who totally destroyed the Whsitle register singers category.\"\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "Predicted label: 1\n",
            "True label: 1\n",
            "Document: FUCKIN' HELL! \n",
            "You are such an idiot. Stop capitalising all your words!\n",
            "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97     16264\n",
            "           1       0.51      0.52      0.52       906\n",
            "\n",
            "    accuracy                           0.95     17170\n",
            "   macro avg       0.74      0.75      0.74     17170\n",
            "weighted avg       0.95      0.95      0.95     17170\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFuppSPtOYYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0b11f427-457a-4189-87fd-e2be12603e77"
      },
      "source": [
        "#train LSTM on toxic\n",
        "model = LSTMnet(num_tokens=len(cleaned_vocab))\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
        "history = []\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "n_iters = df_train.shape[0] // batch_size\n",
        "print(\"Number of iterations for 1 epoch: {}\".format(n_iters))\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0 \n",
        "    for step in range(n_iters):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        sample = df_train.sample(batch_size)\n",
        "\n",
        "        batch_ix, seq_lengths = vectorize(sample.filtered_tokens.tolist(), token_to_id) \n",
        "        batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "        y_true = sample.toxic.tolist()\n",
        "        y_true = torch.tensor(y_true, dtype=torch.float)\n",
        "\n",
        "        y_pred = model(batch_ix, seq_lengths)\n",
        "\n",
        "        loss = criterion(y_pred.squeeze(), y_true)\n",
        "\n",
        "        epoch_loss += loss.item() / n_iters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "            \n",
        "    print('Epoch {}: train loss: {}'.format(epoch, epoch_loss))    "
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of iterations for 1 epoch: 402\n",
            "Epoch 0: train loss: 0.12198619825987911\n",
            "Epoch 1: train loss: 0.06342415216747223\n",
            "Epoch 2: train loss: 0.03659168031328449\n",
            "Epoch 3: train loss: 0.027594609049945762\n",
            "Epoch 4: train loss: 0.016940820696781066\n",
            "Epoch 5: train loss: 0.01490897669238837\n",
            "Epoch 6: train loss: 0.013052777406966323\n",
            "Epoch 7: train loss: 0.009750815514315538\n",
            "Epoch 8: train loss: 0.009463562128989883\n",
            "Epoch 9: train loss: 0.010163960251625593\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_FeiLx2OY9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f76e06da-218e-4c33-d0fd-f395dfe039dd"
      },
      "source": [
        "def make_predictions(model, df_test, batch_size, threshold): \n",
        "    n_prints = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    test_batches = split(df_test, batch_size)\n",
        "    for batch in test_batches:\n",
        "        if not batch.empty:\n",
        "            batch_ix, seq_lengths = vectorize(batch.filtered_tokens.tolist(), token_to_id)\n",
        "\n",
        "            batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "            y_true = batch.toxic.tolist()\n",
        "            y_pred = model(batch_ix, seq_lengths).detach().squeeze().numpy() \n",
        "            y_pred = [int(pred.item() > threshold) for pred in y_pred]\n",
        "        \n",
        "\n",
        "            predictions.extend(y_pred)\n",
        "            true_labels.extend(y_true)\n",
        "\n",
        "    return true_labels, predictions\n",
        "\n",
        "true_labels, predictions = make_predictions(model, df_test, batch_size=64, threshold=0.3)\n",
        "print(classification_report(true_labels, predictions))"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.97     16336\n",
            "           1       0.49      0.52      0.50       834\n",
            "\n",
            "    accuracy                           0.95     17170\n",
            "   macro avg       0.73      0.74      0.74     17170\n",
            "weighted avg       0.95      0.95      0.95     17170\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3DtBvzmKmXn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#balance dataset\n",
        "def balance_df(df):\n",
        "    df = pd.DataFrame(df)\n",
        "    obscene = pd.DataFrame(df[df['obscene']==1])\n",
        "    num_obscene = obscene.shape[0]\n",
        "    categories = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "    for c in categories:\n",
        "            df = df[df[c] == 0]\n",
        "    df = df.sample(num_obscene)\n",
        "    obscene = obscene.append(df)\n",
        "    res = obscene.sample(frac=1)\n",
        "    return res\n",
        "\n",
        "df_sample = filter_noise_tokens(df_binary, cleaned_vocab)\n",
        "df_filtered = df_sample[df_sample.astype(str)['filtered_tokens'] != '[]']\n",
        "df_train, df_test = train_test_split(df_filtered, test_size=0.2, stratify=df_filtered['obscene'])\n",
        "\n",
        "df_train = balance_df(df_train) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jylY5lKOJLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2e5cfb0a-be8f-4b42-a024-9e93d9b3d773"
      },
      "source": [
        "#train on toxic, but on balanced by obscene\n",
        "model = LSTMnet(num_tokens=len(cleaned_vocab))\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
        "history = []\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 10\n",
        "n_iters = df_train.shape[0] // batch_size\n",
        "print(\"Number of iterations for 1 epoch: {}\".format(n_iters))\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0 \n",
        "    for step in range(n_iters):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        sample = df_train.sample(batch_size)\n",
        "\n",
        "        batch_ix, seq_lengths = vectorize(sample.filtered_tokens.tolist(), token_to_id) \n",
        "        batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "        y_true = sample.toxic.tolist()\n",
        "        y_true = torch.tensor(y_true, dtype=torch.float)\n",
        "\n",
        "        y_pred = model(batch_ix, seq_lengths)\n",
        "\n",
        "        loss = criterion(y_pred.squeeze(), y_true)\n",
        "\n",
        "        epoch_loss += loss.item() / n_iters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "            \n",
        "    print('Epoch {}: train loss: {}'.format(epoch, epoch_loss))   "
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of iterations for 1 epoch: 186\n",
            "Epoch 0: train loss: 0.38721716628279734\n",
            "Epoch 1: train loss: 0.21900644421737678\n",
            "Epoch 2: train loss: 0.14094438809420795\n",
            "Epoch 3: train loss: 0.09143749080718526\n",
            "Epoch 4: train loss: 0.06720119281872226\n",
            "Epoch 5: train loss: 0.05604481067688715\n",
            "Epoch 6: train loss: 0.03504557028645649\n",
            "Epoch 7: train loss: 0.030621828294799782\n",
            "Epoch 8: train loss: 0.031731418567404204\n",
            "Epoch 9: train loss: 0.033035006839156104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn4D0jBrOLl7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "e497c495-fe07-4b9b-d33b-d2739a3c58d6"
      },
      "source": [
        "true_labels, predictions = make_predictions(model, df_test, batch_size=64, threshold=0.3)\n",
        "print(classification_report(true_labels, predictions))"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.81      0.89     27035\n",
            "           1       0.18      0.79      0.29      1400\n",
            "\n",
            "    accuracy                           0.81     28435\n",
            "   macro avg       0.58      0.80      0.59     28435\n",
            "weighted avg       0.95      0.81      0.86     28435\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpZ7L5_yNN7g",
        "colab_type": "text"
      },
      "source": [
        "## Task\n",
        "\n",
        "1. Make a dataset balanced: for example select all of the obscene messages, calculate its number and sample from the clean messages equal number of examples. **(1)See if it increased your score on toxic messages.** \n",
        "\n",
        "As the **additional** task you can modify your dataset sampling during the training/testing. Read about Datasets, DataSamplers and DataLoaders in pytorch. Try to apply them. \n",
        "\n",
        "\n",
        "2. Read about RNNs different types (LSTMs and GRUs): \n",
        "  https://colah.github.io/posts/2015-08-Understanding-LSTMs/  \n",
        "\n",
        "  https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21 \n",
        "  \n",
        "  **(2)What is the difference between RNN and LSTM? Why do we need LSTM? Explain it in your own words.**  \n",
        "  \n",
        "  **(3)What is the difference between LSTM and GRU? Explain it in your own words.** \n",
        "  \n",
        "  \n",
        "3. Modify your network to make it possible to work with nn.LSTM or nn.GRU layers. (Their outputs may be a little bit defferent from nn.RNN, so be careful to modify your code accordingly). \n",
        "\n",
        "4. Compare all of the previous examples: classification with RNN (or LSTM/GRU) and FFNN. **(4)Which one performed better according to the metrics? (5)To the time?**\n",
        "\n",
        "5. **(6)How dataset imbalancing are influencing your model? Read about dataset imbalancing and about possibilities to handle them. (7)Write down below what can we do with it, or implement a solution.** \n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTpO8gz2NN7g",
        "colab_type": "text"
      },
      "source": [
        "Please, answer the questions 1-7 and write your answers down below: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDbi3R14PSSU",
        "colab_type": "text"
      },
      "source": [
        "1. Scores become worse.\n",
        "2. LSTM has different layer structure. It uses gates to manage data flow. LSTM was designed to solve short-term memory problem.\n",
        "3. GRU is variation of LSTM, it was designed to solve vanishing gradient problem.\n",
        "4.LSTM net\n",
        "5.FFNN\n",
        "6.__\n",
        "7.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ4IILxPxBEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, read about RNNs (Recurrent Neural Networks).  \n",
    "\n",
    "1. Understand it's difference from the FFNNs. (Write your answer down below)  \n",
    "\n",
    "https://towardsdatascience.com/recurrent-neural-networks-rnn-explained-the-eli5-way-3956887e8b75\n",
    "\n",
    "https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7\n",
    "\n",
    "2. Why do we need recurrent neural networks? \n",
    "3. For which tasks it would work better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Your answer here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Your answer here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Your answer here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60236</td>\n",
       "      <td>b3925e41b823f473</td>\n",
       "      <td>\"\\n\\nThank you Ian. I knew about WP:NOTCENSORE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[``, thank, ian, knew, wp, notcensored, also, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116612</td>\n",
       "      <td>b686d9f97deab4ad</td>\n",
       "      <td>Oh. I never took your comments in any negative...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[oh, never, took, comment, negative, way, perf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72935</td>\n",
       "      <td>d96a1c99002f9cfc</td>\n",
       "      <td>Village pump and newbie \\n\\nI think your handl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[village, pump, newbie, think, handling, newbi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30137</td>\n",
       "      <td>59a0576f85786c1f</td>\n",
       "      <td>I didn't change it hence this BS claim of me s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[n't, change, hence, b, claim, saying, keep, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148580</td>\n",
       "      <td>0f701c200f54455c</td>\n",
       "      <td>What the hell do you people expect? Wikipedia'...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[hell, people, expect, wikipedia, 's, controll...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                id  \\\n",
       "0   60236  b3925e41b823f473   \n",
       "1  116612  b686d9f97deab4ad   \n",
       "2   72935  d96a1c99002f9cfc   \n",
       "3   30137  59a0576f85786c1f   \n",
       "4  148580  0f701c200f54455c   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  \"\\n\\nThank you Ian. I knew about WP:NOTCENSORE...      0             0   \n",
       "1  Oh. I never took your comments in any negative...      0             0   \n",
       "2  Village pump and newbie \\n\\nI think your handl...      0             0   \n",
       "3  I didn't change it hence this BS claim of me s...      0             0   \n",
       "4  What the hell do you people expect? Wikipedia'...      1             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        1       0       1              1   \n",
       "\n",
       "                                             cleaned  toxicity  \n",
       "0  [``, thank, ian, knew, wp, notcensored, also, ...         0  \n",
       "1  [oh, never, took, comment, negative, way, perf...         0  \n",
       "2  [village, pump, newbie, think, handling, newbi...         0  \n",
       "3  [n't, change, hence, b, claim, saying, keep, a...         0  \n",
       "4  [hell, people, expect, wikipedia, 's, controll...         4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the DF created during the previous task\n",
    "\n",
    "df_binary = pd.read_json(\"../jigsaw-toxic-comment-classification-challenge/df_binary.json\")\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with small amount of this data: \n",
    "df_sample, _ = train_test_split(df_binary, test_size=0.9, stratify=df_binary['obscene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size before filtering: 50787\n",
      "Vocab size after filtering: 13304\n"
     ]
    }
   ],
   "source": [
    "def flat_nested(nested):\n",
    "    flatten = []\n",
    "    for item in nested:\n",
    "        if isinstance(item, list):\n",
    "            flatten.extend(item)\n",
    "        else:\n",
    "            flatten.append(item)\n",
    "    return flatten\n",
    "\n",
    "cnt_vocab = Counter(flat_nested(df_sample.cleaned.tolist()))\n",
    "\n",
    "print(\"Vocab size before filtering: {}\".format(len(cnt_vocab)))\n",
    "\n",
    "threshold_count_l = 2\n",
    "threshold_count_h = 500\n",
    "threshold_len = 2\n",
    "\n",
    "cleaned_vocab = [token for token, count in cnt_vocab.items() if \n",
    "                     threshold_count_h > count > threshold_count_l and len(token) > threshold_len\n",
    "                ]\n",
    "print(\"Vocab size after filtering: {}\".format(len(cleaned_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_vocab.append(\" \")\n",
    "# Convert list to set \n",
    "cleaned_vocab = set(cleaned_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {v: k for k, v in enumerate(sorted(cleaned_vocab))}\n",
    "id_to_token = {v: k for k, v in token_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing our raw text to the model we need to represent each raw text by a vector.   \n",
    "Let's do this by creating an empty list with all of the tokens in it represented by its id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, token_to_id, max_len=None, dtype='int32', batch_first = True):\n",
    "    \"\"\"\n",
    "    Casts a list of tokens into rnn-digestable matrix\n",
    "        \"data\" contains only sequences represented by tokens from the dictionary, filter noise before \n",
    "    \"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, data))\n",
    "    # Create a marix with a shape [batch size, max number of tokens in sequence]\n",
    "    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        line_ix = [token_to_id[c] for c in data[i]]\n",
    "        data_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        data_ix = np.transpose(data_ix)\n",
    "\n",
    "    return data_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_noise_tokens(df, cleaned_vocab): \n",
    "    df['filtered_tokens'] = df.cleaned.apply(lambda x: [tok for tok in x if tok in cleaned_vocab])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dorhunova/Projects/Python/flair_research/flair_ner/env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# After applying this function there would be sentences with all tokens filtered - empty lists. \n",
    "df_sample = filter_noise_tokens(df_sample, cleaned_vocab)\n",
    "\n",
    "# Remove examples without any tokens assigned \n",
    "df_filtered = df_sample[df_sample.astype(str)['filtered_tokens'] != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split (would be imbalanced)\n",
    "# df_train, df_test = train_test_split(df_filtered, test_size=0.4, stratify=df_filtered['obscene'])\n",
    "\n",
    "# Select only obscene texts\n",
    "df_obscene = df_filtered[df_filtered['obscene'] == 1] \n",
    "# Select only clean texts \n",
    "df_clean = df_filtered[df_filtered['toxic'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a balanced dataset, number of 1 == number of 0 \n",
    "df_balanced = df_obscene.append(df_clean.sample(df_obscene.shape[0]), ignore_index=True, sort=False)\n",
    "\n",
    "df_train, df_test = train_test_split(df_balanced, test_size=0.4, stratify=df_balanced['obscene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (981, 12)\n",
      "Test shape: (655, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape: {}\".format(df_train.shape))\n",
    "print(\"Test shape: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5413,  1260,  6763,   482,  7202, 12923,   216,  8349,  3511,\n",
       "         2812, 10265,  9325,  3294, 11470,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [ 5668,   728,  3595,  5522,  1404,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [ 3293,  7185,   570, 10795, 10039,  8531,  8527,  9163,  3653,\n",
       "        12073,  9033,   571,  2863,  5517, 12393,  7688,  8683,  1209,\n",
       "         2613,  2893,   905,  6844,  2662, 12392,  6447, 12838,  9083,\n",
       "         7152, 12045,  6574, 12045, 12227,  8724,  4733,  8683, 11065,\n",
       "         7152,  2797,  6574,  1119, 12181,  4527, 12836,  3318,  5599,\n",
       "         9568,  5925,  8553,  1261, 12074,  6250, 10055,   571, 10076,\n",
       "         3318,  9539,  1706, 12897,  1404,  3318,   495, 11009,  2412,\n",
       "         4419, 11009,   579, 11924, 11009,  7629,  4560,  7778,  9539,\n",
       "         8301,  4789, 12392,  3318,  9991, 12328,   493,   528,  4426,\n",
       "         4559, 11117,  9867,  9375,   425, 10048,  3320, 12450, 11242,\n",
       "        12428,  4760,  4760, 12972, 11032,  1032, 13160,  6958,  6389,\n",
       "        11117,   790,   571, 11923, 11117, 13162],\n",
       "       [ 4790, 12227,  5624,  8755, 13214, 11129,  7818, 10092,  5413,\n",
       "         5762, 13146, 11924,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of vectorization \n",
    "\n",
    "vectorize(df_train.sample(4).filtered_tokens.tolist(), token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLoop(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_tokens, emb_size=200, hid_size=128):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
    "        self.rnn = nn.RNN(emb_size, hid_size, batch_first=True)\n",
    "        self.logits = nn.Linear(hid_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        all_hidden_states, hidden = self.rnn(emb)\n",
    "        logits = self.logits(hidden)\n",
    "        output = self.sigmoid(logits)\n",
    "        return output\n",
    "    \n",
    "# Initialise the model \n",
    "model = RNNLoop(num_tokens=len(cleaned_vocab))\n",
    "# specify loss function\n",
    "criterion = nn.BCELoss()\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations for 1 epoch: 15\n",
      "Epoch 0: train loss: 0.707436223824819\n",
      "Epoch 1: train loss: 0.7060327967007954\n",
      "Epoch 2: train loss: 0.7112535278002421\n",
      "Epoch 3: train loss: 0.6693460702896118\n",
      "Epoch 4: train loss: 0.6519306083520253\n",
      "Epoch 5: train loss: 0.712637746334076\n",
      "Epoch 6: train loss: 0.680217734972636\n",
      "Epoch 7: train loss: 0.684671096007029\n",
      "Epoch 8: train loss: 0.6827967087427774\n",
      "Epoch 9: train loss: 0.708529508113861\n",
      "Epoch 10: train loss: 0.6483648419380189\n",
      "Epoch 11: train loss: 0.7002609113852183\n",
      "Epoch 12: train loss: 0.6998960673809053\n",
      "Epoch 13: train loss: 0.6939862966537476\n",
      "Epoch 14: train loss: 0.7067749937375387\n",
      "Epoch 15: train loss: 0.6987305363019307\n",
      "Epoch 16: train loss: 0.6728693942228954\n",
      "Epoch 17: train loss: 0.6675439238548277\n",
      "Epoch 18: train loss: 0.6990039388338725\n",
      "Epoch 19: train loss: 0.6623107075691224\n",
      "Epoch 20: train loss: 0.6954385121663412\n",
      "Epoch 21: train loss: 0.7057859341303507\n",
      "Epoch 22: train loss: 0.6943241635958353\n",
      "Epoch 23: train loss: 0.6917557875315348\n",
      "Epoch 24: train loss: 0.6870120386282603\n",
      "Epoch 25: train loss: 0.6979837258656819\n",
      "Epoch 26: train loss: 0.6738773385683696\n",
      "Epoch 27: train loss: 0.6913922627766926\n",
      "Epoch 28: train loss: 0.6942109147707621\n",
      "Epoch 29: train loss: 0.6814329266548157\n",
      "Epoch 30: train loss: 0.6955336173375448\n",
      "Epoch 31: train loss: 0.6701817075411479\n",
      "Epoch 32: train loss: 0.6734205524126688\n",
      "Epoch 33: train loss: 0.6715410033861796\n",
      "Epoch 34: train loss: 0.6259323716163636\n",
      "Epoch 35: train loss: 0.702410364151001\n",
      "Epoch 36: train loss: 0.6659335772196453\n",
      "Epoch 37: train loss: 0.6897687991460163\n",
      "Epoch 38: train loss: 0.6978464086850484\n",
      "Epoch 39: train loss: 0.6755746285120646\n",
      "Epoch 40: train loss: 0.7077447732289632\n",
      "Epoch 41: train loss: 0.6782791495323182\n",
      "Epoch 42: train loss: 0.678572412331899\n",
      "Epoch 43: train loss: 0.6678314367930094\n",
      "Epoch 44: train loss: 0.6877111713091533\n",
      "Epoch 45: train loss: 0.6596780558427175\n",
      "Epoch 46: train loss: 0.6815252621968586\n",
      "Epoch 47: train loss: 0.7045198996861777\n",
      "Epoch 48: train loss: 0.6797356724739074\n",
      "Epoch 49: train loss: 0.6752882838249207\n",
      "Epoch 50: train loss: 0.6967496752738952\n",
      "Epoch 51: train loss: 0.6356163104375203\n",
      "Epoch 52: train loss: 0.6723118583361307\n",
      "Epoch 53: train loss: 0.6906186620394389\n",
      "Epoch 54: train loss: 0.6968361059824626\n",
      "Epoch 55: train loss: 0.6775372703870136\n",
      "Epoch 56: train loss: 0.6580363988876342\n",
      "Epoch 57: train loss: 0.6624830444653829\n",
      "Epoch 58: train loss: 0.667261521021525\n",
      "Epoch 59: train loss: 0.6856561501820884\n",
      "Epoch 60: train loss: 0.6610657294591268\n",
      "Epoch 61: train loss: 0.6796421607335409\n",
      "Epoch 62: train loss: 0.6645745952924093\n",
      "Epoch 63: train loss: 0.6867054005463918\n",
      "Epoch 64: train loss: 0.6601038416226704\n",
      "Epoch 65: train loss: 0.6736618638038636\n",
      "Epoch 66: train loss: 0.6834478219350179\n",
      "Epoch 67: train loss: 0.6788130760192872\n",
      "Epoch 68: train loss: 0.7069003025690714\n",
      "Epoch 69: train loss: 0.660098393758138\n",
      "Epoch 70: train loss: 0.6697486837704976\n",
      "Epoch 71: train loss: 0.6941494067509969\n",
      "Epoch 72: train loss: 0.6533538500467936\n",
      "Epoch 73: train loss: 0.6858398338158925\n",
      "Epoch 74: train loss: 0.6168927729129791\n",
      "Epoch 75: train loss: 0.7347482919692992\n",
      "Epoch 76: train loss: 0.6142962614695231\n",
      "Epoch 77: train loss: 0.6698448340098063\n",
      "Epoch 78: train loss: 0.6509701629479725\n",
      "Epoch 79: train loss: 0.6464833696683248\n",
      "Epoch 80: train loss: 0.6573457837104797\n",
      "Epoch 81: train loss: 0.6306754589080811\n",
      "Epoch 82: train loss: 0.6650404930114746\n",
      "Epoch 83: train loss: 0.679953936735789\n",
      "Epoch 84: train loss: 0.6494606693585714\n",
      "Epoch 85: train loss: 0.6493617097536724\n",
      "Epoch 86: train loss: 0.6721476952234905\n",
      "Epoch 87: train loss: 0.63804718653361\n",
      "Epoch 88: train loss: 0.6601077338059743\n",
      "Epoch 89: train loss: 0.6547241330146791\n",
      "Epoch 90: train loss: 0.6653302351633709\n",
      "Epoch 91: train loss: 0.6197139581044515\n",
      "Epoch 92: train loss: 0.6686510046323141\n",
      "Epoch 93: train loss: 0.6603468815485635\n",
      "Epoch 94: train loss: 0.6755578676859537\n",
      "Epoch 95: train loss: 0.6537165681521099\n",
      "Epoch 96: train loss: 0.6626662214597066\n",
      "Epoch 97: train loss: 0.6537193894386292\n",
      "Epoch 98: train loss: 0.6649479309717815\n",
      "Epoch 99: train loss: 0.6758101701736451\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "n_epochs = 100 \n",
    "n_iters = df_train.shape[0] // batch_size\n",
    "print(\"Number of iterations for 1 epoch: {}\".format(n_iters))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0 \n",
    "    for step in range(n_iters):\n",
    "\n",
    "        optimizer.zero_grad()    # Forward pass\n",
    "        # Make a random sample from the dataframe \n",
    "        sample = df_train.sample(4)\n",
    "\n",
    "        # Vectorize the obtained sample \n",
    "        batch_ix = vectorize(sample.filtered_tokens.tolist(), token_to_id)\n",
    "        # Convert vectorized batch to tensor \n",
    "        batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
    "\n",
    "        # Select true labels \n",
    "        y_true = sample.obscene.tolist()\n",
    "        # Convert true labels to tensor \n",
    "        y_true = torch.tensor(y_true, dtype=torch.float)\n",
    "\n",
    "        # Make prediction \n",
    "        y_pred = model(batch_ix)\n",
    "\n",
    "        loss = criterion(y_pred.squeeze(), y_true)\n",
    "\n",
    "        epoch_loss += loss.item() / n_iters\n",
    "        loss.backward()   # Backward pass \n",
    "        optimizer.step()\n",
    "\n",
    "#         history.append(loss.data.numpy())\n",
    "#         if (step + 1) % 100 == 0:\n",
    "#             clear_output(True)\n",
    "#             plt.plot(history, label='loss')\n",
    "#             plt.legend()\n",
    "#             plt.show()\n",
    "            \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, epoch_loss))    \n",
    "# assert np.mean(history[:25]) > np.mean(history[-25:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

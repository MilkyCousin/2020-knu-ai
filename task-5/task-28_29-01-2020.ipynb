{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch==1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../jigsaw-toxic-comment-classification-challenge/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[d'aww, match, background, colour, 'm, seeming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, 'm, really, trying, edit, war, 's, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[``, ca, n't, make, real, suggestion, improvem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page, 's]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                             cleaned  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [d'aww, match, background, colour, 'm, seeming...  \n",
       "2  [hey, man, 'm, really, trying, edit, war, 's, ...  \n",
       "3  [``, ca, n't, make, real, suggestion, improvem...  \n",
       "4            [sir, hero, chance, remember, page, 's]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn pytorch basics, this framework will help you to build simple neural networks during this task.   \n",
    "The first neural network we will try to learn is Feed Forward Neural Network which contain one Fully Connected Layer.  \n",
    "It can have 1 or more fully connected layers, also it could be called as MLP - multilayer perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read about PyTorch here:  \n",
    "https://en.wikipedia.org/wiki/PyTorch\n",
    "\n",
    "And here:\n",
    "\n",
    "https://neurohive.io/ru/tutorial/glubokoe-obuchenie-s-pytorch/\n",
    "\n",
    "While reading these articles probably you will meet some unknown terms: \n",
    "backpropagation algorithm, gradient descent, activation function, loss function, etc.  \n",
    "Please, try to look for an information about why do you need all of these stuff. \n",
    "\n",
    "Answer this questions about Neural Nets: \n",
    "\n",
    "1. In previous tasks we created some features manually, tried to weight our features, tried to select special words for vectorization, how deep learning solves this problem? \n",
    "\n",
    "2. Why do we work with tensors in PyTorch?\n",
    "\n",
    "3. Please, find and read information - why do we need an activation functions in our models? Please, refer to the XOR problem with MLP without activation function, find information about it and answer the previous question. \n",
    "\n",
    "4. Please, answer the following question - what gradient is? Why do we need gradient descent algorithm? Which problem it solves? \n",
    "\n",
    "5. What is backpropagation algorithm? \n",
    "\n",
    "6. What is loss function? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer for the question number 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer for the question number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer for the question number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer for the question number 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer for the question number 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer for the question number 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the following article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Feedforward_neural_network\n",
    "\n",
    "What is FFNN? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor:\n",
    "x = torch.ones(1, requires_grad=True)\n",
    "\n",
    "print(x.grad)    # returns None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(x.grad) is None because a tensor x is a scalar, so there is nothing to be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([84.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, requires_grad=True)\n",
    "y = 20 + x\n",
    "z = (y ** 2) * 2 \n",
    "z.backward()     # auto gradient calculation\n",
    "\n",
    "print(x.grad)    # ∂z/∂x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[d'aww, match, background, colour, 'm, seeming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, 'm, really, trying, edit, war, 's, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[``, ca, n't, make, real, suggestion, improvem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page, 's]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                             cleaned  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [d'aww, match, background, colour, 'm, seeming...  \n",
       "2  [hey, man, 'm, really, trying, edit, war, 's, ...  \n",
       "3  [``, ca, n't, make, real, suggestion, improvem...  \n",
       "4            [sir, hero, chance, remember, page, 's]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify labels dtype to 'int', to make summarizing them possible\n",
    "for column in df.columns: \n",
    "    if column not in ['id', 'comment_text', 'cleaned']:\n",
    "        df[column] = df[column].astype('int32')\n",
    "        \n",
    "# Create a toxicity column (sums all of the toxic labels)\n",
    "df['toxicity'] = df.iloc[:,2:8].sum(axis=1)\n",
    "\n",
    "# Clean data - where toxicity is == 0 \n",
    "clean = df[df['toxicity'] == 0]\n",
    "# Messages, which were labelled as obscene\n",
    "obscene = df[df['obscene'] == 1]\n",
    "\n",
    "# Create a dataset for binary classification \n",
    "df_binary = clean.append(obscene, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "df_binary = df_binary.sample(frac=1)\n",
    "\n",
    "# Reset index of the pd.DataFrame\n",
    "df_binary.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84691</td>\n",
       "      <td>fc39f476f0c1dc1a</td>\n",
       "      <td>Random IP \\n\\nThanks for the reply. Was probab...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[random, ip, thanks, reply, wa, probably, ip, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51036</td>\n",
       "      <td>97feb245e31f30d4</td>\n",
       "      <td>\"\\n Archive Index \\n\\n\\nThis report has been g...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[``, archive, index, report, ha, generated, re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19824</td>\n",
       "      <td>3a433606e97303ef</td>\n",
       "      <td>\"\\nSupport, noting that slip knot is already a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[``, support, noting, slip, knot, already, ``,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53884</td>\n",
       "      <td>a0b6ff8c1412c229</td>\n",
       "      <td>This is your last warning. You will be blocked...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[last, warning, blocked, editing, next, time, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110010</td>\n",
       "      <td>8f2a5036f200188b</td>\n",
       "      <td>\"\\n\\nHaha. I can tell you that half of the wor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[``, haha, tell, half, world, ha, even, heard,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                id  \\\n",
       "0   84691  fc39f476f0c1dc1a   \n",
       "1   51036  97feb245e31f30d4   \n",
       "2   19824  3a433606e97303ef   \n",
       "3   53884  a0b6ff8c1412c229   \n",
       "4  110010  8f2a5036f200188b   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Random IP \\n\\nThanks for the reply. Was probab...      0             0   \n",
       "1  \"\\n Archive Index \\n\\n\\nThis report has been g...      0             0   \n",
       "2  \"\\nSupport, noting that slip knot is already a...      0             0   \n",
       "3  This is your last warning. You will be blocked...      0             0   \n",
       "4  \"\\n\\nHaha. I can tell you that half of the wor...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        0       0       0              0   \n",
       "\n",
       "                                             cleaned  toxicity  \n",
       "0  [random, ip, thanks, reply, wa, probably, ip, ...         0  \n",
       "1  [``, archive, index, report, ha, generated, re...         0  \n",
       "2  [``, support, noting, slip, knot, already, ``,...         0  \n",
       "3  [last, warning, blocked, editing, next, time, ...         0  \n",
       "4  [``, haha, tell, half, world, ha, even, heard,...         0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load W2V model \n",
    "\n",
    "we_model = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make stratified sampling, for example: select 500 examples with obscene == 1, and 500 clean examples. \n",
    "''' TASK HERE'''\n",
    "\n",
    "# Select only a small sample of your data (20%), do not train your model on all of the data available \n",
    "# But to make the task easier, make a stratified selection \n",
    "# (number of 1 labels would be approximately equal to number of 0 labels)\n",
    "df_sample, _ = train_test_split('''Your code here ''')\n",
    "\n",
    "# Split the data on the stratified training and test data sets \n",
    "''' TASK HERE'''\n",
    "\n",
    "df_train, df_test = train_test_split('''Your code here ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9107, 11)\n",
      "Test shape: (6072, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape: {}\".format(df_train.shape))\n",
    "print(\"Test shape: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(df_sample): \n",
    "    '''\n",
    "    This function would process a DataFrame creating lists of:\n",
    "        vectors, labels and documents corresponding to each raw document. \n",
    "        \n",
    "    Args: \n",
    "        df: pd.DataFrame - DF to vectorize\n",
    "    Returns: \n",
    "        X: list - Vectorized documents, each value in a list is a torch.tensor\n",
    "        labels: list - Labels for each document, each value in a list is a torch.tensor\n",
    "        documents: list - List of the raw texts of the vectorized documents \n",
    "    '''\n",
    "    \n",
    "    # Obtain vectors for documents, vectorized documents list and labels\n",
    "    X, labels, documents = [], [], []\n",
    "    for i, (document, tokens, label) in enumerate(zip(df_sample.comment_text, df_sample.cleaned, df_sample.obscene)):\n",
    "        row_vectors = []\n",
    "        for kw in tokens:\n",
    "            try: \n",
    "                row_vectors.append(we_model[kw])\n",
    "            except (IndexError, KeyError): \n",
    "                continue\n",
    "        if not row_vectors:\n",
    "            continue\n",
    "        row_vectors = np.asarray(row_vectors)\n",
    "        vec = row_vectors.mean(axis=0)\n",
    "        X.append(torch.tensor(vec))\n",
    "        documents.append(document)\n",
    "        labels.append(torch.tensor(label, dtype=torch.float))\n",
    "        \n",
    "    return X, labels, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, documents_train = get_vectors(df_train)\n",
    "X_test, y_test, documents_test = get_vectors(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a simple NN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify your model to work with batches, not only single item. \n",
    "''' TASK HERE'''\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.logits = nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Makes a forward pass \n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        logits = self.logits(relu)\n",
    "        output = self.sigmoid(logits)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.17013384358194725\n",
      "Epoch 1: train loss: 0.09999112949621966\n",
      "Epoch 2: train loss: 0.08821684899244384\n",
      "Epoch 3: train loss: 0.08163851131012069\n",
      "Epoch 4: train loss: 0.07690653868591601\n",
      "Epoch 5: train loss: 0.0731831143033034\n",
      "Epoch 6: train loss: 0.07011041505292517\n",
      "Epoch 7: train loss: 0.06747566892405941\n",
      "Epoch 8: train loss: 0.06516502173082199\n",
      "Epoch 9: train loss: 0.06307586571729051\n"
     ]
    }
   ],
   "source": [
    "# Initialise the model \n",
    "model = FeedForward(300, 200)\n",
    "\n",
    "# Specify loss and optimization functions:\n",
    "\n",
    "# specify loss function\n",
    "criterion = nn.BCELoss()\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "# Move model to the training mode\n",
    "model.train()\n",
    "\n",
    "# init n_epochs \n",
    "n_epochs = 10\n",
    "\n",
    "# init number of iterations for one epoch \n",
    "# we want our model during the epoch to walk trough all of the training examples \n",
    "# for batch_size == 1, number of iterations would be equal to number of examples \n",
    "# in the training set \n",
    "n_iters = len(X_train)\n",
    "\n",
    "# initialise batch_size\n",
    "# NOTE! for now it's equal == 1, you need to modify your model to make it possible to work with \n",
    "# batches during training, not only making an update for a single example \n",
    "batch_size = 1\n",
    "for epoch in range(n_epochs):  \n",
    "    epoch_loss = 0\n",
    "    for idx in range(n_iters):\n",
    "        \n",
    "        # Selects only 1 sample, modify it to select N samples, N == batch_size\n",
    "        ''' TASK HERE'''\n",
    "        # idx = random.sample(range(len(X_train)), 1) # TIP: You can random sample N examples \n",
    "        \n",
    "        optimizer.zero_grad()    # Forward pass\n",
    "\n",
    "        # Select corresponding data from:\n",
    "        # X (vectors) and labels - for calculating the loss and making a backward pass \n",
    "        # backward pass - updating our weights according to the obtained loss \n",
    "        ''' TASK HERE'''\n",
    "        x = X_train[idx]\n",
    "        y_true = y_train[idx]\n",
    "\n",
    "        y_pred = model(x)    # Compute Loss\n",
    "        loss = criterion(y_pred.squeeze(), y_true)\n",
    "        \n",
    "        epoch_loss += loss.item() / n_iters\n",
    "        loss.backward()   # Backward pass \n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, epoch_loss))    # Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, X_test, y_test, documents_test, threshold): \n",
    "    n_prints = 0\n",
    "    preds = []\n",
    "    for example, label, document in zip(X_test, y_test, documents_test):\n",
    "        pred = model(example)\n",
    "        y_pred = int(pred.item() > threshold)\n",
    "        preds.append(y_pred)\n",
    "        \n",
    "        # Print some examples with obscene documents texts and predicted and true labels \n",
    "        if label.item() == 1.0 and n_prints < 10:\n",
    "            print(\"Predicted label: {}\".format(y_pred))\n",
    "            print(\"True label: {}\".format(label.item()))\n",
    "            print(\"Document: {}\".format(document))\n",
    "            print(\"*-*-\"*20)\n",
    "            n_prints += 1\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 0\n",
      "True label: 1.0\n",
      "Document: I had to fix all that sh%t manually. Do you know how long the common minke page is gonna take you piece of sh%t? What the f%$k is your problem?? Use a converter that doesn't f$%k up conversions or go the f%&k; home you stupid c$nt.\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 1\n",
      "True label: 1.0\n",
      "Document: YOU'RE A FAEGET! \n",
      "\n",
      "FAG\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 0\n",
      "True label: 1.0\n",
      "Document: Have you seen thier forum lately?  There is a new picture of you up with dicks all over it, cumming in your face!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 0\n",
      "True label: 1.0\n",
      "Document: Road Good aRticles are a pain in the ass. I've asked some one involved with the project what needs to be done with the article to take it to GA.  The history needs to be there, especially the early history which is missing.  It also needs more information on the road features, such as intersections, road width, road markings, etc. The few sources I've looked at haven't included this, and the roads people tend to use Google Maps as a source for the major intersections part.\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 0\n",
      "True label: 1.0\n",
      "Document: AURORA JOLIE \n",
      "\n",
      "Y N THE FUK DID YU DELETE HER WIKIPEDIA??? WHO ARE YU??...B**!!!!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 1\n",
      "True label: 1.0\n",
      "Document: i wanna fuck you \n",
      "\n",
      "fuck u fuck u fuck u\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 0\n",
      "True label: 1.0\n",
      "Document: This is ridiculous, Historian! First off, Haredi (in general) or Lithuanian Haredi (in particular) criticism is a perfectly valid topic for the encyclopedia. If it's sourced, it's includable, much as you may not like it. Next, Yated is as prominent as news organ in the Haredi world. Its opinions, including its editorials, do represent the views of the Lithuanian Orthodox leadership, and a discussion of those is valid here as well. Finally, Daniel575 is not even Litvish! He's explicitly a follower of the Dushinsky Hasidus, and by nationality is Dutch. Your positions here are ridiculous. Superfinally, stop using Yiddish and Hebrew terms! Andy is not Jewish, and probably has no idea wtf you're talking about! -  talk/email\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 0\n",
      "True label: 1.0\n",
      "Document: STUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALISTSTUPID SPANISH CENTRALIST\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 1\n",
      "True label: 1.0\n",
      "Document: cunt. I hate your face. You are a fucking shit faced wanker.\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 1\n",
      "True label: 1.0\n",
      "Document: why are u the way u are? \n",
      "\n",
      "ur a little bitch, why did u delete tylers wikipedia. you are probably gayer than him. go to hell\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n"
     ]
    }
   ],
   "source": [
    "# Move model to the eval mode before making a prediction\n",
    "model.eval()\n",
    "preds = make_predictions(model, X_test, y_test, documents_test, threshold=0.5)\n",
    "\n",
    "test_labels = [label.item() for label in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.99      5724\n",
      "         1.0       0.87      0.62      0.72       337\n",
      "\n",
      "    accuracy                           0.97      6061\n",
      "   macro avg       0.92      0.81      0.86      6061\n",
      "weighted avg       0.97      0.97      0.97      6061\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pring a classification report: \n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: \n",
    "\n",
    "#### Find all of the ''' TASK HERE ''' messages. \n",
    "\n",
    "1. Create stratified dataset, make your classes balanced! Train the model. Try to beat the initial score.\n",
    "\n",
    "2. While vectorizing by W2V model, add tf-idf weightning, look at TfidfVectorizer at sklearn. \n",
    "\n",
    "3. Add batch size, modify your model architecture to make it possible to process batches, not only single items. \n",
    "\n",
    "4. Change hidden_size, n_layers, activation function, etc to modify your model. \n",
    "\n",
    "5. Tweak learning rate, see what happened if LR is too small, if too big (0.0001 / 0.8 for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip:\n",
    "# Use tf-idf scores calculated by sklearn:\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    # This function is used to replace a default tokenizer in sklearn. \n",
    "    # If you are passing a tokenized documents to the tf-idf vectorizer - \n",
    "    # it would be much faster \n",
    "    return doc\n",
    "\n",
    "def get_idf(tokenized_docs, max_features=180000):\n",
    "    ''' Returns a tf-idf dictionary: \n",
    "            key: word,\n",
    "            value: tf-idf score. \n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        min_df=3,\n",
    "        max_features=max_features,\n",
    "        analyzer='word',\n",
    "        tokenizer=dummy_fun,\n",
    "        preprocessor=dummy_fun,\n",
    "        token_pattern=None,\n",
    "        ngram_range=(1, 1))\n",
    "\n",
    "    vectorizer.fit(tokenized_docs)\n",
    "    idf_dict = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "    \n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2, advanced\n",
    "\n",
    "Working with nn.Embedding layer \n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html \n",
    "\n",
    "Read an example below. \n",
    "\n",
    "Please, try to modify your initial version of the SingleLayerPerceptron model to the model with one additional layer: \n",
    "\n",
    "1. Define your vocabulary size  \n",
    "2. Add nn.Embedding layer to the model architecture (vocabulary_size, embedding_size) \n",
    "3. Retrain your model - see if metrics increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful parts for the part 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer  to the part 4.3 of the course:\n",
    "\n",
    "https://stepik.org/lesson/262247/\n",
    "\n",
    "It will help you to get the understanding how to use an nn.Embedding layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Let's create a vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_nested(nested):\n",
    "    flatten = []\n",
    "    for item in nested:\n",
    "        if isinstance(item, list):\n",
    "            flatten.extend(item)\n",
    "        else:\n",
    "            flatten.append(item)\n",
    "    return flatten\n",
    "\n",
    "cnt_vocab = Counter(flat_nested(df.cleaned.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 13061\n"
     ]
    }
   ],
   "source": [
    "threshold_count_l = 15\n",
    "threshold_count_h = 500\n",
    "threshold_len = 4\n",
    "cleaned_vocab = [token for token, count in cnt_vocab.items() if \n",
    "                     threshold_count_h > count > threshold_count_l and len(token) > threshold_len\n",
    "                ]\n",
    "print(\"Vocab size: {}\".format(len(cleaned_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to have an id for each of your token \n",
    "\n",
    "token_to_id = {v: k for k, v in enumerate(sorted(cleaned_vocab))}\n",
    "id_to_token = {v: k for k, v in token_to_id.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"task-28_29-01-2020.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tsXRPWN2BGOI","colab_type":"text"},"source":["## Prerequisites"]},{"cell_type":"markdown","metadata":{"id":"FT51fCRBBGOQ","colab_type":"text"},"source":["torch==1.1.0"]},{"cell_type":"code","metadata":{"id":"T2gW25dyBGOU","colab_type":"code","colab":{}},"source":["import random\n","from collections import Counter\n","\n","import numpy as np \n","import pandas as pd \n","import torch \n","import torch.nn as nn \n","\n","from gensim.models import KeyedVectors\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKkTU2XNLx49","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"479beb8d-4065-4300-b4ed-0f0a96c2b573","executionInfo":{"status":"ok","timestamp":1580331379773,"user_tz":-120,"elapsed":588,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"WKY7yNrWBGOi","colab_type":"code","colab":{}},"source":["df = pd.read_csv(\"train.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KEGjXaG4BGOr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":467},"outputId":"5e21b572-9150-461a-bf4a-c2c1ee58b996","executionInfo":{"status":"ok","timestamp":1580314228809,"user_tz":-120,"elapsed":1422,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["df.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>cleaned</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['explanation', 'edits', 'made', 'username', '...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[\"d'aww\", 'match', 'background', 'colour', \"'m...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['hey', 'man', \"'m\", 'really', 'trying', 'edit...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['``', 'ca', \"n't\", 'make', 'real', 'suggestio...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['sir', 'hero', 'chance', 'remember', 'page', ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                                            cleaned\n","0           0  ...  ['explanation', 'edits', 'made', 'username', '...\n","1           1  ...  [\"d'aww\", 'match', 'background', 'colour', \"'m...\n","2           2  ...  ['hey', 'man', \"'m\", 'really', 'trying', 'edit...\n","3           3  ...  ['``', 'ca', \"n't\", 'make', 'real', 'suggestio...\n","4           4  ...  ['sir', 'hero', 'chance', 'remember', 'page', ...\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"eTIPZlQmBGO4","colab_type":"text"},"source":["In this notebook you will learn pytorch basics, this framework will help you to build simple neural networks during this task.   \n","The first neural network we will try to learn is Feed Forward Neural Network which contain one Fully Connected Layer.  \n","It can have 1 or more fully connected layers, also it could be called as MLP - multilayer perceptron. "]},{"cell_type":"markdown","metadata":{"id":"K5ywM13rBGO7","colab_type":"text"},"source":["Read about PyTorch here:  \n","https://en.wikipedia.org/wiki/PyTorch\n","\n","And here:\n","\n","https://neurohive.io/ru/tutorial/glubokoe-obuchenie-s-pytorch/\n","\n","While reading these articles probably you will meet some unknown terms: \n","backpropagation algorithm, gradient descent, activation function, loss function, etc.  \n","Please, try to look for an information about why do you need all of these stuff. \n","\n","Answer this questions about Neural Nets: \n","\n","1. In previous tasks we created some features manually, tried to weight our features, tried to select special words for vectorization, how deep learning solves this problem? \n","\n","2. Why do we work with tensors in PyTorch?\n","\n","3. Please, find and read information - why do we need an activation functions in our models? Please, refer to the XOR problem with MLP without activation function, find information about it and answer the previous question. \n","\n","4. Please, answer the following question - what gradient is? Why do we need gradient descent algorithm? Which problem it solves? \n","\n","5. What is backpropagation algorithm? \n","\n","6. What is loss function? "]},{"cell_type":"markdown","metadata":{"id":"vDoUrh2P5TrR","colab_type":"text"},"source":["1.  Взять тот же Word2Vec, там есть реализация простой нейронной сети. Именно и она есть способом решения поставленной в вопросе проблемы. Дело в гибкости и настраиваемости параметров модели."]},{"cell_type":"markdown","metadata":{"id":"u6lawQyE4iun","colab_type":"text"},"source":["2. По сути, тензоры библиотеки pytorch - те же многомерные массивы библиотеки numpy, обладающие аналогичными возможностями. Используються для вычислений. Если ещё глянуть документацию и поверить в написанное (но лучше проверить, что я и сделал), то вычисления на тензорах могут проводиться как на центральном процессоре, так и на графическом."]},{"cell_type":"markdown","metadata":{"id":"Ml9Nz4OqFCuo","colab_type":"text"},"source":["3.  Активационный процесс заключается в том, когда при необходимом количестве входных данных нейрон передаёт значение далее по сети. Преобразовазованием этого значения занимается функция активации нейрона. Примеры активационных функций - сигмоидная функция (tanh, логистическая, ...), Хэвисайда и т.д.\n","<br>\n","Активационные функции необходимы для гибкости нейронной сети. Ними же решалась задача о линейной несепарабельности данных проблемы XOr."]},{"cell_type":"markdown","metadata":{"id":"8NACw-VV6FPD","colab_type":"text"},"source":["4. Пусть $\\Omega \\subset \\mathbb{R}^d \\> (d \\in \\mathbb{N})$ - область в $\\mathbb{R}^d$. Тогда функция $\\phi: \\Omega \\rightarrow \\mathbb{R}$ - скалярное поле.\n","<br>\n","Градиентом $\\phi$ является следующее выражение:\n","$\\nabla \\phi = (\\frac{\\partial \\phi}{\\partial t_1}, \\frac{\\partial \\phi}{\\partial t_2}, \\ldots, \\frac{\\partial \\phi}{\\partial t_d})$,<br>\n","где $\\frac{\\partial \\phi}{\\partial t_j}$ - частная производная $\\phi$ за переменной $t_j$. Градиент отождествляют с направлением в $\\Omega$, в котором $\\phi$ возрастает быстрее всего.\n","<br>\n","Градиентный спуск - метод нахождения локального экстремума некоторой функции с применением её (отрицательного) градиента. В машинном обучении,если рассматривать нейронные сети, то указанный метод используется в обучении модели в качестве принципа обратного распространения ошибки (backpropagation method). Там же и берётся градиент от функции ошибок (она же определяет качество работу нейронной сети в период циклического обучения).\n","<br>\n","Градиентный спуск используется для решения задачи минимизации среднего значения ошибки на выходе нейронной сети, обновляя весовые параметры модели."]},{"cell_type":"markdown","metadata":{"id":"wSmctPpCE3YZ","colab_type":"text"},"source":["5. Принцип обратного распространения ошибки - способ вычисления градиента функции, который используется при обновлении параметров многослойного персептрона. Цель - минимизация ошибки и получение желаемого результата."]},{"cell_type":"markdown","metadata":{"id":"Qtrb3bezBSV8","colab_type":"text"},"source":["6. Функция потерь - чувствительная к выбросам функция несогласия наблюдаемых данных и тех, что были предсказаны так званой подогнанной функцией модели."]},{"cell_type":"markdown","metadata":{"id":"SKmlTe8aBGP4","colab_type":"text"},"source":["Read the following article:\n","\n","https://en.wikipedia.org/wiki/Feedforward_neural_network\n","\n","What is FFNN? "]},{"cell_type":"markdown","metadata":{"id":"zCbY8qCEB0ra","colab_type":"text"},"source":["Нейронная сеть с прямой связью - тип сети, где входные данные обрабатывается из одного конца потока в другой, при этом поток состоит из последовательно соединенных нейронов, которые передают необходимые сигналы.\n","<br>\n","Для такого типа сетей циклы или петли обратной связи не характерны.\n","<br>\n","Простые примеры сетей такого плана: персептроны однослойные и многослойные."]},{"cell_type":"markdown","metadata":{"id":"WbbPBeoHBGQA","colab_type":"text"},"source":["## PyTorch basics"]},{"cell_type":"markdown","metadata":{"id":"YDkbQ7i3BGQC","colab_type":"text"},"source":["#### Autograd"]},{"cell_type":"code","metadata":{"id":"Y3MaIiMlBGQH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"89e927e3-ab8a-4030-feb0-bfb6358fcbfd","executionInfo":{"status":"ok","timestamp":1580314263899,"user_tz":-120,"elapsed":876,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["# Creating a tensor:\n","x = torch.ones(1, requires_grad=True)\n","\n","print(x.grad)    # returns None"],"execution_count":4,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iONaddY3BGQP","colab_type":"text"},"source":["print(x.grad) is None because a tensor x is a scalar, so there is nothing to be calculated."]},{"cell_type":"code","metadata":{"id":"2-0Jsk2cBGQR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5147c5c7-837a-48fe-e468-17d79ffaddaa","executionInfo":{"status":"ok","timestamp":1580314267383,"user_tz":-120,"elapsed":847,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["x = torch.ones(1, requires_grad=True)\n","y = 20 + x\n","z = (y ** 2) * 2 \n","z.backward()     # auto gradient calculation\n","\n","print(x.grad)    # ∂z/∂x "],"execution_count":5,"outputs":[{"output_type":"stream","text":["tensor([84.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rkRXolFfBGQb","colab_type":"text"},"source":["### Prepare the data"]},{"cell_type":"code","metadata":{"id":"EFpI_dT9BGQf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":467},"outputId":"fbadabe8-2d08-4020-848e-fb43629f2eb0","executionInfo":{"status":"ok","timestamp":1580314276946,"user_tz":-120,"elapsed":892,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["df.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>cleaned</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['explanation', 'edits', 'made', 'username', '...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[\"d'aww\", 'match', 'background', 'colour', \"'m...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['hey', 'man', \"'m\", 'really', 'trying', 'edit...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['``', 'ca', \"n't\", 'make', 'real', 'suggestio...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['sir', 'hero', 'chance', 'remember', 'page', ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                                            cleaned\n","0           0  ...  ['explanation', 'edits', 'made', 'username', '...\n","1           1  ...  [\"d'aww\", 'match', 'background', 'colour', \"'m...\n","2           2  ...  ['hey', 'man', \"'m\", 'really', 'trying', 'edit...\n","3           3  ...  ['``', 'ca', \"n't\", 'make', 'real', 'suggestio...\n","4           4  ...  ['sir', 'hero', 'chance', 'remember', 'page', ...\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"5cTDg0LjBGQp","colab_type":"code","colab":{}},"source":["# Modify labels dtype to 'int', to make summarizing them possible\n","for column in df.columns: \n","    if column not in ['id', 'comment_text', 'cleaned']:\n","        df[column] = df[column].astype('int32')\n","        \n","# Create a toxicity column (sums all of the toxic labels)\n","df['toxicity'] = df.iloc[:,2:8].sum(axis=1)\n","\n","# Clean data - where toxicity is == 0 \n","clean = df[df['toxicity'] == 0]\n","# Messages, which were labelled as obscene\n","obscene = df[df['obscene'] == 1]\n","\n","# Create a dataset for binary classification \n","df_binary = clean.append(obscene, ignore_index=True, sort=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dplHZtY5BGQx","colab_type":"code","colab":{}},"source":["# Shuffle\n","df_binary = df_binary.sample(frac=1)\n","\n","# Reset index of the pd.DataFrame\n","df_binary.reset_index(inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnXaVRgxBGQ4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":567},"outputId":"596c5c07-574f-4579-8516-c77ded6f8d64","executionInfo":{"status":"ok","timestamp":1580314288286,"user_tz":-120,"elapsed":968,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["df_binary.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>Unnamed: 0</th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>cleaned</th>\n","      <th>toxicity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>100556</td>\n","      <td>111910</td>\n","      <td>56aa435ccdab23e3</td>\n","      <td>And there is your main problem. Your edited a ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['main', 'problem', 'edited', 'page', 'band', ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>99725</td>\n","      <td>110989</td>\n","      <td>51c3e418d77f16be</td>\n","      <td>Needs rewording \\n\\nWhole passages are lifted ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['need', 'rewording', 'whole', 'passage', 'lif...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>106372</td>\n","      <td>118360</td>\n","      <td>786adbf417e61232</td>\n","      <td>First of all, please indicate on your user pag...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['first', 'please', 'indicate', 'user', 'page'...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>125406</td>\n","      <td>139555</td>\n","      <td>eae9a684c7d803ba</td>\n","      <td>\", 2 December 2008 (UTC)\\nAlso, with respect t...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['``', '2', 'december', '2008', 'utc', 'also',...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>14031</td>\n","      <td>15601</td>\n","      <td>292c4283aa8adaaf</td>\n","      <td>GR \\n\\nI'll try and spend some time on the art...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>['gr', \"'ll\", 'try', 'spend', 'time', 'article...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    index  ...  toxicity\n","0  100556  ...         0\n","1   99725  ...         0\n","2  106372  ...         0\n","3  125406  ...         0\n","4   14031  ...         0\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"c5FEsONUBGRA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"b0d2d1bd-d56d-4a9d-ef92-746d3fa5bf4f","executionInfo":{"status":"ok","timestamp":1580314641499,"user_tz":-120,"elapsed":349840,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["# Load W2V model \n","import gensim.downloader as api\n","we_model = api.load('word2vec-google-news-300')\n","#we_model = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zqIIroE8BGRJ","colab_type":"code","colab":{}},"source":["# Make stratified sampling, for example: select 500 examples with obscene == 1, and 500 clean examples. \n","''' TASK HERE'''\n","\n","# Select only a small sample of your data (20%), do not train your model on all of the data available \n","# But to make the task easier, make a stratified selection \n","# (number of 1 labels would be approximately equal to number of 0 labels)\n","df_sample, _ = train_test_split(df_binary, train_size = 0.2)\n","\n","# Split the data on the stratified training and test data sets \n","''' TASK HERE'''\n","\n","df_train, df_test = train_test_split(\n","    df_sample, train_size = 0.6, stratify = df_sample['obscene'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9b7g9cfsBGRT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"267e2ba4-e4c0-4130-a5b9-05be0089d9e5","executionInfo":{"status":"ok","timestamp":1580318448231,"user_tz":-120,"elapsed":792,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["print(\"Train shape: {}\".format(df_train.shape))\n","print(\"Test shape: {}\".format(df_test.shape))"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Train shape: (18221, 12)\n","Test shape: (12148, 12)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bKsYP7WuBGRb","colab_type":"code","colab":{}},"source":["def get_vectors(df_sample): \n","    '''\n","    This function would process a DataFrame creating lists of:\n","        vectors, labels and documents corresponding to each raw document. \n","        \n","    Args: \n","        df: pd.DataFrame - DF to vectorize\n","    Returns: \n","        X: list - Vectorized documents, each value in a list is a torch.tensor\n","        labels: list - Labels for each document, each value in a list is a torch.tensor\n","        documents: list - List of the raw texts of the vectorized documents \n","    '''\n","    \n","    # Obtain vectors for documents, vectorized documents list and labels\n","    X, labels, documents = [], [], []\n","    for i, (document, tokens, label) in enumerate(zip(df_sample.comment_text, df_sample.cleaned, df_sample.obscene)):\n","        row_vectors = []\n","        for kw in tokens:\n","            try: \n","                row_vectors.append(we_model[kw])\n","            except (IndexError, KeyError): \n","                continue\n","        if not row_vectors:\n","            continue\n","        row_vectors = np.asarray(row_vectors)\n","        vec = row_vectors.mean(axis=0)\n","        X.append(torch.tensor(vec))\n","        documents.append(document)\n","        labels.append(torch.tensor(label, dtype=torch.float))\n","        \n","    return X, labels, documents"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I2EKt_yKBGRj","colab_type":"code","colab":{}},"source":["X_train, y_train, documents_train = get_vectors(df_train)\n","X_test, y_test, documents_test = get_vectors(df_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_lazhoAmBGRq","colab_type":"text"},"source":["### How to create a simple NN: "]},{"cell_type":"code","metadata":{"id":"J64yR9qYBGRs","colab_type":"code","colab":{}},"source":["# Modify your model to work with batches, not only single item. \n","''' TASK HERE'''\n","\n","class FeedForward(nn.Module):\n","    \n","    def __init__(self, input_size, hidden_size):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        \n","        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n","        self.relu = nn.ReLU()\n","        self.logits = nn.Linear(self.hidden_size, 1)\n","        self.sigmoid = nn.Sigmoid()\n","        \n","    def forward(self, x):\n","        # Makes a forward pass \n","        hidden = self.fc1(x)\n","        relu = self.relu(hidden)\n","        logits = self.logits(relu)\n","        output = self.sigmoid(logits)\n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1Xcu9YoPPiR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":123},"outputId":"6f0c9364-fb37-4176-ec59-8a71145f98de","executionInfo":{"status":"ok","timestamp":1580318489602,"user_tz":-120,"elapsed":1017,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["model = FeedForward(300, 200)\n","model"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FeedForward(\n","  (fc1): Linear(in_features=300, out_features=200, bias=True)\n","  (relu): ReLU()\n","  (logits): Linear(in_features=200, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"eBcxJ8SoBGR0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"125aee87-d8ac-48e5-c8c7-554034f1b028","executionInfo":{"status":"ok","timestamp":1580325671120,"user_tz":-120,"elapsed":159723,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["# Initialise the model \n","\n","\n","# Specify loss and optimization functions:\n","\n","# specify loss function\n","criterion = nn.BCELoss()\n","# specify optimizer\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n","\n","# Move model to the training mode\n","model.train()\n","\n","# init n_epochs \n","n_epochs = 10\n","\n","# init number of iterations for one epoch \n","# we want our model during the epoch to walk trough all of the training examples \n","# for batch_size == 1, number of iterations would be equal to number of examples \n","# in the training set \n","n_iters = len(X_train)\n","\n","# initialise batch_size\n","# NOTE! for now it's equal == 1, you need to modify your model to make it possible to work with \n","# batches during training, not only making an update for a single example \n","batch_size = 1\n","for epoch in range(n_epochs):  \n","    epoch_loss = 0\n","    for idx in range(n_iters):\n","        \n","        # Selects only 1 sample, modify it to select N samples, N == batch_size\n","        ''' TASK HERE'''\n","        # idx = random.sample(range(len(X_train)), 1) # TIP: You can random sample N examples \n","        \n","        optimizer.zero_grad()    # Forward pass\n","\n","        # Select corresponding data from:\n","        # X (vectors) and labels - for calculating the loss and making a backward pass \n","        # backward pass - updating our weights according to the obtained loss \n","        ''' TASK HERE'''\n","        x = X_train[idx]\n","        y_true = y_train[idx]\n","\n","        y_pred = model(x)    # Compute Loss\n","        loss = criterion(y_pred.squeeze(), y_true)\n","        \n","        epoch_loss += loss.item() / n_iters\n","        loss.backward()   # Backward pass \n","        optimizer.step()\n","        \n","    print('Epoch {}: train loss: {}'.format(epoch, epoch_loss))    # Backward pass"],"execution_count":55,"outputs":[{"output_type":"stream","text":["Epoch 0: train loss: 0.15862504894716078\n","Epoch 1: train loss: 0.15816792545241073\n","Epoch 2: train loss: 0.1576178433830413\n","Epoch 3: train loss: 0.1572119482741705\n","Epoch 4: train loss: 0.1567066553905162\n","Epoch 5: train loss: 0.15633618162955137\n","Epoch 6: train loss: 0.15596891036991206\n","Epoch 7: train loss: 0.15559410756721104\n","Epoch 8: train loss: 0.15525694491526815\n","Epoch 9: train loss: 0.15502369151111636\n","Epoch 10: train loss: 0.15465777890029828\n","Epoch 11: train loss: 0.1543910327060794\n","Epoch 12: train loss: 0.15412987406842216\n","Epoch 13: train loss: 0.1539037730773472\n","Epoch 14: train loss: 0.1535646347953046\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7fXDJlodBGR9","colab_type":"code","colab":{}},"source":["def make_predictions(model, X_test, y_test, documents_test, threshold): \n","    n_prints = 0\n","    preds = []\n","    for example, label, document in zip(X_test, y_test, documents_test):\n","        pred = model(example)\n","        y_pred = int(pred.item() > threshold)\n","        preds.append(y_pred)\n","        \n","        # Print some examples with obscene documents texts and predicted and true labels \n","        if label.item() == 1.0 and n_prints < 10:\n","            print(\"Predicted label: {}\".format(y_pred))\n","            print(\"True label: {}\".format(label.item()))\n","            print(\"Document: {}\".format(document))\n","            print(\"*-*-\"*20)\n","            n_prints += 1\n","        \n","    return preds"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FpEp2GxyBGSK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9ca711cb-26cc-438f-d278-2ea4fee97a4f","executionInfo":{"status":"ok","timestamp":1580325771504,"user_tz":-120,"elapsed":3636,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["# Move model to the eval mode before making a prediction\n","model.eval()\n","preds = make_predictions(model, X_test, y_test, documents_test, threshold=0.5)\n","\n","test_labels = [label.item() for label in y_test]"],"execution_count":57,"outputs":[{"output_type":"stream","text":["Predicted label: 0\n","True label: 1.0\n","Document: ONOREM IS STILL A FAUGOTT\n","*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n","Predicted label: 1\n","True label: 1.0\n","Document: dr karl loves himself, fuck yourselves wikipedia \n","\n","go fuck yourselves wikipedia\n","*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n","Predicted label: 0\n","True label: 1.0\n","Document: \"\n","\n"," Thake v Maurice \n","\n","Just because it had a picture of a penis!  \"\n","*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n","Predicted label: 0\n","True label: 1.0\n","Document: \"\n","\n"," You're a fucking moron \n","\n","Please kill yourself. You are a cancerous moron devoid of all intelligence and have no business editing Wikipedia or having any privileges thereof. I demand that you restore my userpages in full, removing any copyright infringement contained within. Mate1 \"\n","*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n","Predicted label: 0\n","True label: 1.0\n","Document: HLQall of these things happen to be lies so go fuck yourself and die lol jk D\n","*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n","Predicted label: 0\n","True label: 1.0\n","Document: Oh, and the rifle I used when I was in the Basij of Isfahan. The rifle was so powerful, I couldnt hold it properly. I was fucking 13. And nobody sent me there. When you see Iraqi war planes fucking your town and home, defending your own people doesnt understand the meaning of age.\n","*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n","Predicted label: 1\n","True label: 1.0\n","Document: Oh no you don't. F&CK;!\n","*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n","Predicted label: 0\n","True label: 1.0\n","Document: You Are A Complete Idiot \n","\n","I didn't create this account just for promotion, thought I could actually start editing wikipedia articles since I find so many that are complete useless when I do google searches. But thanks for being a complete idiot. The only warning I should be getting is for personal attacks against your stupidity. You pissed me off and now its gonna suck for you.\n","*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n","Predicted label: 0\n","True label: 1.0\n","Document: (A white fatass that ain't sh!t and acting like a bihcpadding\n","*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n","Predicted label: 0\n","True label: 1.0\n","Document: \"\n","\n"," QUIT EDITING CHICAGO MED \n","\n","YOU ARE SO STUPID, HOW CAN SOMEONE POST ON HERE AND GET IT VERFIED WITHOUT YOU CHANGING IT, INFORMATION POSTED ON HERE THATS CITED IS NOT COPYRIGHT. ARE YOU FROM AMERICA U IDIOT GET A LIFE. STOP RUINGING CHICAGO MEDS PAGE  — Preceding unsigned comment added by 67.184.255.230   \"\n","*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FL7gteP9Sc_E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":176},"outputId":"838d9fcd-a957-43e4-ff61-e2100cea1753","executionInfo":{"status":"ok","timestamp":1580325779539,"user_tz":-120,"elapsed":1373,"user":{"displayName":"Daniel The Human","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBUmMy579YAVePyM-d7M5a9AuelZcd0sWL1969gVw=s64","userId":"18199465969344515242"}}},"source":["print(classification_report(test_labels, preds))"],"execution_count":58,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.96      1.00      0.98     11465\n","         1.0       0.74      0.24      0.36       681\n","\n","    accuracy                           0.95     12146\n","   macro avg       0.85      0.62      0.67     12146\n","weighted avg       0.94      0.95      0.94     12146\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"BTZtxGDMBGSR","colab_type":"code","colab":{},"outputId":"a83acbc7-928b-4fb7-a018-de69acc99b45"},"source":["# init classification report"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","         0.0       0.98      0.99      0.99      5724\n","         1.0       0.87      0.62      0.72       337\n","\n","    accuracy                           0.97      6061\n","   macro avg       0.92      0.81      0.86      6061\n","weighted avg       0.97      0.97      0.97      6061\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FyaDBaimBGSY","colab_type":"text"},"source":["## Task 1: \n","\n","#### Find all of the ''' TASK HERE ''' messages. \n","\n","1. Create stratified dataset, make your classes balanced! Train the model. Try to beat the initial score.\n","\n","2. While vectorizing by W2V model, add tf-idf weightning, look at TfidfVectorizer at sklearn. \n","\n","3. Add batch size, modify your model architecture to make it possible to process batches, not only single items. \n","\n","4. Change hidden_size, n_layers, activation function, etc to modify your model. \n","\n","5. Tweak learning rate, see what happened if LR is too small, if too big (0.0001 / 0.8 for example)"]},{"cell_type":"code","metadata":{"id":"_uZidQEdBGSa","colab_type":"code","colab":{}},"source":["# Tip:\n","# Use tf-idf scores calculated by sklearn:\n","\n","def dummy_fun(doc):\n","    # This function is used to replace a default tokenizer in sklearn. \n","    # If you are passing a tokenized documents to the tf-idf vectorizer - \n","    # it would be much faster \n","    return doc\n","\n","def get_idf(tokenized_docs, max_features=180000):\n","    ''' Returns a tf-idf dictionary: \n","            key: word,\n","            value: tf-idf score. \n","    '''\n","    vectorizer = TfidfVectorizer(\n","        min_df=3,\n","        max_features=max_features,\n","        analyzer='word',\n","        tokenizer=dummy_fun,\n","        preprocessor=dummy_fun,\n","        token_pattern=None,\n","        ngram_range=(1, 1))\n","\n","    vectorizer.fit(tokenized_docs)\n","    idf_dict = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n","    \n","    return idf_dict"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pdj4E3D5BGSh","colab_type":"text"},"source":["## Task 2, advanced\n","\n","Working with nn.Embedding layer \n","\n","https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html \n","\n","Read an example below. \n","\n","Please, try to modify your initial version of the SingleLayerPerceptron model to the model with one additional layer: \n","\n","1. Define your vocabulary size  \n","2. Add nn.Embedding layer to the model architecture (vocabulary_size, embedding_size) \n","3. Retrain your model - see if metrics increased."]},{"cell_type":"markdown","metadata":{"id":"EyMP9FAuBGSj","colab_type":"text"},"source":["### Useful parts for the part 2: "]},{"cell_type":"markdown","metadata":{"id":"e0JA6aFDBGSn","colab_type":"text"},"source":["Refer  to the part 4.3 of the course:\n","\n","https://stepik.org/lesson/262247/\n","\n","It will help you to get the understanding how to use an nn.Embedding layer. "]},{"cell_type":"markdown","metadata":{"id":"zEf2AuG_BGSq","colab_type":"text"},"source":["#####  Let's create a vocabulary: "]},{"cell_type":"code","metadata":{"id":"HUh8Sk3qBGSu","colab_type":"code","colab":{}},"source":["def flat_nested(nested):\n","    flatten = []\n","    for item in nested:\n","        if isinstance(item, list):\n","            flatten.extend(item)\n","        else:\n","            flatten.append(item)\n","    return flatten\n","\n","cnt_vocab = Counter(flat_nested(df.cleaned.tolist()))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"etFoZNG8BGS0","colab_type":"code","colab":{},"outputId":"fc88eb41-2df6-46c7-d018-8f177874accc"},"source":["threshold_count_l = 15\n","threshold_count_h = 500\n","threshold_len = 4\n","cleaned_vocab = [token for token, count in cnt_vocab.items() if \n","                     threshold_count_h > count > threshold_count_l and len(token) > threshold_len\n","                ]\n","print(\"Vocab size: {}\".format(len(cleaned_vocab)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Vocab size: 13061\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rjyk-id_BGS7","colab_type":"code","colab":{}},"source":["# You will need to have an id for each of your token \n","\n","token_to_id = {v: k for k, v in enumerate(sorted(cleaned_vocab))}\n","id_to_token = {v: k for k, v in token_to_id.items()}"],"execution_count":0,"outputs":[]}]}